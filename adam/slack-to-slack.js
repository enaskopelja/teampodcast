const ARTICLES = [
{
  doi: '10.1101/2023.02.03.527007',
  title: 'Correspondence between functional scores from deep mutational scans and predicted effects on protein stability',
  authors: [ 'Gerasimavicius Lukas', 'Livesey Benjamin J', 'Marsh Joseph A.' ],
  abstract: 'Many methodologically diverse computational methods have been applied to the growing challenge of predicting and interpreting the effects of protein variants. As many pathogenic mutations have a perturbing effect on protein stability or intermolecular interactions, one highly interpretable approach is to use protein structural information to model the physical impacts of variants and predict their likely effects on protein stability and interactions. Previous efforts have assessed the accuracy of stability predictors in reproducing thermodynamically accurate values and evaluated their ability to distinguish between known pathogenic and benign mutations. Here, we take an alternate approach, and explore how well stability predictor scores correlate with functional impacts derived from deep mutational scanning (DMS) experiments. In this work, we compare the predictions of 9 protein stability-based tools against mutant protein fitness values from 45 independent DMS datasets, covering 161,441 unique single amino acid variants. We find that FoldX and Rosetta show the strongest correlations with DMS-based functional scores, similar to their previous top performance in distinguishing between pathogenic and benign variants. For both methods, performance is considerably improved when considering intermolecular interactions from protein complex structures, when available. Finally, we also highlight that predicted stability effects show consistently higher correlations with certain DMS experimental phenotypes, particularly those based upon protein abundance, and, in certain cases, can be competitive with other sequence-based variant effect prediction methodologies for predicting functional scores from DMS experiments.',
  body: [
    'Recent decades have seen massive breakthroughs in optimizing genomic sequencing for large-scale operations, revealing the high prevalence of genetic variation in human populations. Many genetic variants are from missense mutations, which cause a change in the identity of single amino acid residues at the protein level. However, the precise phenotypic consequences of most variants remain uncertain as mutants are seldom functionally characterized in clinical settings, while alternative approaches to conclusively classify variants, such as genetic testing and pedigree studies, require many cases.',
    'Multiplex assays of variant effects have emerged as methodologies with the potential to measure the effects of large numbers of genetic variants in parallel within a single experiment. MAVEs produce interpretable variant-function maps by associating each variant to a quantitative assay measurement for a select phenotype. MAVEs that involve protein-based assays of amino acid variants are often referred to as deep mutational scanning experiments. DMS has been widely adopted to explore effects of amino acid variation using a variety of different experimental phenotypes, such as protein abundance, activity or general cellular fitness. However, while the number of proteins that have been characterized through DMS grows constantly, and use of these methodologies and coordination between groups is increasing through the Atlas of Variant Effects Alliance, as of now DMS is not up to the challenge of evaluating all possible substitutions in the entire proteome, both due to costs and the inherent limitations of assaying specific phenotypes.',
    'As an alternative or complement to experimental approaches for characterizing variants, considerable efforts have been put into developing generalizable computational models for predicting the effects of protein variants. A large number of variant effect predictors exist that leverage different properties, including evolutionary sequence conservation, phylogenetic relationships and physicochemical properties, to evaluate the likelihood of a variant being damaging. However, the scores output by these VEPs seldom provide an interpretable context to the underlying disease mechanism. An alternative approach is presented by structure-based protein stability predictors, which can evaluate the change in Gibbs free energy of folding or intermolecular interaction upon mutation. Stability predictors are frequently used in the fields of protein engineering and even clinical genetics, despite not being trained for disease identification, because they can distinguish between stabilizing and destabilizing energetic effects and provide clues as to possible pathogenic mechanisms.',
    'The methodological approaches to predicting stability impacts of mutations are diverse. FoldXand Rosettause empirical physics-based potentials with additional statistical terms based on observations from bimolecular structures. ENCoM is a unique method that takes into account how mutations can impact protein dynamics and stability through normal mode analysis. A combination of evolutionary and structural information has been employed in the untrained DDGun3D predictor. Some recent predictors, like mCSM, have been derived through machine learning using various features. Given such heterogeneity in approaches, numerous studies have been carried out to benchmark the performance of these predictors in reproducing realistic ΔΔG values that agree with experimental thermostability data. Furthermore, there have been attempts to explore and address biases, such as data circularity, overprediction of destabilizing variants and lack of prediction symmetry. However, as stability predictors are now routinely used for protein engineering and disease identification purposes, it is crucial to know how well ΔΔG serves as a proxy score for pathogenicity, and thus how prevalent are destabilizing loss-of-function mechanisms in the pool of all possible mutations. We have previously assessed the performance of ΔΔG values from stability predictors in distinguishing between pathogenic and putatively benign missense variants in a classification task. Phenotypic assays now offer further opportunity to more quantitatively interrogate the extent to which predicted ΔΔG agrees with assayed fitness or activity of protein variants.',
    'DMS datasets currently provide the most extensive experimentally derived representation of the functional variant effect landscape, and they have been very successfully utilized in recent VEP benchmarking studies. It was further shown that DMS assay values themselves can, in some cases, be better at distinguishing pathogenic from benign variants than current computational approaches. However, a number of caveats should be understood when using DMS scores to evaluate how damaging effects are represented through predicted changes in stability. Realistically, we can expect that assays that evaluate phenotypes such as protein abundance or complex formation, should show the best agreement with stability predictions, as they are well suited to detect destabilizing loss-of-function molecular mechanisms. Other types of assays, such as general competitive growth experiments, are potentially sensitive to non-destabilizing but damaging mutations, such as those associated with gain-of-function or dominant-negative effects. We have previously shown that damaging mutations, which manifest through such non-loss-of-function mechanisms, tend to be mild at a protein structural level, and not well identified through prediction of stability effects. Thus, some specific types of assays, which do not measure stability directly, may show very heterogenous agreement with stability predictors. However, we believe DMS values provide an unbiased, independent way of comparing predictors, and at the same time allow us to explore how well destabilizing loss-of-function mechanisms can be identified through specific experimental phenotypes.',
    'In this study, using a large number of DMS datasets as a benchmark, we quantified the capability of structure-based protein stability predictors to accurately rank the functional impacts of variants. We demonstrate that FoldX and Rosetta predictions derived on protein complex structures significantly outperform other tools in assessing the functional impact of mutations. We also show how evaluating full biological assemblies improves our ability to relate predictions to functional phenotypes involving protein or DNA binding. Finally, we explore how certain types of DMS phenotypes, specifically ones related to protein abundance, correlate the best with variant stability predictions due to their closer association with destabilizing loss-of-function mechanisms.',
    'Our study primarily aimed to examine the relationship between stability, structure and function, by interrogating the capacity of different protein stability predictors to score variants in line with experimentally determined functional impacts. We found that FoldX and Rosetta show the strongest correlations with DMS measurements, and also demonstrated the importance of protein complex structures for evaluating the functional impact of variants to the fullest extent. DMS experiments proved to be instrumental for this benchmarking task, as their results are currently the most direct representation of the functional protein variant landscape. Our results support the choice of Rosetta as a predictive tool, which has recently been used in similar studies exploring DMS functional score relationships with stability- and conservation-based metrics. Unlike many VEPs, which are optimized for human mutations, or influenced by the widely varying sequence coverage across evolutionary space, stability predictors should be well suited to evaluate proteins in an organismagnostic manner, as most are grounded in physics or approximate physical terms by proxy. Finally, we must accentuate that stability predictors are not designed for disease variant identification, and their training and design is aimed at reproducing realistic ΔΔG values that reflect experimental thermostability changes, and not rank functional impacts. Thus, our predictor ranking does not necessarily imply that FoldX or Rosetta are the most accurate tools for predicting either the ΔΔG magnitude, or the direction of stability perturbing effects. Despite this, stability predictors are still routinely used in clinical research for variant prioritization and mechanistic interpretation, mostly because of their high interpretability due to their structure-based nature, and exploring the ability of ΔΔG to reflect functional impact could lead to a more effective application of these methodologies for such purposes.',
    'Interestingly, we have previously also demonstrated FoldX was the best out of all tested stability predictors at distinguishing between confirmed human disease variants and putatively benign ones through both ΔΔG and | ΔΔG |, while Rosetta did not rank as high on that particular task. One aspect of FoldX performance that made it useful for disease identification was its tendency to assign excessively large stability perturbation scores to disease variants, due to the clashes they caused within structures, very effectively separating the score densities between putatively benign mutations and truly deleterious ones. However, in this current work we demonstrate that FoldX and Rosetta also have the capacity to maintain an accurate relative ranking of functional variant effects, and not just produce outlier scores for disease mutations. This is potentially of great benefit, as it shows such predictors can delineate between hypomorphic and full loss-of-function effects for a given protein, which could be useful when interpreting and prioritizing variants or patient genotypes.',
    'Some heterogeneity in predictor performance is not surprising. Previous efforts to benchmark their accuracy in reproducing realistic ΔΔG values have revealed highly varied performance, owing to the different methodological approaches and biases. A likely reason for the success of FoldX and Rosetta are their empirical scoring functions, containing energetic and statistical terms parametrized based on experimental data. It is unlikely that the source of high FoldX and Rosetta performance is overtraining on test data, as we are benchmarking a different kind of performance altogether. Instead, the high agreement is likely due to an underlying association of loss-of-function mechanisms with the protein and phenotype in question. RaSP, a sequence-based method, benefits indirectly through being parametrized against Rosetta predictions, while leveraging considerably improved computational speed through a simplified model, beating other predictors that use sequence features. More novel approaches combining machine learning with sequence- or structure-based features, such as mCSM, DDGun3D, PoPMuSiC and INPS3D, do not seem to overall be as effective at ranking functional impacts. However, these state-of-the-art methods have been shown to be accurate at predicting actual ΔΔG values, and improve upon other methodologies in terms of prediction symmetry, and are less sensitive to resolution and substitution type biases.',
    'Interestingly, we observed the more unconventional methods, such as ENCoM and DynaMut2, benefit the most from using absolute ΔΔG values, but also, despite having adjusted for effect directions of predictors and DMS sets to match, often demonstrate moderate inverse correlations, likely due to experimental noise. It is also important to point that DDGun3D and INPS3D, the hybrid sequence and structure-based approaches, effectively include sequence-derived features representing evolutionary conservation in their predictions. Conservation is known to be predictive of damaging mutations, regardless of molecular mechanism, which would suggest that these methods could be capable of predicting mutations at conserved positions to be morestabilizing than they actually are, resulting in a stronger correlation with the functional scores from DMS assays. Given that non-loss-of-function disease variants tend to be considerably structurally milder due to their molecular mechanisms, we would expect them to be poorly predicted by conventional stability predictors, since their damaging effects are unlikely to be due to destabilization. DDGun3D and INPS3D appear to show relatively better correlations, compared with other methods, on essential, highly conserved genes, such as HSP82 and PAB1, or some genes with mixed disease mechanism like TARDBP and SRC. However, sequence features do not appear to consistently favour the prediction of non-LOF disease gene variants, like CALM1, and overall these hybrid predictors demonstrate poor performance in the correlation ranking, and especially on the abundance-based DMS datasets.',
    'In this study we also demonstrated the utility of using complex structures of biological units, as monomer structures may not be sufficient for assessing the full functional impact of a variant, for instance due to molecular mechanisms involving intermolecular interactions, as shown previously. Both FoldX and Rosetta, the best ranked methods, and ENCoM, the poorest, were able to take into account structures containing more than one protein chain, and in the case of FoldX and Rosetta also other biomolecules. All methods saw increased correlations with DMS values for some datasets which are based on phenotypes that involved assessing binding, aggregation, or for genes with known involvement in functional interactions. The correlation between FoldX and the transcription factor GAL4 saw considerable benefit, because FoldX is able to evaluate the stability perturbations involving DNA. Of course, such an approach depends on having available structures, and being able to assess the relevance of a given assembly for assessing a particular phenotype. While DeepMind have now made monomeric AlphaFold2 model structures of the whole proteome available to all, the Protein Data Bank still remains the main source of functionally relevant protein complexes.',
    'Our work hints at the pervasiveness of destabilizing loss-of-function mechanisms throughout the functional variant landscape, and the importance of choosing the most informative phenotypes for DMS experiments. Both stability predictors and VEPs we tested performed the best on experiments interrogating protein abundance and stability, such as various assays involving fluorescent reporters or VAMP-seq, and some antibiotic survival experiments. However, such DMS datasets are less likely to accurately reflect impacts of variants associated with other molecular mechanisms. In the case of proteins with multiple functions or disease mechanism, multiple different assays would be required to gleam the full scope of a variant. This could be alleviated by using competitive growth assays, which allow to capture the broadest selection of functional effects from variants, however, the delineation of what molecular mechanism might be responsible for the increase or reduction in fitness is lost. Other general issues with DMS data are that it can be noisy, restricted to a small set of experimental conditions, and also removed from the original cellular context. Due to these limitations, as well as the enormous resource cost of most current DMS methodologies, they are unlikely to replace computational prediction tools as the main avenue to fully understanding functional effects of missense mutations in the near future. However, an exciting new methodology, dubbed cDNA display proteolysis, was recently shown to be capable of assessing functional variant effects on protein thermodynamic stability at tremendous scale and speed. While limited to a stability phenotype, such a DMS approach also presents a valuable opportunity to gleam insight into the mechanisms of LOF disease, further test the accuracy of current computational tools on a large independent dataset and use it for training and developing better methodologies.'
  ]
},
{
  doi: '10.1101/2021.10.10.463845',
  title: 'Deep learning redesign of PETase for practical PET degrading applications',
  authors: [
    'Lu Hongyuan',
    'Diaz Daniel J.',
    'Czarnecki Natalie J.',
    'Zhu Congzhi',
    'Kim Wantae',
    'Shroff Raghav',
    'Acosta Daniel J.',
    'Alexander Brad',
    'Cole Hannah',
    'Zhang Yan Jessie',
    'Lynd Nathaniel',
    'Ellington Andrew D.',
    'Alper Hal S.'
  ],
  abstract: 'Plastic waste poses an ecological challenge. While current plastic waste management largely relies on unsustainable, energy-intensive, or even hazardous physicochemical and mechanical processes, enzymatic degradation offers a green and sustainable route for plastic waste recycling. Poly(ethylene terephthalate) (PET) has been extensively used in packaging and for the manufacture of fabrics and single-used containers, accounting for 12% of global solid waste. The practical application of PET hydrolases has been hampered by their lack of robustness and the requirement for high processing temperatures. Here, we use a structure-based, deep learning algorithm to engineer an extremely robust and highly active PET hydrolase. Our best resulting mutant (FAST-PETase:unctional,ctive,table, andolerant PETase) exhibits superior PET-hydrolytic activity relative to both wild-type and engineered alternatives, (including a leaf-branch compost cutinase and its mutant) and possesses enhanced thermostability and pH tolerance. We demonstrate that whole, untreated, post-consumer PET from 51 different plastic products can all be completely degraded by FAST-PETase within one week, and in as little as 24 hours at 50 °C. Finally, we demonstrate two paths for closed-loop PET recycling and valorization. First, we re-synthesize virgin PET from the monomers recovered after enzymatic depolymerization. Second, we enablemicrobially-enabled valorization using astrain together with FAST-PETase to degrade PET and utilize the evolved monomers as a carbon source for growth and polyhydroxyalkanoate production. Collectively, our results demonstrate the substantial improvements enabled by deep learning and a viable route for enzymatic plastic recycling at the industrial scale.',
  body: [
    'Poly(ethylene terephthalate) composes 70% of synthetic textile fibers and 10% of non-fiber plastic packaging, and correspondingly represents an enormous waste stream of single-use, manufactured materials. Yet, a circular carbon economy for PET is theoretically attainable through rapid enzymatic depolymerization followed by either chemical repolymerization or microbial upcycling/valorization into other products. However, all existing PET-hydrolyzing enzymes are limited in their capacity to either function within moderate pH/temperature ranges or directly utilize untreated post-consumer plastics. Such traits are essential fordepolymerization and for simplified, low-cost industrial-scale processes. To overcome these limitations, we employed deep learning and protein engineering approaches to generate a PHE that has exceptionally high activity across a broad range of raw PET substrates (both model and actual post-consumer PET, temperatures, and pH levels in a manner that out-performs all other known PHEs and rationally-derived mutants.',
    'Enzymatic depolymerization of PET was first reported in 2005 and has been nascently demonstrated using 19 distinct PHEs derived from esterases, lipases, and cutinases. However, the majority of these enzymes only show appreciable hydrolytic activity at high reaction temperatures (i.e. at or exceeding the PET glass transition temperature of ca. 70 °C) and with highly processed substrates. For example, an engineered leaf-branch compost cutinase can degrade 90% of pretreated pc-PET within 10 hours at 72 °C and a pH of 8.0. Most other PHEs similarly show poor activity at moderate temperaturesand more neutral pH conditions, greatly restricting/ microbially-enabled degradation solutions for PET waste. This limitation is of critical concern as 40% of uncollectable plastics reside in natural environments. In addition, converting untreated post-consumer plastic waste at near ambient temperature would be preferable for industrial applications, whereas elevated temperatures and pre-treatment increase net operating costs.',
    'While the PHE from the PET-assimilating bacterium(PETase) can operate at ambient conditions, it is highly labile and loses activity even at 37 °C after 24 hours, thereby limiting practical applications. Nonetheless, this mesophilic enzyme has previously seen attempts to enhance thermostability, robustness and function. The most notable engineered PETase variants—ThermoPETaseand DuraPETase—were created through rational protein engineering and computational redesign strategies, respectively. Although the thermostability and catalytic activity of these two mutants were improvedunder certain conditions, they nonetheless had overall lower PET-hydrolytic activity at mild temperatures.',
    'We posited that highly focused protein engineering approaches such as those described above cannot take into account the evolutionary trade-off between overall stability and activity, and that a neutral, structure-based, deep learning neural network might generally improve enzyme function across all conditions. To this end, we employed our 3D self-supervised, convolutional neural network, MutCompute(Supplementary Information Fig. 1) to identify stabilizing mutations. This algorithm learns the local chemical microenvironments of amino acids based on training over 19,000 sequence-diverse protein structures from the Protein Data Bank and can readily predict positions within a protein where wild-type amino acids are not optimized for their local environments. We employed MutCompute to obtain a discrete probability distribution for the structural fit of all 20 canonical amino acids at every position in both wild-type PETase and ThermoPETase (crystal structures PDB: 5XJH and 6IJ6) (Supplementary Information Fig. 2), essentially carrying out a comprehensive scanning mutagenesis of the protein. The predicted distributions were rendered onto the protein crystal structure to identify positions where wild-type amino acid residues were ‘less fit’ than potential substitutions. Predictions were then ranked by predicted probabilities (fold-change of fit) (; Supplementary Information Fig. 3). Using a stepwise combination strategy, a total of 159 single or multiple predicted mutations were generated in various PETase scaffolds. Variants exhibiting improved catalytic activity (as measured by esterase activity and plastic degradation rates) and thermostability (as measured by protein melting temperature were characterized further. Amongst this set, four predicted mutations (S121E, N233K, R224Q and T140D) resulted in the highest improvements, both singly and in combination, and were selected for further assembly and analysis (see Additional Supplementary Discussion in Supplementary Information Fig. 4 for a further discussion of the mutant down-select). Encouragingly, two substitutions (S121E and T140D) were reported in the literature after our initial predictions, whereas the remaining residues are entirely unique, thus emphasizing the importance of a neutral, deep learning-based approach to identifying critical substitutions.',
    'We assembled all 29 possible combinations using these four mutations across three PETase scaffolds (wild-type PETase, ThermoPETase, and DuraPETase). Of note, two could not be purified using the DuraPETase background after multiple attempts. Thermostability analysis of the remaining 27 mutants indicated that 24 (ca. 89%) resulted in elevated Trelative to their respective scaffolds (Supplementary Information Fig. 5). The highest change in thermostability from their respective PETase scaffolds were observed for variants PETasewith a Tof 58.1 °C (ΔT=10 °C from WT PETase), ThermoPETasewith a Tof 67.4 °C (ΔT=9 °C from ThermoPETase), and DuraPETasewith a Tof 83.5 °C (ΔT=5 °C from DuraPETase). The latter mutant represents the most thermostable PETase mutant reported to date. It was noted that the protein yield of all 27 variants was improved (up to 3.8-fold increase) compared with the parental scaffold, further underscoring the ability of Mutcompute to identify mutants of higher stability (Supplementary Information Fig. 6). The portability and combinatorial synergy of these mutations across scaffolds demonstrates the power of this neural network-based approach.',
    'Next, we sought to evaluate the PET hydrolytic activity of these more stable variants across a range of temperatures from 30 to 60 °C using an amorphous PET film (gf-PET, from the supplier Goodfellow, PA, USA) commonly used in the literature. This comparison immediately revealed that the machine-learning guided predictions greatly enhanced PET-hydrolytic activity and extended the range of working temperature in all scaffolds. In particular, PETaseexhibited a 3.4-fold and 29-fold increase in PET-hydrolytic activity at 30 and 40 °C respectively, over wild-type PETase. Enzyme mutants based on the ThermoPETase scaffold showed an extended range of working temperature (30-60 ⁰C) and exhibited significantly higher activity than their counterparts. Within this set, the best variant from the ThermoPETase scaffold (containing N233K and R224Q on top of S121E), named FAST-PETase (unctional,ctive,table, andolerant PETase), showed 2.4-fold and 38-fold higher activity at 40 and 50 ⁰C, respectively compared to ThermoPETase alone. At 50 ⁰C, FAST-PETase displayed the highest overall degradation of all mutants and temperatures activity releasing 33.8 mM of PET monomers in 96 hours. The DuraPETase scaffold in general exhibited relatively low activity at mild temperatures (30–50 ⁰C), but improvements were nevertheless realized at higher temperatures (55–60 ⁰C) as demonstrated by the most thermostable PETase mutant-DuraPETase().',
    'Crystal structure analysis of FAST-PETase at 1.44 Å resolution explains the enhanced stability through newly formed, favorable residue interactions. The N233K mutation places a positively-charged lysine next to E204 and establishes an intramolecular salt bridge. The side chain of R224, when mutated to Gln, forms a hydrogen bond to the carbonyl group of S192. Finally, the S121E mutation enables a new water-mediated hydrogen-bonding network with H186 and N172.',
    'To evaluate the catalytic resilience of these mutants to environmental conditions, FAST-PETase were compared to previously reported wild-type and mutant PHEs including wild-type PETase, ThermoPETase, DuraPETase, LCC, the most active mutant LCC(ICCM) using gf-PET across a range of pH (6.5 – 8.0) and temperatures (30-40 ⁰C) (Supplementary Information Fig. 7). This comparative analysis demonstrated the unique catalytic capability of FAST-PETase to function at low pH levels and ambient temperature. Specifically, FAST-PETase outperformed other PHEs (including prior rational designs) at all pH conditions. Especially at pH 7, FAST-PETase exhibited activities that were 9.7 and 115 times as high as that of wild-type PETase at 30 and 40 ⁰C, respectively (Supplementary Information Fig. 7). This enzymatic performance makes FAST-PETase an excellent candidate for mild temperatures and moderate pH enzymatic degradation of PET seen in conditions ofplastic degradation.',
    'Beyond model plastic substrates, it is critical to demonstrate the performance of PETase enzymes on raw, untreated pc-PET. Notably, unlike the gf-PET used above and throughout the literature, there is no singular pcPET substrate. To this end, we collected 51 samples of post-consumer plastic products used in the packaging of food, beverages, medications, office supplies, household goods and cosmetics available at local grocery store chains and treated this raw material enzymatically with FAST-PETase at 50 °C (Supplementary Information Fig. 8). Despite their heterogeneity including physical properties such as crystallinity, molecular weight, and thickness as well as different compositions including additives and plasticizers, hole-punched samples from this wide array of PET products were all fully degraded by FAST-PETase within one week and in as little as 24 hours. While thickness of the plastic did correlate with degradation time (as thickness and mass are related), neither this metric nor crystallinity or any other measured trait of PET alone determined overall degradation rates (Supplementary Information Fig. 9).',
    'Among the post-consumer products tested above, we further evaluated the sample from a bean cake container that was completely degraded by FAST-PETase within 24 hrs at 50 ⁰C. A time-course analysis revealed that the degradation of this pc-PET film exhibited an almost linear decay rate using FAST-PETase in terms of the total PET monomers released. Concomitantly, degradation of the pc-PET film by FAST-PETase brought an increase in the crystallinity from 1.2 % to 7.7% over 24 hrs (Supplementary Information Fig. 10). Atomic Force Microscopy as well as Scanning Electron Microscopy (Supplementary Information Fig. 11) further showed that the reaction progression of FAST-PETase as it produced increasingly deeper and larger holes in the pc-PET surface resulting in increased surface roughness (and visible opaqueness) over reaction time (Supplementary Information Fig. 12). In contrast, the PET-hydrolytic activity of wild-type PETase, ThermoPETase, DuraPETase, LCC and ICCM toward this pc-PET was substantially lower (3.2 to 141.6-fold) than that of FAST-PETase under the same conditions. Interestingly, even at their previously reported optimal reaction temperature of 72 ⁰C, the activity of LCC and ICCM was still 4.9-fold and 1.5-fold lower than that of FAST-PETase at 50 ⁰C. Further experimental analysis (Supplementary Information Fig. 13) indicated that LCC and ICCM exhibited their highest degradation rate against this pc-PET film at 60 ⁰C. However, even at 60⁰C, the activity of LCC and ICCM was still lower than that of FAST-PETase at 50 ⁰C. Moreover, we demonstrate that the depolymerization process with FAST-PETase is easily scalable to large, untreated pieces of plastic (in this case, 6.4 g rather than 11 mg) simply by increasing net reaction volumes. Given these results, FAST-PETase can serve as a promising biocatalyst for the enzyme-based platform aimed at recycling raw, untreated PET waste, with advantages of lower operating cost and higher degradation efficiency of pc-PET, in contrast to ICCM that requires a higher reaction temperature.',
    'Beyond packaging materials, PET is used heavily in the synthetic textile industry. To this end, we evaluated the potential application of FAST-PETase to partially degrade commercial polyester products. Five different commercial polyester products were treated with FAST-PETase at 50 ⁰C, releasing higher amounts of terephthalic acid and Mono-(2-hydroxyethyl)terephthalate relative to that of the samples treated with other PHEs. This indicates that FAST-PETase can potentially be used for rapid and efficient degradation of the PET fragments embedded in textile fabrics, providing a potential route for recovering PET monomers from commercial polyester products and reducing the leaching of microfibers into the environment.',
    'Given the high activity of this FAST-PETase mutant at ambient temperatures and pH conditions, we hypothesized that this enzyme would be suitable for various enzymatic-microbial and enzymatic-chemical processing of PET. In this regard, PET depolymerization is only half of the circular plastic economy and we demonstrate here the compatibility of FAST-PETase with both chemical and biological recycling/upcycling applications to close the cycle. First, we demonstrate a closed-cycle PET re-constitution by first depolymerizing a tinted post-consumer plastic waste utilizing FAST-PETase and subsequently recovering monomers. TPA was recovered from the degradation solution with a yield of 96.8% and with a purity of over 99%. We then regenerate virgin PET directly from the degradation solution using chemical polymerization. A complete cycle of degradation to re-polymerization can be accomplished in as little as a few days (Supplementary Information Fig. 14). These results demonstrate the feasibility of a closed-loop enzymatic/chemical recycling process to generate a clear, virgin PET film from non-petroleum resources. Moreover, this workflow bypasses the challenges of recycling mixed-color PET products.',
    'Second, we sought to utilize the degradation capability of FAST-PETase at ambient temperature to enable direct depolymerization and microbial valorization of monomers. To this end, we evaluated a simultaneous biodegradation scheme using FAST-PETase to demonstrate that this mutant enzyme is microbe-compatible. In particular, a soil bacteriaGo19capable of naturally utilizing TPA as a carbon and energy source and capable of producing polyhydroxyalkanoates was employed. Initially, we sought to combine exogenous FAST-PETase with this host to explore the possibility of simultaneous PET depolymerization and fermentation.Go19 was inoculated into a minimal medium supplemented with an unpretreated pc-PET film absent of any other carbon source. Upon adding 200 nM of purified FAST-PETase to the culture medium, growth ofGo19 was observed concomitant with the degraded pc-PET film which displayed opacity and lost 20.2 ± 2.1% of its initial weight after 4 days. Through this experiment, we observed that the TPA liberated from the hydrolysis of pc-PET film by FAST-PETase was consumed by theGo19 for growth and PHAs accumulation. In contrast, when wild-type PETase, ThermoPETase, DuraPETase, LCC, or ICCM was used as the catalyst in such process, the cell density ofGo 19 and the weight loss of the pc-PET film were all significantly lower than when FAST-PETase was used. These results demonstrated that FAST-PETase exhibited the highest PET-hydrolytic activity under cell-growth compatible conditions when compared with other PHEs tested. This demonstration represents the first simultaneous bioprocess that integrates enzymatic PET depolymerization and TPA conversion to PHAs at ambient temperatures and neutral pH.',
    'In conclusion, this work utilized a structure-based deep learning model to identify portable substitutions that imparted improved stability and function across a variety of PETase scaffolds. The best variant, FAST-PETase, exhibits superior activity over a wide range of temperatures (30–50 ⁰C), and exceptional compatibility with cell-growth conditions. We demonstrate this capacity via the rapid, efficient, and complete degradation of bulk, untreated pc-PET waste and a reduction of PET fragments embedded in textile fabrics. The properties of this variant are ultimately suitable for both low-cost industrial recycling as well as forplastic degradation applications, as demonstrated by simultaneous bioprocessing withGo 19. Collectively, these results demonstrate the potential for structure-based deep learning in protein engineering and the opportunities for converting mesophilic enzyme scaffolds into broad-range biocatalysts for a cyclic plastic economy.'
  ]
},
{
  doi: '10.1101/2023.01.16.524265',
  title: 'Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling',
  authors: [
    'Elnaggar Ahmed',
    'Essam Hazem',
    'Salah-Eldin Wafaa',
    'Moustafa Walid',
    'Elkerdawy Mohamed',
    'Rochereau Charlotte',
    'Rost Burkhard'
  ],
  abstract: 'As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google’s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.',
  body: [
    'The analogy between the syntax-semantics of natural languages and the sequence-function of proteins has revolutionized the way humans investigate the language of life. Although this analogy is intrinsically valuable when viewed as a precedent step in history leading to the adaptation of NLP’s advances on the domain of proteins (e.g., language models), conclusions from the field of NLP do not translate, fully, to protein language. Not only are NLP’s model sizes are pursued, it is even proposed that scaling-up protein language models may be significantly more impactful than scaling-up natural language models. The proportionality between the model size and the richness of its learned representations is rather -falsely-encouraged by observing language models of a massive number of parameters trained on a massive number of steps still undergoing notable learning gradient and hence perceived as under-fitted. As a result, opting for more meaningful protein representations or more accurate modeling has gradually shifted to opting for larger models and accordingly, more computational power - less accessibility. Notably, PLM sizes have jumped from ∼10[] to ∼10[] parameters recently. Shedding the light, chronologically, on protein language model state-of-the-art, we baseline our size-performance benchmark using ProtTrans’s ProtT5-XL-U50, an encoder-decoder transformer pre-trained on UniRef50 database whose number of parameters is 3B for training and 1.5B for inference. The evolution of model performance with respect to its size was then demonstrated via RITA, a family of language models taking a first step towards establishing scaling principles for protein sequence modeling. RITA showcases 4 different models with a performance-proportional increase in size from 85M, to 300M, to 680M, to 1.2B parameters. The same trend was then reinforced by ProGen2, a suite of protein language models that are trained on different sequence datasets and whose number of parameters is scaled up to 6.4B. Finally and up to the publication date of this manuscript, the latest contribution promoting model up-scaling is ESM-2, a poll of general-purpose protein language models that also showcase a performance-proportional increase in size from 650M, to 3B, to 15B parameters. The simplified relation between bigger and seemingly-better PLMs, ignores several aspects, including computational costs, task-agnostic model design, and implementation. This raises the research innovation entry barrier and constrains it to scalability. Although model size is, without a doubt, a high impact attribute in pursuing the aforementioned objectives, it is not the only one. The same direction in up-scaling pre-training datasets has proven to be conditional (i.e., bigger datasets are not necessarily better than smaller datasets of higher quality). We build upon the same direction arguing that up-scaling language models is, too, conditional (i.e., bigger models are not necessarily better than smaller models of protein knowledge-guided means of optimization). In this work, our main objective is to integrate knowledge-guided optimization in an iterative empirical framework that promotes accessibility to research innovation via attainable resources. We title our work “Ankh” (i.e. an Ancient Egyptian symbol denoting the key of life) in analogy to how our model “unlocks” the language of life via learning superior representations of its “letters”, the amino acids. This is expanded into two pieces of evidence in evaluating Ankh in terms of optimization and generality. Firstly, surpassing the performance of the SOTA in a representative range of structure and function benchmarks combined with a generation analysis for protein engineering on High-N and One-N (single sequence-based) applications, where N refers to the number of input sequences. Secondly, fulfilling this performance via a poll of optimized attributes that not only include the model design but also its development, training, and deployment software and hardware. We provide two pre-trained models referred to asand, offering two modes of computation depending on the application demands. For convenience, we refer to our main model,, as.'
  ]
},
{
  doi: '10.1101/2022.04.10.487779',
  title: 'Learning inverse folding from millions of predicted structures',
  authors: [
    'Hsu Chloe',
    'Verkuil Robert',
    'Liu Jason',
    'Lin Zeming',
    'Hie Brian',
    'Sercu Tom',
    'Lerer Adam',
    'Rives Alexander'
  ],
  abstract: 'We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues, an overall improvement of almost 10 percentage points over existing methods. The model generalizes to a variety of more complex tasks including design of protein complexes, partially masked structures, binding interfaces, and multiple states.',
  body: [
    'Designing novel amino acid sequences that encode proteins with desired properties, known as, is a central challenge in bioengineering. The most well-established approaches to this problem use an energy function which directly models the physical basis of a protein’s folded state.',
    'Recently a new class of deep learning based approaches has been proposed, using generative models to predict sequences for structures, generate backbone structures, jointly generate structures and sequences, or model sequences directly. The potential to learn the rules of protein design directly from data makes deep generative models a promising alternative to current physics-based energy functions.',
    'However, the relatively small number of experimentally determined protein structures places a limit on deep learning approaches. Experimentally determined structures cover less than 0.1% of the known space of protein sequences. While the UniRef sequence database has over 50 million clusters at 50% sequence identity; as of January 2022, the Protein Data Bank contains structures for fewer than 53,000 unique sequences clustered at the same level of identity.',
    'Here we explore whether predicted structures can be used to overcome the limitation of experimental data. With progress in protein structure prediction, it is now possible to consider learning from predicted structures at scale. Predicting structures for the sequences in large databases can expand the structural coverage of protein sequences by orders of magnitude. To train an inverse model for protein design, we predict structures for 12 million sequences in UniRef50 using AlphaFold2.',
    'We focus on the problem of predicting sequences from back-bone structures, known asor fixed back-bone design. We approach inverse folding as a sequence-to-sequence problem, using an au-toregressive encoder-decoder architecture, where the model is tasked with recovering the native sequence of a protein from the coordinates of its backbone atoms.',
    'We make use of the large number of sequences with un-known structures by adding them as additional training data, conditioning the model on predicted structures when the experimental structures are unknown. This approach parallels back-translation in machine translation, where predicted translations in one direction are used to improve a model in the opposite direction. Back-translation has been found to effectively learn from extra target data (i.e. sequences) even when the predicted inputs (i.e. structures) are of low quality.',
    'We find that existing approaches have been limited by data. While current state-of-the-art inverse folding models degrade when training is augmented with predicted structures, much larger models and different model architectures can effectively learn from the additional data, leading to an improvement of nearly 10 percentage points in the recovery of sequences for structurally held out native backbones.',
    'We evaluate models on fixed backbone design benchmarks from prior work, and assess the generalization capabilities across a series of tasks including design of complexes and binding sites, partially masked backbones, and multiple conformations. We further consider the use of the models for zero-shot prediction of mutational effects on protein function and stability, complex stability, and binding affinity.',
    'The goal of inverse folding is to design sequences that fold to a desired structure. In this work, we focus on the backbone structure without considering side chains. While each of the 20 amino acid has a specific side chain, they share a common set of atoms that make up the amino acid backbone. Among the backbone atoms, we choose the N, C(alpha Carbon), and C atom coordinates to represent the backbone.',
    'Using the structures of naturally existing proteins we can train a model for this task by supervising it to predict the protein’s native sequence from the coordinates of its backbone atoms in three-dimensional space. Formally we represent this problem as one of learning the conditional distribution(|), where for a protein of length, given a sequenceof spatial coordinates for each of the backbone atomsin the structure, the objective is to predictthe native sequence of amino acids. This density is modeled autoregressively through a sequence-to-sequence encoder-decoder:We train a model by minimizing the negative log likelihood of the data. We can design sequences by sampling, or by finding sequences that maximize the conditional probability given the desired structure.',
    'We evaluate models across a variety of benchmarks in two overall settings: fixed backbone sequence design and zeroshot prediction of mutation effects. For fixed backbone design, we start with evaluation in the standard setting of sequence design given all backbone coordinates. Then, we make the sequence design task more challenging along three dimensions: introducing masking on coordinates; generalization to protein complexes; and conditioning on multiple conformations. Additionally, we show that inverse folding models are effective zero-shot predictors for protein complex stability, binding affinity, and insertion effects.',
    'While there are billions of protein sequences in the largest sequence databases, the number of available experimentally determined structures is on the order of hundreds of thousands, imposing a limit on generative methods that learn from protein structure data. In this work, we explored whether predicted structures from recent deep learning methods can be used in tandem with experimental structures to train models for protein design.',
    'To this end, we generated structures for 12 million UniRef50 sequences using AlphaFold2. As a result of training with this data we observe improvements in perplexity and sequence recovery by substantial margins, and demonstrate generalization to longer protein complexes, to proteins in multiple conformations, and to zero-shot prediction for mutation effects on binding affinity and AAV packaging. These results highlight that in addition to the geometric inductive biases which have been the major focus for work on inverse-folding to date, finding ways to leverage more sources of training data is an equally important path to improved modeling capabilities.',
    'We also take initial steps toward more general structure-conditional protein design tasks. By integrating backbone span masking into the inverse folding task and using a sequence-to-sequence transformer, reasonable sequence predictions can be achieved for short masked spans.',
    'If ways can be found to continue to leverage predicted structures for generative models of proteins, it may be possible to create models that learn to design proteins from an expanded universe of the billions of natural sequences whose structures are currently unknown.'
  ]
},
{
  doi: '10.1101/2022.10.07.502662',
  title: 'Machine Learning Optimization of Candidate Antibodies Yields Highly Diverse Sub-nanomolar Affinity Antibody Libraries',
  authors: [
    'Li Lin',
    'Gupta Esther',
    'Spaeth John',
    'Shing Leslie',
    'Jaimes Rafael',
    'Caceres Rajmonda Sulo',
    'Bepler Tristan',
    'Walsh Matthew E.'
  ],
  abstract: 'Therapeutic antibodies are an important and rapidly growing drug modality. However, the design and discovery of early-stage antibody therapeutics remain a time and cost-intensive endeavor. In this work, we present an end-to-end Bayesian, language model-based method for designing large and diverse libraries of high-affinity single-chain variable fragments (scFvs). We integrate target-specific binding affinities with information from millions of natural protein sequences in a probabilistic machine learning framework to design thousands of scFvs that are then empirically measured. In a head-to-head comparison with a directed evolution approach, we show that the best scFv generated from our method represents a 28.8-fold improvement in binding over the best scFv from the directed evolution. Additionally, 99% of the designed scFvs in our most successful library are improvements over the initial candidate scFv. By comparing a library’s predicted success to actual measurements, we demonstrate our method’s ability to explore tradeoffs between library success and diversity during the design phase and prior to experimental testing. The results of our work highlight the significant impact machine learning models can have on scFv development. We expect our end-to-end method to be broadly applicable and able to provide value to other protein engineering tasks.',
  body: [
    'Therapeutic antibodies are an important and rapidly growing drug modality. Because the vast search space of antibody sequences renders exhaustive evaluation of the entire antibody space infeasible, screening relatively small numbers of antibodies from synthetic generation, animal immunizations or human donors is used to identify candidate antibodies. The screened library represents a small portion of the overall search space, and the resultant candidate antibodies are often weak binders or suffer from developability issues. Optimization of these candidates is needed to improve binding and other development characteristics.',
    'Due to the combinatorial scaling of sequence space, step-wise, iterative approaches are often used to optimize antibody binding against target molecules,, but are time consuming and effort is wasted interrogating nonfunctional antibodies. Improved binders may need to be further altered to improve other properties, such as hydrophobicity,, but such alterations can negatively influence the previously optimized binding, resulting in additional measurement and engineering cycles. This process of identifying the final antibody routinely takes about 12-months to complete. The ability to efficiently engineer antibodies with favorable binding and high diversity earlier in the development process would reduce the impact of unfavorable antibody characteristics that are often identified later in the process, improve the developability potential and reduce the time required in early drug development.',
    'While computational methods can guide the search of biologically relevant antibodies, mostapproaches require target structures or antibody-epitope complex structures to be known. Machine learning approaches can be used to effectively represent biological data and rapidly explore their vast design spaces in silico. Such approaches can uncover complex and flexible features from high-dimensional data and have shown great promise in many application areas, including protein structure prediction, and drug discovery and design. Similarly, existing ML-driven antibody optimization has shown promising results in designing antibodies with improved binding characteristics against a target and that antibody binding can be learned from only sequence data and without the need for the target’s structure. A more recent work has presented an ML-driven antibody optimization approach that achieves broader neutralizing activity against diverse SARS-CoV-2 variants by learning the mutational effect on protein-protein interactions from protein complex structures. Other works have investigated general purpose pre-trained generative language models for designing antibody libraries that display good physical properties,, but these methods are not target-specific and only offer modest improvements over conventional libraries that are, often, already based on natural antibody repertoires. Finally, none of the existing work allows the evaluation of designed antibody libraries prior to experimentation, a critical feature that allows for accelerated design cycles.',
    'In this work, we develop an end-to-end ML-driven single-chain variable fragment design framework that uniquely combines state-of-art language models, Bayesian optimization and high-throughput experimentation. Because we synthesize explicitly defined oligo pools of 200bp, our method allows the design of the entire scFv chain (heavy or light). Furthermore, it does not assume candidate scFvs strongly bind to the target, and relies on sequence data without the need for sequence alignments or knowledge of the target antigen structure, allowing the method to be applicable to early-stage antibody development for any target antigen. We demonstrate our end-to-end framework can rapidly and cost-effectively lead to the design of diverse target-specific scFv libraries with therapeutically relevant binding affinities. At a meaningful scale, and in a head-to-head comparison with the directed evolution approach, we show that our ML-based approach produces significantly stronger binders. More remarkably, our ML-designed scFv libraries are highly diverse, demonstrating the ability of our approach to efficiently extrapolate and discover mutationally distant, high affinity scFvs. Lastly, we show how our method can provide general insights to the engineering process. We can evaluate the performance of an scFv library in silico, explore the affinity-diversity tradeoff prior to experimental testing, weigh the choice of optimizing CDRs jointly or individually, and combine our method with other software tools to explore other desired development properties, such as hydrophobicity and isoelectric point, of scFvs in designed libraries. Our results highlight the impact ML models can have on early-stage scFv development. Through coordinated data generation, ML model development, training and optimization, we are able to start with only a target protein sequence and after a single round of optimization, generate large, diverse libraries of high-affinity scFvs against the target.',
    'We demonstrate for the first time, in a head-to-head comparison with a conventional directed evolution strategy, scFvs designed with our ML approach are significantly stronger binders, especially at high levels of diversity, where, remarkably, our models are able to accurately predict binding affinity for extremely high order mutants. The libraries generated through our method have diverse biophysical properties (Supplementary Fig. 11). This allows for the selection of multiple preclinical candidates, uncorrelated in their downstream failure modes, such that if one fails, the entire pipeline is not likely to fail for the same reason. We also believe that our framework is applicable to any task aiming to maximize or minimize a characteristic of an scFv, such as minimizing off-target binding or maximizing neutralization. Pending data availability, we see ML-based multi-objective scFv optimization as an approachable task and viable option for streamlining scFv development.',
    'We separately explored our model performance as a function of the amount of training data and demonstrated additional data, expectedly, results in improved performance. However, after about 7,000 measurements, additional measurements result in less significant performance increases. For this work, we trained our supervised sequence-to-affinity models on all 43,341 measurements that were available to us, but future engineering attempts may optimize use of financial resources by increasing the number of cycles while reducing the number of measurements per cycle. Because of cost limitations associated with DNA synthesis, we chose to generate our training data by introducing k = 1, 2, or 3 random mutations, but our models are able to successfully extrapolate much further than that. Future work would benefit from an improved understanding of the way in which training data is generated, if there is dependence on the choice of model, and if performing multiple measurement cycles impacts the choice.',
    'Recently, other works have presented general purpose pre-trained generative language models for antibody design,. By training on natural antibody repertoires, Shin. were able to design antibody libraries that display good physical properties and are enriched for binders. In the future, our approach can be combined with these by using a pre-trained generative language model to design the initial mutagenesis library used for training our supervised learning approach. Our initial analysis indicates that this approach is likely to increase the success rate of the initial library by several fold. Furthermore, pre-trained models could also condition on features of the target epitope to design target-specific initial libraries that are then fine-tuned with our framework.',
    'We demonstrate the ability to rapidly design large libraries of potently binding scFvs, but our framework also extends to other domains of protein engineering where large scale functional mutagenesis screens are being applied. Our framework is neither scFv nor binding-specific and, therefore, can be applied to engineer other proteins for other functional properties. We expect machine learning approaches like ours, combined with high throughput mutagenesis screens, will soon become the standard in protein engineering.'
  ]
}
]

const asd = async () => {
  const urlResponse = await fetch("https://faas-ams3-2a2df116.doserverless.co/api/v1/web/fn-c5c8b0ac-65ad-4431-a584-56e7b664e9a6/slack/get_urls")
  const urls = await urlResponse.json()
  const articleJSONArray = []
  for (const articleUrl of urls) {
    const biorxivApiResponse = await fetch (articleUrl.replace('https://www.biorxiv.org/content/', 'https://api.biorxiv.org/details/biorxiv/').replace(/v1$/, ''))
    await new Promise(r => setTimeout(r, 4000));
    const biorxivApi = await biorxivApiResponse.json()
    if (biorxivApi.messages[0].status === 'ok' && biorxivApi.collection[0]) {
      const { doi, title, date, category, abstract, jatsxml } = biorxivApi.collection[0]
      try {
        const articleJSONResponse = await fetch(
          'https://us-central1-grounded-garage-377420.cloudfunctions.net/parse_article', 
          { 
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({ url: jatsxml })
          }
        )
        const articleJSON = await articleJSONResponse.json()
        articleJSONArray.push(articleJSON)
      } catch (e) {
        console.warn(e)
      }
    }
  }

  const summaryResponses = ARTICLES.map((article) =>  fetch("http://164.90.183.32:5000/openai/summarize", {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ articles: [article], prompts: ["Write a podcast transcript of two people talking about the above article. The two people should discuss the main takeaways, some strong and weak points, and end forward looking. Each person should start speaking on a new line.\nPerson 1:"] })
    })
  )

  Promise.all(summaryResponses).then(async (summaryResponses) => {
    const summaries = []
    for (const response of summaryResponses) {
      const json = await response.json()
      summaries.push(json.body.responses[0].response)
    }

    const conversation = summaries.map((summary, i) => {
      if (i === 0) {
        return `Person 1: Welcome to another addition of the Cradle Nerd podcast! Our first article is: ${ARTICLES[i].title}. Person 1: ${summary}`
      }
      if (i < summaries.length - 1) {
        return `Person 1: Moving onto our next article, ${ARTICLES[i].title}. Person 1: ${summary}`
      }
      return `Person 1: Our last episode for today is ${ARTICLES[i].title}. Person 1: ${summary}. Person 1: Thanks for listening to this episode of the Cradle Nerd podcast. Please don't forget that the script was fully generated by AI, so take everything we said with a grain of salt. Person 2: This podcast couldn't have been created without the technologies of OpenAI and play dot H T. Stay tuned until the next episode, and keep on learning!`
    }).join('\n').replace(/\n/gm, '').replace(/ANSWER: \d./gm, '').split(/(Person \d+: ?)/gm)

    const convArray = []
    conversation.forEach((str) => {
      if (str && str.length > 0) {
          if (str.includes('Person')) {
              convArray.push({ voice: str.replace(/:\s*/g, '') })
          } else if (convArray[convArray.length - 1]) {
              convArray[convArray.length - 1].text = str
          }
      }
    })
    const cleanedConv = convArray.filter((entry) => entry.text).map((entry) => ({
      text: entry.text.replace(/Person 1/gm, 'Larry').replace(/Person 2/gm, 'Susan'),
      voice: entry.voice === 'Person 1' ? 'Larry' : 'Susan'
    }))

    const res = await fetch('https://text-to-speech-6rmz6mmrha-ez.a.run.app', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ body: { conversation: cleanedConv }} )
    })

    const transcript_text = cleanedConv.map(({ text, voice }) => `${voice}: ${text}`).join('\n')
    const podcast_audio_url = await res.text()
    const data = {
        "podcast_title" : "Week 10",
        "podcast_summary" : "Top papers from week 10",
        "podcast_transcript_text" : transcript_text,
        podcast_audio_url
    }
  
    const postEpisodeURL = 'https://faas-ams3-2a2df116.doserverless.co/api/v1/web/fn-c5c8b0ac-65ad-4431-a584-56e7b664e9a6/transistor/publish_episode'
    const podcastResponse = await fetch(postEpisodeURL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(data)
    })

    const { share_url } = await podcastResponse.json()
    const slackUrl = 'https://faas-ams3-2a2df116.doserverless.co/api/v1/web/fn-c5c8b0ac-65ad-4431-a584-56e7b664e9a6/slack/post_podcast'
    const success = await fetch(slackUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        "message" : share_url
      })
    })
    return await success.text()
  })
}

asd()