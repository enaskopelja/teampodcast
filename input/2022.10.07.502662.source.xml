<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2022.10.07.502662</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.10.07.502662v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2022.10.07.502662</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2022.10.07.502662</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2022.10.07.502662</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2022.10.07.502662</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Machine Learning Optimization of Candidate Antibodies Yields Highly Diverse Sub-nanomolar Affinity Antibody Libraries</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author; email: <email hwp:id="email-1">lin.li@ll.mit.edu</email></corresp><fn id="n1" fn-type="equal" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2 xref-fn-1-3"><label>†</label><p hwp:id="p-1">These authors contributed equally to this work.</p></fn><fn id="n2" fn-type="others" hwp:id="fn-2"><p hwp:id="p-2">E-mail: <email hwp:id="email-2">Lin.Li@LL.MIT.EDU</email></p></fn><fn id="n3" fn-type="supported-by" hwp:id="fn-3"><label>1</label><p hwp:id="p-3">DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering. © 2022 Massachusetts Institute of Technology. Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014). Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-4087-6149</contrib-id><name name-style="western" hwp:sortable="Li Lin"><surname>Li</surname><given-names>Lin</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-4087-6149"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Gupta Esther"><surname>Gupta</surname><given-names>Esther</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Spaeth John"><surname>Spaeth</surname><given-names>John</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-3" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Shing Leslie"><surname>Shing</surname><given-names>Leslie</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-4" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Jaimes Rafael"><surname>Jaimes</surname><given-names>Rafael</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-5" hwp:rel-id="aff-1">1</xref></contrib><contrib contrib-type="author" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Caceres Rajmonda Sulo"><surname>Caceres</surname><given-names>Rajmonda Sulo</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-6" hwp:rel-id="aff-1">1</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">†</xref></contrib><contrib contrib-type="author" hwp:id="contrib-7"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5595-9954</contrib-id><name name-style="western" hwp:sortable="Bepler Tristan"><surname>Bepler</surname><given-names>Tristan</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">†</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0001-5595-9954"/></contrib><contrib contrib-type="author" hwp:id="contrib-8"><name name-style="western" hwp:sortable="Walsh Matthew E."><surname>Walsh</surname><given-names>Matthew E.</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-7" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a4" hwp:id="xref-aff-4-1" hwp:rel-id="aff-4">4</xref><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-3" hwp:rel-id="fn-1">†</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2 xref-aff-1-3 xref-aff-1-4 xref-aff-1-5 xref-aff-1-6 xref-aff-1-7"><label>1</label><institution hwp:id="institution-1">Massachusetts Institute of Technology Lincoln Laboratory</institution>, Lexington, MA, <country>USA</country></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1"><label>2</label><institution hwp:id="institution-2">Research Laboratory of Electronics, Massachusetts Institute of Technology</institution>, Cambridge, MA, <country>USA</country></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Simons Electron Microscopy Center, New York Structural Biology Center</institution>, New York, NY, <country>USA</country></aff><aff id="a4" hwp:id="aff-4" hwp:rev-id="xref-aff-4-1"><label>4</label><institution hwp:id="institution-4">Johns Hopkins Bloomberg School of Public Health</institution>, Baltimore, MD, <country>USA</country></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2022"><year>2022</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2022-10-07T15:27:12-07:00">
    <day>7</day><month>10</month><year>2022</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2022-10-07T15:27:12-07:00">
    <day>7</day><month>10</month><year>2022</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2022-10-07T15:40:57-07:00">
    <day>7</day><month>10</month><year>2022</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2022-10-07T15:40:57-07:00">
    <day>7</day><month>10</month><year>2022</year>
  </pub-date><elocation-id>2022.10.07.502662</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2022-10-07"><day>07</day><month>10</month><year>2022</year></date>
<date date-type="rev-recd" hwp:start="2022-10-07"><day>07</day><month>10</month><year>2022</year></date>
<date date-type="accepted" hwp:start="2022-10-07"><day>07</day><month>10</month><year>2022</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2022, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2022</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-4">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="502662.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2022/pdf/2022.10.07.502662v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="502662.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2022/abstracts/2022.10.07.502662v1/2022.10.07.502662v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2022/fulltext/2022.10.07.502662v1/2022.10.07.502662v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-5">Therapeutic antibodies are an important and rapidly growing drug modality. However, the design and discovery of early-stage antibody therapeutics remain a time and cost-intensive endeavor. In this work, we present an end-to-end Bayesian, language model-based method for designing large and diverse libraries of high-affinity single-chain variable fragments (scFvs). We integrate target-specific binding affinities with information from millions of natural protein sequences in a probabilistic machine learning framework to design thousands of scFvs that are then empirically measured. In a head-to-head comparison with a directed evolution approach, we show that the best scFv generated from our method represents a 28.8-fold improvement in binding over the best scFv from the directed evolution. Additionally, 99% of the designed scFvs in our most successful library are improvements over the initial candidate scFv. By comparing a library’s predicted success to actual measurements, we demonstrate our method’s ability to explore tradeoffs between library success and diversity during the design phase and prior to experimental testing. The results of our work highlight the significant impact machine learning models can have on scFv development. We expect our end-to-end method to be broadly applicable and able to provide value to other protein engineering tasks.</p></abstract><counts><page-count count="17"/></counts><custom-meta-wrap><custom-meta hwp:id="custom-meta-1"><meta-name>special-property</meta-name><meta-value>contains-inline-supplementary-material</meta-value></custom-meta></custom-meta-wrap></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-6">Tristan Bepler is the co-founder and CEO of NE47 Bio Inc., a company that provides machine learning services and software for protein engineering. MIT has filed a provisional patent application on certain described methods.</p></notes></notes></front><body><sec id="s1" hwp:id="sec-1"><title hwp:id="title-3">Introduction</title><p hwp:id="p-7">Therapeutic antibodies are an important and rapidly growing drug modality. Because the vast search space of antibody sequences renders exhaustive evaluation of the entire antibody space infeasible, screening relatively small numbers of antibodies from synthetic generation, animal immunizations or human donors is used to identify candidate antibodies. The screened library represents a small portion of the overall search space, and the resultant candidate antibodies are often weak binders or suffer from developability issues. Optimization of these candidates is needed to improve binding and other development characteristics.</p><p hwp:id="p-8">Due to the combinatorial scaling of sequence space, step-wise, iterative approaches are often used to optimize antibody binding against target molecules [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>], [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>], but are time consuming and effort is wasted interrogating nonfunctional antibodies. Improved binders may need to be further altered to improve other properties, such as hydrophobicity [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>], [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>], but such alterations can negatively influence the previously optimized binding, resulting in additional measurement and engineering cycles. This process of identifying the final antibody routinely takes about 12-months to complete [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>]. The ability to efficiently engineer antibodies with favorable binding and high diversity earlier in the development process would reduce the impact of unfavorable antibody characteristics that are often identified later in the process, improve the developability potential and reduce the time required in early drug development.</p><p hwp:id="p-9">While computational methods can guide the search of biologically relevant antibodies, most <italic toggle="yes">de novo</italic> approaches require target structures or antibody-epitope complex structures to be known [<xref ref-type="bibr" rid="c5" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">5</xref>]–[<xref ref-type="bibr" rid="c7" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">7</xref>]. Machine learning (ML) approaches can be used to effectively represent biological data and rapidly explore their vast design spaces in silico. Such approaches can uncover complex and flexible features from high-dimensional data [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>]–[<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>] and have shown great promise in many application areas, including protein structure prediction [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>], and drug discovery and design [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>]–[<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>]. Similarly, existing ML-driven antibody optimization has shown promising results in designing antibodies with improved binding characteristics against a target and that antibody binding can be learned from only sequence data and without the need for the target’s structure [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>]. A more recent work has presented an ML-driven antibody optimization approach that achieves broader neutralizing activity against diverse SARS-CoV-2 variants by learning the mutational effect on protein-protein interactions from protein complex structures [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">20</xref>]. Other works have investigated general purpose pre-trained generative language models for designing antibody libraries that display good physical properties [<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>], [<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>], but these methods are not target-specific and only offer modest improvements over conventional libraries that are, often, already based on natural antibody repertoires. Finally, none of the existing work allows the evaluation of designed antibody libraries prior to experimentation, a critical feature that allows for accelerated design cycles.</p><p hwp:id="p-10">In this work, we develop an end-to-end ML-driven single-chain variable fragment (scFv) design framework that uniquely combines state-of-art language models, Bayesian optimization and high-throughput experimentation (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1</xref>). Because we synthesize explicitly defined oligo pools of 200bp, our method allows the design of the entire scFv chain (heavy or light). Furthermore, it does not assume candidate scFvs strongly bind to the target, and relies on sequence data without the need for sequence alignments or knowledge of the target antigen structure, allowing the method to be applicable to early-stage antibody development for any target antigen. We demonstrate our end-to-end framework can rapidly and cost-effectively lead to the design of diverse target-specific scFv libraries with therapeutically relevant binding affinities. At a meaningful scale (~10<sup>4</sup> sequences), and in a head-to-head comparison with the directed evolution approach, we show that our ML-based approach produces significantly stronger binders. More remarkably, our ML-designed scFv libraries are highly diverse, demonstrating the ability of our approach to efficiently extrapolate and discover mutationally distant, high affinity scFvs. Lastly, we show how our method can provide general insights to the engineering process. We can evaluate the performance of an scFv library in silico, explore the affinity-diversity tradeoff prior to experimental testing, weigh the choice of optimizing CDRs jointly or individually, and combine our method with other software tools to explore other desired development properties, such as hydrophobicity and isoelectric point, of scFvs in designed libraries. Our results highlight the impact ML models can have on early-stage scFv development. Through coordinated data generation, ML model development, training and optimization, we are able to start with only a target protein sequence and after a single round of optimization, generate large, diverse libraries of high-affinity scFvs against the target.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2 xref-fig-1-3 xref-fig-1-4 xref-fig-1-5 xref-fig-1-6 xref-fig-1-7 xref-fig-1-8 xref-fig-1-9 xref-fig-1-10"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.10.07.502662v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Fig. 1</label><caption hwp:id="caption-1"><title hwp:id="title-4">Illustration of the end-to-end ML-driven antibody design process.</title><p hwp:id="p-11"><bold>A</bold>. The training data is generated via random mutations of the candidate scFv antibody along the entire CDR region, followed by high-throughput binding quantification. <bold>B</bold>. This training data combined with publicly available protein sequences is used to train, refine and evaluate ML models that drive the in silico sequence design process. <bold>C</bold>. The designed libraries are experimentally validated. ML-driven designs produce highly diverse antibodies (sequences as far as 23 mutations away), with strong on-target binding (the best design is 2818% better than the directed evolution approach), and high success rate (as high as 99%). <bold>D</bold>. Detailed ML-driven design process: (1) supervised fine-tuning of pretrained language models on the training data to predict binding affinities with uncertainty quantification; (2) in silico scFv antibody design via Bayesian optimization over ML-extrapolated fitness landscape; (3) in silico antibody library evaluation</p></caption><graphic xlink:href="502662v1_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig></sec><sec id="s2" hwp:id="sec-2"><title hwp:id="title-5">Results</title><sec id="s2a" hwp:id="sec-3"><title hwp:id="title-6">Development of an end-to-end, target-specific scFv optimization process</title><p hwp:id="p-12">We hypothesized that by integrating target-specific binding affinities with information from millions of natural protein sequences in a probabilistic machine learning framework, we could rapidly engineer scFvs that are significantly stronger binders than what typical directed evolution approaches would produce. To engineer a given candidate scFv against the target molecule, we developed a five-step process that uniquely combines state-of-art language models, Bayesian optimization and high-throughput experimentation to generate high-affinity scFv libraries (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1</xref> and Methods section):</p><list list-type="order" hwp:id="list-1"><list-item hwp:id="list-item-1"><p hwp:id="p-13">High-throughput binding quantification of random mutants of the candidate scFv to create supervised training data (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-3" hwp:rel-id="F1">Fig. 1A</xref>).</p></list-item><list-item hwp:id="list-item-2"><p hwp:id="p-14">Unsupervised pre-training of language models [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>], [<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>] on large numbers of protein sequences to distill biologically relevant information and represent scFv sequences (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-4" hwp:rel-id="F1">Fig. 1B</xref> and <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-5" hwp:rel-id="F1">1D</xref>).</p></list-item><list-item hwp:id="list-item-3"><p hwp:id="p-15">Supervised fine-tuning of pretrained language models on the training data to predict binding affinities with uncertainty quantification (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-6" hwp:rel-id="F1">Fig. 1B</xref> and <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-7" hwp:rel-id="F1">1D</xref>).</p></list-item><list-item hwp:id="list-item-4"><p hwp:id="p-16">Construction of Bayesian-based scFv fitness landscape extrapolated from the trained sequence-to-affinity model, followed by in silico scFv design via Bayesian optimization and in silico design validation (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-8" hwp:rel-id="F1">Fig. 1B</xref> and <xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-9" hwp:rel-id="F1">1D</xref>).</p></list-item><list-item hwp:id="list-item-5"><p hwp:id="p-17">Experimental validation of top scFv sequences that are in silico predicted to have strong binding affinities (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-10" hwp:rel-id="F1">Fig. 1C</xref>).</p></list-item></list><p hwp:id="p-18">We generated our supervised training data using an engineered yeast mating assay and have published it separately to support its reuse [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>]. This data includes 17,118 heavy chain scFv binding measurements and 26,223 light chain scFv binding measurements against a single target peptide and were collected in pooled, yeast-based mating assays. All heavy- and light-chain sequences in this data were generated by performing random k=1,2,3 mutations of the candidate antibody Ab-14 along the entire CDR within its respective chain. The binding measurements are provided on a log-scale, with lower values indicating stronger binding.</p><p hwp:id="p-19">We pre-trained four BERT masked language models, i.e., a protein language model, an antibody heavy chain model, an antibody light chain model and a paired heavy-light chain model. The protein language model was trained on the Pfam data [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>], and antibody-specific language models were trained on human naïve antibodies from the Observed Antibody Space (OAS) database [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>].</p><p hwp:id="p-20">To train sequence-to-affinity models, we investigated two approaches to predict affinities with uncertainty quantification: an ensemble method and Gaussian Process (GP). Both approaches use learned knowledge from pre-trained language models and provide meaningful sequence-to-affinity models from which one can design high-affinity scFv libraries. We trained separate sequence-to-affinity models for Ab-14-H heavy-chain variants and Ab-14-L light-chain variants using the corresponding training data. We observed strong positive correlation between predicted and experimentally measured binding affinities on the hold-out test data (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Fig. 4A</xref>).</p><p hwp:id="p-21">To generate high-affinity scFv libraries, a Bayesian-based fitness landscape was constructed to map the entire scFv sequence to a posterior probability, i.e., the probability that the estimated binding affinity is better than the candidate scFv Ab-14. This is in contrast to the fitness landscape that goes directly from sequence to estimated binding affinity. To perform optimization to maximize the posterior probability, the choice of sampling algorithm is critical in determining the library diversity. Three strategies were used: hill climb (HC), genetic algorithm (GA) and Gibbs sampling. HC is a greedy algorithm that performs a local search and only finds local maximums. GA is an evolutionary-based algorithm that is more robust in exploiting the sequence space further away from the initial sequence. Gibbs sampling takes sequential actions in a manner that balances exploitation and exploration and can generate sequences with high diversity.</p><p hwp:id="p-22">We applied our sampling approaches to generate heavy chain and light chain variant scFvs that optimize Ab-14. We also used a Position-Specific Score Matrix (PSSM)-based method representative of traditional directed evolution approaches to generate a control sequence set. The generated sequences from each method are rank-ordered based on the posterior probability and top sequences are selected. This resulted in seven scFv libraries per chain: three libraries from optimizing the ensemble-based fitness function (namely, En-HC, En-GA and En-Gibbs), three libraries from optimizing the GP-based fitness function (namely, GP-HC, GP-GA, GP-Gibbs), and one PSSM library. As a sanity check, we also generated scFv mutants with an average of k=2 random mutations from the 10 strongest binders of the supervised training data. All sequences were synthesized and experimentally tested using the same high-throughput yeast display method as for the training data generation; Supplementary Table 5 and 6 provide the exact number of sequences from each library.</p></sec><sec id="s2b" hwp:id="sec-4"><title hwp:id="title-7">ML-generated ScFv Libraries Outperform Conventional Directed Evolution</title><p hwp:id="p-23">We assessed the quality of each ML-derived scFv library by comparing the binding strength of the best design and the percent of success to the PSSM-generated library. We define the percent of success as the percent of scFvs that have a better binding score than the initial scFv, Ab-14. We chose PSSM libraries as comparators because they better reflect the traditional optimization process and are generally better than random mutation libraries (Supplementary Fig. 5). <bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref></bold> contains characterization of the best binding scFv from each library. The sequences of these scFvs can be found in the supplementary material (Tables 7 and 8). The best scFvs from ML-optimized libraries are significantly stronger binders than those from the PSSM library, and generally have more mutations. The strongest binding heavy chain design is from the En-Gen library and binds 28.8-fold stronger than the strongest scFv in the PSSM library. The best light-chain design is in the En-Gibbs library achieving a 7.8-fold improvement over the best scFv from the PSSM library. Note that the best heavy-chain scFv binds a lot stronger to the target than the best light-chain scFv. To investigate further, we rank-ordered all designed scFvs across different libraries by the empirically-measured binding affinity and observed that heavy-chain designs are generally stronger binders than light-chain designs (Supplementary Fig. 10A).</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2 xref-table-wrap-1-3 xref-table-wrap-1-4 xref-table-wrap-1-5 xref-table-wrap-1-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.10.07.502662v1/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-2"><p hwp:id="p-24">Characterization of the top scFv from each library.</p></caption><graphic xlink:href="502662v1_tbl1" position="float" orientation="portrait" hwp:id="graphic-2"/></table-wrap><p hwp:id="p-25"><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Fig. 2</xref> shows the performance and diversity of designed libraries. For Ab-14-H heavy chain designs, with the exception of sequences in the En-Gibbs library, all ML-optimized libraries outperform the PSSM library in terms of median binding affinity (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Fig. 2A</xref>), and are significantly more successful than the 23.8% success of the PSSM library (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Fig 2B</xref>). The En-HC (94.3%) and En-GA (96%) libraries are particularly successful and outperform all GP-generated libraries (59.4% - 84.2%). For the Ab-14-L light chain designs, all ML-optimized libraries outperform the PSSM library in both median binding (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2D</xref>) and percent of success where the PSSM library is 45.6% successful (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig. 2E</xref>). The percent of success of GP-based libraries (95.7-99%) further outperforms all ensemble-based libraries (67.9-73.5%).</p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6 xref-fig-2-7 xref-fig-2-8 xref-fig-2-9 xref-fig-2-10 xref-fig-2-11 xref-fig-2-12 xref-fig-2-13 xref-fig-2-14 xref-fig-2-15 xref-fig-2-16"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.10.07.502662v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Fig. 2</label><caption hwp:id="caption-3"><title hwp:id="title-8">ML-optimized antibody libraries outperform the PSSM directed evolution approach with high percentage of success and high diversity.</title><p hwp:id="p-26">The binding quantification assay was conducted from six technical replicates. A replicate with an empty measured affinity value indicates that it was beyond the limit of detection and was deemed poor binders. For sequences that have at least 3 (out of 6) empirical binding affinities, we use the averaged values as ground-truth measured affinities. The rest of the sequences (with less than 3 empirical measurements) are automatically considered as un-successful designs. (A) Measured affinity distribution of Ab-14-H heavy chain designs (excluding sequences with no ground-truth affinity value). Affinities of unsuccessful sequences are set to be 5.48 (the largest assay value of all Ab-14-H variants). (B) Percent of sequences that have stronger binding affinity than the candidate antibody for all the Ab-14-H variant libraries. (C) Diversity comparison for all the Ab-14-H variant libraries. (D) Measured affinity distribution of Ab-14-L light chain designs. Affinities of unsuccessful sequences are set to be 5.53 (the largest assay value of all Ab-14-L variants). (E) Percent of success for all the Ab-14-L variant libraries. (F) Diversity comparison for all the Ab-14-L variant libraries.</p></caption><graphic xlink:href="502662v1_fig2" position="float" orientation="portrait" hwp:id="graphic-3"/></fig></sec><sec id="s2c" hwp:id="sec-5"><title hwp:id="title-9">ML-generated Libraries can be Highly Diverse</title><p hwp:id="p-27">We measured the library diversity using two mutational distance metrics: <inline-formula hwp:id="inline-formula-1"><alternatives hwp:id="alternatives-1"><inline-graphic xlink:href="502662v1_inline1.gif" hwp:id="inline-graphic-1"/></alternatives></inline-formula> (the average distance to the initial Ab-14), and <italic toggle="yes">d<sub>pw</sub></italic> (the average pairwise distance). The former <inline-formula hwp:id="inline-formula-2"><alternatives hwp:id="alternatives-2"><inline-graphic xlink:href="502662v1_inline2.gif" hwp:id="inline-graphic-2"/></alternatives></inline-formula> indicates how far the designs are from the training data and the latter <italic toggle="yes">d<sub>pw</sub></italic> measures the intra-library diversity. For Ab-14-H variant designs, all ML-optimized libraries have higher <inline-formula hwp:id="inline-formula-3"><alternatives hwp:id="alternatives-3"><inline-graphic xlink:href="502662v1_inline3.gif" hwp:id="inline-graphic-3"/></alternatives></inline-formula> than the PSSM library (with <inline-formula hwp:id="inline-formula-4"><alternatives hwp:id="alternatives-4"><inline-graphic xlink:href="502662v1_inline4.gif" hwp:id="inline-graphic-4"/></alternatives></inline-formula>). The ensemble-based libraries also have significantly higher <inline-formula hwp:id="inline-formula-5"><alternatives hwp:id="alternatives-5"><inline-graphic xlink:href="502662v1_inline5.gif" hwp:id="inline-graphic-5"/></alternatives></inline-formula> (7.9 - 15.6) than the GP-based libraries (3.4-3.7), indicating that the methods are able to extrapolate and design sequences that are far beyond the training data (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Fig. 2C</xref>). In particular, sequences in the En-Gibbs library are on average 15.6 distance away from Ab-14-H and 14.9 distance away from each other (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-7" hwp:rel-id="F2">Fig. 2C</xref>). However, this increase in mutational distance comes at the cost of reduced affinity, suggesting that there is eventually a tradeoff between the two.</p><p hwp:id="p-28">For Ab-14-L variant designs, all ML-optimized libraries are significantly further away from Ab-14-L than the PSSM library, with <inline-formula hwp:id="inline-formula-6"><alternatives hwp:id="alternatives-6"><inline-graphic xlink:href="502662v1_inline6.gif" hwp:id="inline-graphic-6"/></alternatives></inline-formula> for the PSSM library, <inline-formula hwp:id="inline-formula-7"><alternatives hwp:id="alternatives-7"><inline-graphic xlink:href="502662v1_inline7.gif" hwp:id="inline-graphic-7"/></alternatives></inline-formula> ranging from 4.3 to 7.4 for GP-based libraries and <inline-formula hwp:id="inline-formula-8"><alternatives hwp:id="alternatives-8"><inline-graphic xlink:href="502662v1_inline8.gif" hwp:id="inline-graphic-8"/></alternatives></inline-formula> ranging from 12.4 to 21.3 for ensemble-based libraries (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-8" hwp:rel-id="F2">Fig. 2F</xref>). With the exception of GP-GA (<italic toggle="yes">d<sub>pw</sub></italic>=4.5), all ML-optimized libraries have higher <italic toggle="yes">d<sub>pw</sub></italic> (ranging from 6.3 to 22.4) than the PSSM library (<italic toggle="yes">d<sub>pw</sub></italic>=5.9). In particular, the En-Gibbs library consists of sequences that are on average 21.3 distance away from Ab-14-L and 22.4 distance away from each other (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-9" hwp:rel-id="F2">Fig. 2F</xref>).</p><p hwp:id="p-29"><xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Fig. 3</xref> shows the 2-D embeddings of all antibody libraries and the training data. We observed a similar trend for both light- and heavy-chain designs, that is, the PSSM library is the closest to the training data while the ensemble-based libraries are the farthest away from the training data. More interestingly, all optimization-based libraries occupy a distinct subspace from the training data and PSSM library, highlighting the extrapolating power of the various optimization approaches that we applied. Ensemble-based libraries are highly divergent and also group distinctly from the other libraries; both the best heavy- and light-chain designs were discovered via optimizing the ensemble-extrapolated fitness function, underlining the value of exploring further away from the initial candidate sequence.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.10.07.502662v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Fig. 3</label><caption hwp:id="caption-4"><title hwp:id="title-10">T-SNE sequence embedding of ML-libraries, PSSM library and the training data reveals distinct sampled sequence subspaces.</title><p hwp:id="p-30">The t-SNE embeddings allow visualization of the sequence space by embedding the sequences into 2-D based while approximately preserving the edit distance between sequences. (A) 2-D embeddings of Ab-14-H variant designs and best scFvs from each library along with fold improvement over the best scFv of the PSSM library and mutational distance from Ab-14. (B) 2-D embeddings of Ab-14-L variant designs and best scFvs from each library.</p></caption><graphic xlink:href="502662v1_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s2d" hwp:id="sec-6"><title hwp:id="title-11">Model Performance and Sampling Diversity are Key Factors in Generating a Quality Library</title><p hwp:id="p-31">To understand key factors that determine the quality of a generated library, we first evaluated the performance of the two sequence-to-affinity models, using held-out test data and empirical binding measurements of our designed sequences (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Fig. 4</xref>). We compared the Spearman correlation and the mean absolute error (MAE) of model predictions and measured values. We observed that the ensemble sequence-to-affinity model does better at predicting affinity than the GP model. When evaluated on the held-out test data, Spearman correlation scores of both heavy- and light-chain ensemble models are slightly higher (heavy-chain model: 0.51; light-chain model: 0.69) than the respective GP models; see <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Fig. 4A</xref>. When evaluated on designed Ab-14-L variants, the light-chain ensemble model is also slightly better with a Spearman correlation of 0.61. The most notable difference is when evaluating on designed Ab-14-H variants, where the heavy-chain ensemble model has a Spearman correlation of 0.70 but the heavy-chain GP model performs significantly worse (−0.45). This is primarily due to the prediction limit of the GP model on sequences that are far beyond the training data. When evaluated the MAE of our prediction models with respect to the mutational distance on designed sequences, we observed a sharp increase in MAE on sequences with six or more mutations away from Ab-14-H for the heavy-chain GP model, and on sequences with ten or more mutations away from Ab-14-L for the light-chain GP model (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Fig 4B</xref>). Ensemble models exhibit no notable increase in MAE as the mutational distance increases, indicating that the ensemble approach is more generalizable to higher-order mutants than the GP model. Nevertheless, GP-based libraries, when compared to the PSSM library, are significantly more successful while having comparable sequence diversity (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-10" hwp:rel-id="F2">Fig. 2</xref>).</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.10.07.502662v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Fig. 4</label><caption hwp:id="caption-5"><title hwp:id="title-12">Sequence-to-affinity model evaluation.</title><p hwp:id="p-32">(A) Regression performance on hold-out test data and on the designed libraries; the ensemble model is more predictive than the GP model on both datasets. (B) Mean absolute error (MAE) and variance of the GP and ensemble models with respect to mutational distance from Ab-14. Evaluation was performed on sequences with at least 3 (out of 6) empirical binding affinities and the averaged values are used as the ground-truth. Ensemble models are more robust at extrapolating mutationally distant scFvs while the GP models do not predict well on sequences that are mutationally far away from Ab-14. Note that the error bar of the heavy-chain ensemble model shows a non-trivial increase on sequences that are twelve or more mutations away from Ab-14, suggesting that the model’s predictability decreases with increase in mutational distance.</p></caption><graphic xlink:href="502662v1_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig><p hwp:id="p-33">While ML-guided exploration of sequence space allows for identification of more scFvs with optimized binding, we postulate that if this set comes from diverse sequence space, it will also have diverse development properties thus limiting the chance of correlated downstream development failure. We observed that a good prediction model is necessary but not sufficient to generate a diverse library with high affinity. Equally important to the prediction model is the choice of sampling algorithm. When using the ensemble-extrapolated fitness landscape to engineer 14-Ab-H, hill climb and genetic algorithms found scFvs with significant (28.8 and 28.2-fold, respectively) increases in binding over the best PSSM-sampled scFv (<bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref></bold>), and both methods were highly successful (94.3% and 96% success, respectively); see <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-11" hwp:rel-id="F2">Fig. 2A</xref> and <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-12" hwp:rel-id="F2">2B</xref>. However, when combined with the Gibbs sampling algorithm, the best scFv sampled was only 2.9-fold better (<bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-3" hwp:rel-id="T1">Table 1</xref></bold>), and the library was generally unsuccessful (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-13" hwp:rel-id="F2">Fig 2A</xref> and <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-14" hwp:rel-id="F2">2B</xref>). With the two diversity metrics of the En-Gibbs-generated sequences almost double that of the En-HC and En-GA libraries, it indicates that the significant increase in diversity of the En-Gibbs library has a detrimental effect on library affinity due to the eventual limit of the model predictability on sequences that are deemed too far from the training data (<xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-15" hwp:rel-id="F2">Fig. 2C</xref>, Supplementary Fig. 2). Interestingly, when engineering the light chain (14-Ab-L), the En-Gibbs combination found the strongest binder (7.8-fold improvement over PSSM) with a striking 23 mutations from the Ab-14-L sequence <bold>(<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-4" hwp:rel-id="T1">Table 1</xref>)</bold>. For the ensemble-based libraries, as the library diversity increased, so too did the binding strength of its top scFv <bold>(</bold><xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-16" hwp:rel-id="F2">Fig. 2F</xref> and <bold><xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-5" hwp:rel-id="T1">Table 1</xref>)</bold>. En-HC, the least diverse ensemble-generated 14-Ab-L library, was the only library that failed to contain an scFv outperforming the top PSSM-generated scFv <bold>(<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-6" hwp:rel-id="T1">Table 1</xref>)</bold>. In this instance, the increased library diversity is beneficial, suggesting the value in exploring away from the initial candidate sequence. To avoid unsuccessful library designs while still being able to explore sufficiently high orders of mutants, it is important to control the diversity of sampled sequences via parameter tuning of the sampling algorithm and have the ability to explore the tradeoff between performance and diversity in silico prior to experimental testing.</p></sec><sec id="s2e" hwp:id="sec-7"><title hwp:id="title-13">Bayesian-based Approach Provides Insights Prior to Experimental Testing</title><p hwp:id="p-34">We defined an in silico performance metric that quantifies the binding performance of a library prior to experimental testing. With the Bayesian approach, the fitness score is the posterior probability of a sequence in the library having a stronger binding affinity than the candidate scFv Ab-14. We average the individual fitness scores of the full library to come up with our metric - an estimate of the probability of success (i.e., the estimated percent of sequences having a better binding performance than the threshold value). We first evaluated the utility of the metric on the hold-out test data from the training scFv library as we vary the threshold value that defines strong binders and show the estimated percent of success matches well to the actual percent of success (Supplementary Fig. 7).</p><p hwp:id="p-35">We applied the metric (estimated percent of success) to the designed libraries and ranked them. We compared the library ranking based on the estimated and measured percent of success (Supplementary Table 9). For PSSM and ensemble-based libraries, the predicted rankings match well to the actual rankings with a rank correlation of 0.8. For ranking PSSM and GP-based libraries, the metric predicts all rankings correctly for Ab-14-H variant libraries and a rank correlation of 0.8 for Ab-14-L variant libraries. Moreover, we observed that the estimated percent of success captures well the relative performance of designed libraries for both heavy- and light-chain designs (Supplementary Fig. 8 and 9).</p><p hwp:id="p-36">We then sought to extend the application of the in silico metric to comparing the choice of optimizing one CDR to optimizing all three simultaneously. For this comparison, designs were generated using the genetic algorithm sampling over the ensemble-extrapolated fitness landscape. We observed that designing all heavy-chain CDRs leads to sequences with higher estimated percent of success than when designing individual CDRs (Supplementary Fig. 10B).</p><p hwp:id="p-37">Based on these findings, we demonstrate that the performance metric can be used to understand design choices and explore tradeoffs between performance and diversity to inform library selection prior to experimental testing.</p></sec></sec><sec id="s3" hwp:id="sec-8"><title hwp:id="title-14">Discussion</title><p hwp:id="p-38">We demonstrate for the first time, in a head-to-head comparison with a conventional directed evolution strategy, scFvs designed with our ML approach are significantly stronger binders, especially at high levels of diversity, where, remarkably, our models are able to accurately predict binding affinity for extremely high order mutants. The libraries generated through our method have diverse biophysical properties (Supplementary Fig. 11). This allows for the selection of multiple preclinical candidates, uncorrelated in their downstream failure modes, such that if one fails, the entire pipeline is not likely to fail for the same reason. We also believe that our framework is applicable to any task aiming to maximize or minimize a characteristic of an scFv, such as minimizing off-target binding or maximizing neutralization. Pending data availability, we see ML-based multi-objective scFv optimization as an approachable task and viable option for streamlining scFv development.</p><p hwp:id="p-39">We separately explored our model performance as a function of the amount of training data and demonstrated additional data, expectedly, results in improved performance [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>]. However, after about 7,000 measurements, additional measurements result in less significant performance increases [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">26</xref>]. For this work, we trained our supervised sequence-to-affinity models on all 43,341 measurements that were available to us, but future engineering attempts may optimize use of financial resources by increasing the number of cycles while reducing the number of measurements per cycle. Because of cost limitations associated with DNA synthesis, we chose to generate our training data by introducing k = 1, 2, or 3 random mutations, but our models are able to successfully extrapolate much further than that. Future work would benefit from an improved understanding of the way in which training data is generated, if there is dependence on the choice of model, and if performing multiple measurement cycles impacts the choice.</p><p hwp:id="p-40">Recently, other works have presented general purpose pre-trained generative language models for antibody design [<xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-2" hwp:rel-id="ref-18">18</xref>], [<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">19</xref>]. By training on natural antibody repertoires, Shin <italic toggle="yes">et. al</italic>. [<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-2" hwp:rel-id="ref-22">22</xref>] were able to design antibody libraries that display good physical properties and are enriched for binders. In the future, our approach can be combined with these by using a pre-trained generative language model to design the initial mutagenesis library used for training our supervised learning approach. Our initial analysis indicates that this approach is likely to increase the success rate of the initial library by several fold. Furthermore, pre-trained models could also condition on features of the target epitope to design target-specific initial libraries that are then fine-tuned with our framework.</p><p hwp:id="p-41">We demonstrate the ability to rapidly design large libraries of potently binding scFvs, but our framework also extends to other domains of protein engineering where large scale functional mutagenesis screens are being applied. Our framework is neither scFv nor binding-specific and, therefore, can be applied to engineer other proteins for other functional properties. We expect machine learning approaches like ours, combined with high throughput mutagenesis screens, will soon become the standard in protein engineering.</p></sec><sec id="s4" hwp:id="sec-9"><title hwp:id="title-15">Online Methods</title><sec id="s4a" hwp:id="sec-10"><title hwp:id="title-16">Training data for Language Models</title><p hwp:id="p-42">We used sequences from Pfam and Observed Antibody Space (OAS) databases to train four separate language models (i.e., a protein language model, an antibody heavy chain model, an antibody light chain model and a paired heavy-light chain model). The Pfam is a database of curated protein families containing raw sequences of amino acids for individual protein domains [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>]. We use the same data splits as provided in [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-2" hwp:rel-id="ref-8">8</xref>]. The train, validation and test splits contain 32,593,668, 1,715,454 and 44,311 sequences, respectively. The full OAS database contains immune repertoires from over 75 studies containing a diverse set of immune states [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>]. We curated only studies with naïve human subjects and removed redundant sequences across the studies. This results in 37 studies containing 270,171,931 heavy chain sequences, 9 studies containing 70,838,791 light chain sequences, and 3 studies containing 33,881 heavy-light sequence pairs. The train, validation and test sequences are split based on studies. Given that there are limited heavy-light sequence pairs in the OAS data, to train the paired heavy-light chain model, we used all the data from OAS heavy chains, OAS light chains and OAS heavy-light sequence pairs. Supplementary Table 4 summarizes the number of sequences in train, validation and test data for the four language model training datasets.</p></sec><sec id="s4b" hwp:id="sec-11"><title hwp:id="title-17">Training BERT Language Models</title><p hwp:id="p-43">We used the BERT masked language model to encode protein/antibody sequences. It models the probability of an amino acid sequence <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>) by considering the probability distribution over each amino acid at each position conditioned on all other amino acids in the sequence, that is, <inline-formula hwp:id="inline-formula-9"><alternatives hwp:id="alternatives-9"><inline-graphic xlink:href="502662v1_inline9.gif" hwp:id="inline-graphic-9"/></alternatives></inline-formula>, where <italic toggle="yes">x<sub>i</sub></italic> represents the <italic toggle="yes">i<sup>th</sup></italic> amino acid in the sequence of length L. We pretrained four separate BERT language models, i.e., a protein language model, an antibody heavy chain model, an antibody light chain model and a paired heavy-light chain model, using the Pfam data [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-2" hwp:rel-id="ref-24">24</xref>] and OAS data [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-2" hwp:rel-id="ref-25">25</xref>]. Specifically, BERT masked language models were trained with 768 input embedding size, 24 hidden layers, 1024 hidden size, 4096 intermediate feed-forward size and 16 attention heads. All the other architecture details are fixed to their default values as provided in [<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-3" hwp:rel-id="ref-8">8</xref>]. We trained the language model to predict randomly masked amino acids in a single sequence or a sequence pair. For training the protein language model, antibody heavy chain model and antibody light chain model, the input is a single sequence of amino acids. For training the paired heavy-light chain model, the input is a concatenation of heavy and light sequences separated by a special token. Token type IDs are set to 0 for the ‘CLS’ token, 1 for the heavy chain amino acids and 2 for the light chain amino acids to identify two types of chains. Position IDs are set to be the integer position of the amino acid within its respective chain. The Pfam language model was initialized randomly. All other language models were initialized with the pre-trained Pfam model. For all models, the learning rate is set to 10<sup>−5</sup>, batch size is 1024 and the warm-up step is 10,000. One training epoch is defined as one full iteration over all the sequences in the training data. All models were trained until convergence of the cross-entropy loss value (which is evaluated on the validation data after every epoch), or until the maximum number of epochs,10, was reached. All models were trained on NVIDIA Volta V100 GPUs using a distributed compute architecture.</p><p hwp:id="p-44">The standard average perplexity score is used to evaluate the language model performance on the hold-out test data. The perplexity measures how well the trained language models are at predicting the masked tokens. Lower values indicate better performance. The average perplexities of the 4 language models on the respective test data are 13.15 for the Pfam model, 1.56 for the heavy-chain model, 1.43 for the light-chain model and 1.16 for the paired model. When evaluated on the OAS light-chain test data, the average perplexities of the 4 language models are 7.47, 16.40, 1.43 and 1.42, respectively. When evaluated on the OAS heavy-chain test data, the average perplexities of the 4 language models are 12.20, 15.30, 1.56 and 1.56, respectively.</p></sec><sec id="s4c" hwp:id="sec-12"><title hwp:id="title-18">Experimental Binding Measurements for Sequence-to-Affinity Model Training</title><p hwp:id="p-45">We separately published the training data to support ease-of-reuse [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-2" hwp:rel-id="ref-23">23</xref>]. Briefly, experimental measurements were made, in technical triplicate, by A-Alpha Bio, LLC and are a “predicted affinity” value [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>]. This value is derived by comparing the number of binding interactions observed between a pair of proteins to the number of binding interactions of protein-protein pairs with known affinity. When a replicate had no binding measurement reported, we assumed poor binding. For this work, we trained sequence-to-affinity models for the Ab-14-H heavy-chain and Ab-14-L light-chain variants. Each dataset was randomly split into train/validation/test sets with 0.8/0.1/0.1 split.</p></sec><sec id="s4d" hwp:id="sec-13"><title hwp:id="title-19">Training Sequence-to-Affinity Models via Transfer Learning</title><p hwp:id="p-46">We trained separate target-specific sequence-to-affinity models for Ab-14-H variants and Ab-14-L variants. We used model fine-tuning as a way to transfer knowledge learned from pre-trained language models to predicting sequence affinities. We investigated two approaches, which in addition to affinity prediction, provide estimates of prediction uncertainties: an ensemble method and Gaussian Process (GP). Both approaches use learned knowledge from pretrained language models and provide meaningful sequence-to-affinity models from which one can design a diverse antibody library.</p><p hwp:id="p-47">The ensemble model consists of 16 different trained regression models that were fine-tuned from the 4 pretrained language models with two different regression loss functions and two different data preprocessing steps (Supplementary Table 5). The two loss functions used were the mean squared error (MSE) and the mean absolute error (MAE) between the predicted affinities and measured affinities. For the data preprocessing step, we considered two options for how we treated missing values, i.e., antibodies with blank binding measurements. Since the experimental assay on the initial antibody library was conducted in triplicate (each antibody sequence has 3 binding measurements), we either drop the assay with missing value or impute it with the median value of all assays of the same candidate chain. Then we take the average of all binding measurements corresponding to the same antibody. To train a regression model, we fine-tuned the pre-trained BERT language model (initially trained on massive sequence data without affinity measurements) by continuing to train it on a smaller set of antibody sequences with experimental binding measurements. The outputs of the ensemble model are the mean and the standard deviation of the outputs of the 16 regression models.</p><p hwp:id="p-48">While the ensemble method is known to enhance predictive performance, GP is another powerful technique used for quantifying uncertainties. For the GP model, We used the pretrained heavy-chain language model to train the GP model for the heavy chain sequence-to-affinity model and the pretrained light-chain language model for the light chain sequence-to-affinity GP model. Sequences were represented by first concatenating the learned vector representations of each amino acid from the pretrained language model, and then performing principal component analysis (PCA) to reduce the vector dimension to 1,024. The GP model was trained on these reduced vector representations. The trained GP model outputs a mean and a standard deviation of the binding affinity prediction.</p></sec><sec id="s4e" hwp:id="sec-14"><title hwp:id="title-20">ML-Extrapolated Fitness Functions</title><p hwp:id="p-49">To generate a high affinity scFv library in silico, we used a Bayesian-based acquisition function extrapolated from the sequence-to-affinity model to construct the scFv fitness landscape. In contrast to non-Bayesian settings where the sequence is mapped directly to estimated affinity, the fitness function is defined to be a mapping from the entire scFv sequence to a posterior probability f(x) = p(aff(x)&lt;σ|x) that the estimated binding affinity aff(x) of the sequence x is better than a threshold. The threshold was set to the averaged assayed value of Ab-14 in the training data. Assuming a Gaussian distribution, f(x) can be computed using the mean and standard deviation of the prediction from the trained sequence-to-affinity model. For each scFv chain (Ab-14-H and Ab-14-L), we computed two fitness functions, extrapolated from the ensemble model and GP model, respectively. The proposed fitness function captures the model uncertainty during the optimization and enables us to estimate the performance of our antibody designs prior to experimental testing.</p></sec><sec id="s4f" hwp:id="sec-15"><title hwp:id="title-21">Optimization Strategies via Sampling</title><p hwp:id="p-50">The goal is to sample scFv sequences with the highest extrapolated fitness value f(x). The optimization was performed using 3 different sampling algorithms: a greedy algorithm called hill climb (HC) [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>], an evolutionary algorithm called genetic algorithm (GA) [<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>] and Gibbs sampling [<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>]. We initialized the HC and GA sampling processes with the 10 strongest binders (seed sequences) from the supervised training data and the Gibbs sampling with the strongest binders from the training data.</p><p hwp:id="p-51">For the hill climb algorithm, we initialized the optimization by randomly mutating a seed sequence with an expected number of k=2 mutations. At each step, the algorithm performs a local search around the current sequence and samples the next sequence that has the highest fitness value. The search continues until it can no longer find a sequence that has a better fitness value than the current sequence. We defined the local search space to be the 1,000 mutants of the current sequence, consisting of all the k=1 mutations and random k=2 mutations. The greedy-based hill climb was run 100 times with random restart around a random seed sequence.</p><p hwp:id="p-52">The genetic algorithm (GA) is an evolution-based search heuristic, where the fittest individuals are selected to produce offspring of the next generation. We initialized the population with a random seed sequence from the top 10 binders. Parents were chosen from the current population based on the Wright-Fisher model of evolution [<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>] where members of the current population become parents with a probability exponential to their fitness values, that is, p(x)~exp(f(x)/β). Sequences with high fitness have more chances to pass their genes to the next generation. A single-point crossover was performed on two parent sequences randomly selected from the parent population, and followed by randomly mutating individual child sequences with an expected k=1 mutation. The algorithm was terminated when it no longer produced new sequences (the population converged). The algorithm was run 100 times; each was initialized from a random seed sequence. The parameter <italic toggle="yes">β</italic> was set to be 0.2 for the ensemble-based fitness function and 0.5 for the GP-based fitness function. The selection of parameter value <italic toggle="yes">β</italic> directly affects the diversity of generated sequence designs. Depending on the design needs, one can tune this parameter to adjust the overall library design.</p><p hwp:id="p-53">Gibbs sampling is a Markov Chain Monte Carlo (MCMC) algorithm that samples a sequence according to some joint distribution by generating random variates from each of the full conditional distributions. We initialized the algorithm from the top seed sequence (the sequence with the strongest binding affinity in the training data). At each step, we randomly selected a position <italic toggle="yes">i</italic> in the sequence, sampled a mutant <inline-formula hwp:id="inline-formula-10"><alternatives hwp:id="alternatives-10"><inline-graphic xlink:href="502662v1_inline10.gif" hwp:id="inline-graphic-10"/></alternatives></inline-formula> at the selected position with a conditional probability <italic toggle="yes">p</italic>(<italic toggle="yes">x<sub>i</sub></italic>|<italic toggle="yes">x</italic><sub>1</sub> … <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic>−1</sub>, <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic>+1</sub>…<italic toggle="yes">x<sub>L</sub></italic>) and updated the sequence by replacing the <italic toggle="yes">i<sup>th</sup></italic> token with the sampled token <inline-formula hwp:id="inline-formula-11"><alternatives hwp:id="alternatives-11"><inline-graphic xlink:href="502662v1_inline11.gif" hwp:id="inline-graphic-11"/></alternatives></inline-formula>. The conditional probability was defined to be exponential to the fitness values, that is, <italic toggle="yes">p</italic>(<italic toggle="yes">x<sub>i</sub></italic>|<italic toggle="yes">x</italic><sub>1</sub> … <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic>−1</sub>, <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic>+1</sub>…<italic toggle="yes">x<sub>L</sub></italic> ~exp(γ*f(x)). The Gibbs sampling was run once with 30,000 iterations. The value γ was set to be 18 for the Ab-14-H ensemble-based fitness function, and 20 for both the Ab-14-L ensemble- and GP-based fitness function. Multiple γ values were used to sample the Ab-14-H GP-based fitness function. This is due to the limited number of sequences that can be sampled at any specific γ value for the given fitness function. To ensure that enough sequences can be sampled, we used γ=10,3,2 and ran the Gibbs algorithm three times to sample a sufficient number of sequences.</p></sec><sec id="s4g" hwp:id="sec-16"><title hwp:id="title-22">ML-Optimized ScFv Libraries</title><p hwp:id="p-54">For each scFv chain (Ab-14-H variants and Ab-14-L variants), we constructed two fitness functions extrapolated from the ensemble and GP model, respectively. For each fitness function, we performed optimization using three sampling strategies. This resulted in 6 libraries per chain: 3 libraries from optimizing the ensemble-based fitness function (namely, En-HC, En-GA and En-Gibbs), and 3 libraries from optimizing the GP-based fitness function (namely, GP-HC, GP-GA, GP-Gibbs). We then rank-ordered the generated sequences based on their fitness score per library and selected the top 6,000 sequences per library for experimental validation. Supplementary Fig. 2 and <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig. 3</xref> show the distribution of the designed sequences with respect to various mutational distances to demonstrate the library diversity: (1) mutational distance to the candidate scFv Ab-14, (2) mutational distance to the nearest top 10 binders and (3) pairwise mutational distance in a library. The first two distance metrics measure the number of mutations the designed antibodies are from Ab-14 and the best sequences in the training data, respectively. The third distance metric measures the intra-library diversity.</p></sec><sec id="s4h" hwp:id="sec-17"><title hwp:id="title-23">Evolution Directed Libraries</title><p hwp:id="p-55">We built two baseline libraries based on conventional directed evolution strategies: random mutations and the PSSM-based method. The random mutation library was constructed by randomly mutating amino acid tokens from the seed sequences in the training data with a k=2 average number of mutations. Using this method, 2,097 Ab-14-H heavy-chain variants and 477 Ab-14-L light-chain variants were generated for experimental testing.</p><p hwp:id="p-56">For the PSSM-based library, we used sequences in the training data with measured affinities that are as good or better than the candidate scFv Ab-14. We fitted the PSSM by counting the occurrence of each amino acid at each position in the CDRs with a small pseudocount. The fitted PSSM is a matrix of probability scores for each amino acid at each position, representing the statistical patterns of the training sequences that are better than Ab-14. We then drew samples to generate designs based on the fitted PSSM. Contrary to the random mutation approach, the PSSM-based approach is not restricted to a pre-defined mutational distance and could generate sequences that are far from the candidate antibody if the computed PSSM allows. The PSSM method resulted in 7,748 Ab-14-H heavy-chain variant designs and 8,257 Ab-14-L light-chain variant designs that were sent for experimental testing. Supplementary Fig. 2 and <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig. 3</xref> show the distribution of the generated sequences with respect to the mutational distances.</p></sec><sec id="s4i" hwp:id="sec-18"><title hwp:id="title-24">Experimental Validation of the Designed Sequences</title><p hwp:id="p-57">The AlphaSeq assay was performed by A-Alpha Bio LLC, with the target protein, scFv libraries and additional positive and negative controls to accommodate the large library sizes. The AlphaSeq assay was conducted from six technical replicates. Some in silico library members are absent from the resulting empirical dataset because they were unsuccessfully mapped in the haploid step and thus had no binding affinity data available. Supplementary Table 6 and Table 7 summarize the number and percentage of sequences present in the experimental data.</p><p hwp:id="p-58">For evaluation, we only consider designs that are present in the experimental AlphaSeq data. For sequences that are present in the affinity data and have at least three out of six empirical affinity values, the values are averaged and used as ground-truth measured affinities. Sequences with two or fewer empirical measurements are considered poor binders, and are included in the performance evaluation as un-successful designs. Supplementary Fig. 4 shows the binding distribution of selected libraries and the ML-optimized library outperforms the directed evolution approaches.</p></sec><sec id="s4j" hwp:id="sec-19"><title hwp:id="title-25">Biophysical property calculation, statistical analysis of libraries, and UMAP embedding</title><p hwp:id="p-59">Biophysical properties were calculated based on the sequences of the heavy and light chain variants in each library using BioPython [<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]. Specifically, isoelectric points were calculated using <italic toggle="yes">pK</italic> values and methods described by [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">33</xref>]–[<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>]. Hydrophobicity was calculated using the Kyte &amp; Doolittle index [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>]. The hydrophobicity score of each amino acid was averaged over the sequence of each variant to give an overall hydrophobicity score for each sequence. These properties were compared between libraries using the independent 2 sample t-test implemented in scipy [<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>] with default parameters.</p></sec></sec><sec sec-type="supplementary-material" hwp:id="sec-20"><title hwp:id="title-26">Supporting information</title><supplementary-material position="float" orientation="portrait" hwp:id="DC1"><object-id pub-id-type="other" hwp:sub-type="slug">DC1</object-id><label>Supplementary Material</label><media xlink:href="supplements/502662_file03.pdf" position="float" orientation="portrait" hwp:id="media-1"/></supplementary-material></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-27">Acknowledgements</title><p hwp:id="p-60">We would like to thank the scientists at A-Alpha Bio for assistance in generation of data; Eric Schwoebel, Joshua Dettman, and Tim Lu for thoughtful discussion of the research program and approach; Jack McGowan and Irene Stapleford for graphic design support; and the many other colleagues at MIT LL who have supported this project.</p></ack><sec hwp:id="sec-21"><title hwp:id="title-28">Author Contributions</title><p hwp:id="p-61">R.C., T.B., and M.W., conceived the project; R.C. and M.W. designed the data collection process; L.S. performed in silico mutagenesis; L.L., R.C., and T.B. designed model architectures; L.L., E.W., J.S., and T.B. performed algorithm development; L.L., E.W., J.S., and L.S. trained models; L.L., E.W., and L.S. evaluated models; L.L., E.W., J.S., T.B., and M.W. performed data analysis; L.L., R.J., R.C., T.B., and M.W. wrote and edited the manuscript.</p></sec><sec sec-type="COi-statement" hwp:id="sec-22"><title hwp:id="title-29">Competing Interests</title><p hwp:id="p-62">Tristan Bepler is the co-founder and CEO of NE47 Bio Inc., a company that provides machine learning services and software for protein engineering. MIT has filed a provisional patent application on certain described methods.</p></sec><sec hwp:id="sec-23"><title hwp:id="title-30">Data availability</title><p hwp:id="p-63">Raw data for the training data has been deposited to Zenodo at DOI:</p></sec><sec hwp:id="sec-24"><title hwp:id="title-31">Code availability</title><p hwp:id="p-64">Code for training self-supervised language models and supervised sequence-to-affinity models are available on github</p></sec><ref-list hwp:id="ref-list-1"><title hwp:id="title-32">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Arnold F. H."><given-names>F. H.</given-names> <surname>Arnold</surname></string-name>, “<article-title hwp:id="article-title-2">Directed Evolution: Bringing New Chemistry to Life</article-title>,” <source hwp:id="source-1">Angewandte Chemie International Edition</source>, vol. <volume>57</volume>, no. <issue>16</issue>, pp. <fpage>4143</fpage>–<lpage>4148</lpage>, <year>2018</year>, doi: <pub-id pub-id-type="doi">10.1002/anie.201708408</pub-id>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><label>[2]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Kelley B."><given-names>B.</given-names> <surname>Kelley</surname></string-name>, “<article-title hwp:id="article-title-3">Developing therapeutic monoclonal antibodies at pandemic pace</article-title>,” <source hwp:id="source-2">Nat Biotechnol</source>, vol. <volume>38</volume>, no. <issue>5</issue>, Art. no. 5, <month>May</month> <year>2020</year>, doi: <pub-id pub-id-type="doi">10.1038/s41587-020-0512-5</pub-id>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1"><label>[3]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Yang R."><given-names>R.</given-names> <surname>Yang</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-4">Rapid assessment of oxidation via middle-down LCMS correlates with methionine side-chain solvent-accessible surface area for 121 clinical stage monoclonal antibodies</article-title>,” <source hwp:id="source-3">mAbs</source>, vol. <volume>9</volume>, no. <issue>4</issue>, pp. <fpage>646</fpage>–<lpage>653</lpage>, <month>May</month> <year>2017</year>, doi: <pub-id pub-id-type="doi">10.1080/19420862.2017.1290753</pub-id>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.4" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Jain T."><given-names>T.</given-names> <surname>Jain</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-5">Biophysical properties of the clinical-stage antibody landscape</article-title>,” <source hwp:id="source-4">Proc. Natl. Acad. Sci. U.S.A.</source>, vol. <volume>114</volume>, no. <issue>5</issue>, pp. <fpage>944</fpage>–<lpage>949</lpage>, <month>Jan</month>. <year>2017</year>, doi: <pub-id pub-id-type="doi">10.1073/pnas.1616408114</pub-id>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.5" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Fischman S."><given-names>S.</given-names> <surname>Fischman</surname></string-name> and <string-name name-style="western" hwp:sortable="Ofran Y."><given-names>Y.</given-names> <surname>Ofran</surname></string-name>, “<article-title hwp:id="article-title-6">Computational design of antibodies</article-title>,” <source hwp:id="source-5">Curr Opin Struct Biol</source>, vol. <volume>51</volume>, pp. <fpage>156</fpage>–<lpage>162</lpage>, <month>Aug</month>. <year>2018</year>, doi: <pub-id pub-id-type="doi">10.1016/j.sbi.2018.04.007</pub-id>.</citation></ref><ref id="c6" hwp:id="ref-6"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Pantazes R. J."><given-names>R. J.</given-names> <surname>Pantazes</surname></string-name> and <string-name name-style="western" hwp:sortable="Maranas C. D."><given-names>C. D.</given-names> <surname>Maranas</surname></string-name>, “<article-title hwp:id="article-title-7">OptCDR: a general computational method for the design of antibody complementarity determining regions for targeted epitope binding</article-title>,” <source hwp:id="source-6">Protein Eng Des Sel</source>, vol. <volume>23</volume>, no. <issue>11</issue>, pp. <fpage>849</fpage>–<lpage>858</lpage>, <month>Nov</month>. <year>2010</year>, doi: <pub-id pub-id-type="doi">10.1093/protein/gzq061</pub-id>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1"><label>[7]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Li T."><given-names>T.</given-names> <surname>Li</surname></string-name>, <string-name name-style="western" hwp:sortable="Pantazes R. J."><given-names>R. J.</given-names> <surname>Pantazes</surname></string-name>, and <string-name name-style="western" hwp:sortable="Maranas C. D."><given-names>C. D.</given-names> <surname>Maranas</surname></string-name>, “<article-title hwp:id="article-title-8">OptMAVEn – A New Framework for the de novo Design of Antibody Variable Region Models Targeting Specific Antigen Epitopes</article-title>,” <source hwp:id="source-7">PLOS ONE</source>, vol. <volume>9</volume>, no. <issue>8</issue>, p. <fpage>e105954</fpage>, <month>Aug</month>. <year>2014</year>, doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0105954</pub-id>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1 xref-ref-8-2 xref-ref-8-3"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Rao R."><given-names>R.</given-names> <surname>Rao</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-9">Evaluating Protein Transfer Learning with TAPE</article-title>,” <source hwp:id="source-8">Adv Neural Inf Process Syst</source>, vol. <volume>32</volume>, pp. <fpage>9689</fpage>–<lpage>9701</lpage>, <month>Dec</month>. <year>2019</year>.</citation></ref><ref id="c9" hwp:id="ref-9"><label>[9]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Bepler T."><given-names>T.</given-names> <surname>Bepler</surname></string-name> and <string-name name-style="western" hwp:sortable="Berger B."><given-names>B.</given-names> <surname>Berger</surname></string-name>, “<article-title hwp:id="article-title-10">Learning the protein language: Evolution, structure, and function</article-title>,” <source hwp:id="source-9">Cell Systems</source>, vol. <volume>12</volume>, no. <issue>6</issue>, pp. <fpage>654</fpage>–<lpage>669.e3</lpage>, <month>Jun</month>. <year>2021</year>, doi: <pub-id pub-id-type="doi">10.1016/j.cels.2021.05.017</pub-id>.</citation></ref><ref id="c10" hwp:id="ref-10"><label>[10]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.10" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Alley E. C."><given-names>E. C.</given-names> <surname>Alley</surname></string-name>, <string-name name-style="western" hwp:sortable="Khimulya G."><given-names>G.</given-names> <surname>Khimulya</surname></string-name>, <string-name name-style="western" hwp:sortable="Biswas S."><given-names>S.</given-names> <surname>Biswas</surname></string-name>, <string-name name-style="western" hwp:sortable="AlQuraishi M."><given-names>M.</given-names> <surname>AlQuraishi</surname></string-name>, and <string-name name-style="western" hwp:sortable="Church G. M."><given-names>G. M.</given-names> <surname>Church</surname></string-name>, “<article-title hwp:id="article-title-11">Unified rational protein engineering with sequence-based deep representation learning</article-title>,” <source hwp:id="source-10">Nat Methods</source>, vol. <volume>16</volume>, no. <issue>12</issue>, Art. no. 12, <month>Dec</month>. <year>2019</year>, doi: <pub-id pub-id-type="doi">10.1038/s41592-019-0598-1</pub-id>.</citation></ref><ref id="c11" hwp:id="ref-11"><label>[11]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Bepler T."><given-names>T.</given-names> <surname>Bepler</surname></string-name> and <string-name name-style="western" hwp:sortable="Berger B."><given-names>B.</given-names> <surname>Berger</surname></string-name>, “<article-title hwp:id="article-title-12">Learning protein sequence embeddings using information from structure</article-title>,” <source hwp:id="source-11">arXiv</source>, arXiv:<pub-id pub-id-type="arxiv">1902.08661</pub-id>, <month>Oct</month>. <year>2019</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.1902.08661</pub-id>.</citation></ref><ref id="c12" hwp:id="ref-12"><label>[12]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Zhang Z."><given-names>Z.</given-names> <surname>Zhang</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-13">Protein Representation Learning by Geometric Structure Pretraining</article-title>,” <source hwp:id="source-12">arXiv</source>, arXiv:<pub-id pub-id-type="arxiv">2203.06125</pub-id>, <month>May</month> <year>2022</year>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2203.06125</pub-id>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><label>[13]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Yang K."><given-names>K.</given-names> <surname>Yang</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-14">Analyzing Learned Molecular Representations for Property Prediction</article-title>,” <source hwp:id="source-13">J. Chem. Inf. Model</source>., vol. <volume>59</volume>, no. <issue>8</issue>, pp. <fpage>3370</fpage>–<lpage>3388</lpage>, <month>Aug</month>. <year>2019</year>, doi: <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Jumper J."><given-names>J.</given-names> <surname>Jumper</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-15">Highly accurate protein structure prediction with AlphaFold</article-title>,” <source hwp:id="source-14">Nature</source>, vol. <volume>596</volume>, no. <issue>7873</issue>, Art. no. 7873, <month>Aug</month>. <year>2021</year>, doi: <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2"><label>[15]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Liu G."><given-names>G.</given-names> <surname>Liu</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-16">Antibody complementarity determining region design using high-capacity machine learning</article-title>,” <source hwp:id="source-15">Bioinformatics</source>, vol. <volume>36</volume>, no. <issue>7</issue>, pp. <fpage>2126</fpage>–<lpage>2133</lpage>, <month>Apr</month>. <year>2020</year>, doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btz895</pub-id>.</citation></ref><ref id="c16" hwp:id="ref-16"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Stokes J. M."><given-names>J. M.</given-names> <surname>Stokes</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-17">A Deep Learning Approach to Antibiotic Discovery</article-title>,” <source hwp:id="source-16">Cell</source>, vol. <volume>180</volume>, no. <issue>4</issue>, pp. <fpage>688</fpage>–<lpage>702.e13</lpage>, <month>Feb</month>. <year>2020</year>, doi: <pub-id pub-id-type="doi">10.1016/j.cell.2020.01.021</pub-id>.</citation></ref><ref id="c17" hwp:id="ref-17"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Mason D. M."><given-names>D. M.</given-names> <surname>Mason</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-18">Optimization of therapeutic antibodies by predicting antigen specificity from antibody sequence via deep learning</article-title>,” <source hwp:id="source-17">Nat Biomed Eng</source>, vol. <volume>5</volume>, no. <issue>6</issue>, Art. no. 6, <month>Jun</month>. <year>2021</year>, doi: <pub-id pub-id-type="doi">10.1038/s41551-021-00699-9</pub-id>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1 xref-ref-18-2"><label>[18]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.18" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Saka K."><given-names>K.</given-names> <surname>Saka</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-19">Antibody design using LSTM based deep generative model from phage display library for affinity maturation</article-title>,” <source hwp:id="source-18">Sci Rep</source>, vol. <volume>11</volume>, no. <issue>1</issue>, p. <fpage>5852</fpage>, <month>Mar</month>. <year>2021</year>, doi: <pub-id pub-id-type="doi">10.1038/s41598-021-85274-7</pub-id>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2"><label>[19]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Shin J.-E."><given-names>J.-E.</given-names> <surname>Shin</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-20">Protein design and variant prediction using autoregressive generative models</article-title>,” <source hwp:id="source-19">Nat Commun</source>, vol. <volume>12</volume>, no. <issue>1</issue>, Art. no. 1, <month>Apr</month>. <year>2021</year>, doi: <pub-id pub-id-type="doi">10.1038/s41467-021-22732-w</pub-id>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2"><label>[20]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.20" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Shan S."><given-names>S.</given-names> <surname>Shan</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-21">Deep learning guided optimization of human antibody against SARS-CoV-2 variants with broad neutralization</article-title>,” <source hwp:id="source-20">Proceedings of the National Academy of Sciences</source>, vol. <volume>119</volume>, no. <issue>11</issue>, p. <fpage>e2122954119</fpage>, <month>Mar</month>. <year>2022</year>, doi: <pub-id pub-id-type="doi">10.1073/pnas.2122954119</pub-id>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><label>[21]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.21" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Vaswani A."><given-names>A.</given-names> <surname>Vaswani</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-22">Attention is All you Need</article-title>,” in <source hwp:id="source-21">Advances in Neural Information Processing Systems</source>, <year>2017</year>, vol. <volume>30</volume>. Accessed: <date-in-citation content-type="access-date">May 11, 2022</date-in-citation>. [Online]. Available: <ext-link l:rel="related" l:ref-type="uri" l:ref="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" hwp:id="ext-link-2">https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</ext-link></citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1 xref-ref-22-2"><label>[22]</label><citation publication-type="confproc" citation-type="confproc" ref:id="2022.10.07.502662v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Devlin J."><given-names>J.</given-names> <surname>Devlin</surname></string-name>, <string-name name-style="western" hwp:sortable="Chang M.-W."><given-names>M.-W.</given-names> <surname>Chang</surname></string-name>, <string-name name-style="western" hwp:sortable="Lee K."><given-names>K.</given-names> <surname>Lee</surname></string-name>, and <string-name name-style="western" hwp:sortable="Toutanova K."><given-names>K.</given-names> <surname>Toutanova</surname></string-name>, “<article-title hwp:id="article-title-23">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title>,” in <conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</conf-name>, <conf-loc>Minneapolis, Minnesota</conf-loc>, <month>Jun</month>. <year>2019</year>, pp. <fpage>4171</fpage>–<lpage>4186</lpage>. doi: <pub-id pub-id-type="doi">10.18653/v1/N19-1423</pub-id>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1 xref-ref-23-2"><label>[23]</label><citation publication-type="other" citation-type="journal" ref:id="2022.10.07.502662v1.23" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Engelhart Emily"><surname>Engelhart</surname>, <given-names>Emily</given-names></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-24">A dataset comprised of binding interactions for 104,972 antibodies against a SARS-CoV-2 peptide</article-title>,” <source hwp:id="source-22">Scientific Data</source>, (In press).</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1 xref-ref-24-2"><label>[24]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="El-Gebali S."><given-names>S.</given-names> <surname>El-Gebali</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-25">The Pfam protein families database in 2019</article-title>,” <source hwp:id="source-23">Nucleic Acids Research</source>, vol. <volume>47</volume>, no. <issue>D1</issue>, pp. <fpage>D427</fpage>–<lpage>D432</lpage>, <month>Jan</month>. <year>2019</year>, doi: <pub-id pub-id-type="doi">10.1093/nar/gky995</pub-id>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1 xref-ref-25-2"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Kovaltsuk A."><given-names>A.</given-names> <surname>Kovaltsuk</surname></string-name>, <string-name name-style="western" hwp:sortable="Leem J."><given-names>J.</given-names> <surname>Leem</surname></string-name>, <string-name name-style="western" hwp:sortable="Kelm S."><given-names>S.</given-names> <surname>Kelm</surname></string-name>, <string-name name-style="western" hwp:sortable="Snowden J."><given-names>J.</given-names> <surname>Snowden</surname></string-name>, <string-name name-style="western" hwp:sortable="Deane C. M."><given-names>C. M.</given-names> <surname>Deane</surname></string-name>, and <string-name name-style="western" hwp:sortable="Krawczyk K."><given-names>K.</given-names> <surname>Krawczyk</surname></string-name>, “<article-title hwp:id="article-title-26">Observed Antibody Space: A Resource for Data Mining Next-Generation Sequencing of Antibody Repertoires</article-title>,” <source hwp:id="source-24">J.I.</source>, vol. <volume>201</volume>, no. <issue>8</issue>, pp. <fpage>2502</fpage>–<lpage>2509</lpage>, <month>Oct</month>. <year>2018</year>, doi: <pub-id pub-id-type="doi">10.4049/jimmunol.1800708</pub-id>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2"><label>[26]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.26" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Li L."><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name name-style="western" hwp:sortable="Gupta E."><given-names>E.</given-names> <surname>Gupta</surname></string-name>, <string-name name-style="western" hwp:sortable="Spaeth J."><given-names>J.</given-names> <surname>Spaeth</surname></string-name>, <string-name name-style="western" hwp:sortable="Shing L."><given-names>L.</given-names> <surname>Shing</surname></string-name>, <string-name name-style="western" hwp:sortable="Bepler T."><given-names>T.</given-names> <surname>Bepler</surname></string-name>, and <string-name name-style="western" hwp:sortable="Caceres R. S."><given-names>R. S.</given-names> <surname>Caceres</surname></string-name>, “<article-title hwp:id="article-title-27">Antibody Representation Learning for Drug Discovery</article-title>.” <source hwp:id="source-25">arXiv</source>, <month>Oct</month>. <year>05</year>, 2022. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2210.02881</pub-id>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1"><label>[27]</label><citation publication-type="website" citation-type="web" ref:id="2022.10.07.502662v1.27" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-27">“<article-title hwp:id="article-title-28">High-throughput characterization of protein–protein interactions by reprogramming yeast mating | PNAS</article-title>.” <ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.pnas.org/doi/10.1073/pnas.1705867114" ext-link-type="uri" xlink:href="https://www.pnas.org/doi/10.1073/pnas.1705867114" hwp:id="ext-link-3">https://www.pnas.org/doi/10.1073/pnas.1705867114</ext-link> (accessed <date-in-citation content-type="access-date">Jun. 07, 2022</date-in-citation>).</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>[28]</label><citation publication-type="website" citation-type="web" ref:id="2022.10.07.502662v1.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28">“<article-title hwp:id="article-title-29">Artificial Intelligence: A Modern Approach, 4th US ed</article-title>.” <ext-link l:rel="related" l:ref-type="uri" l:ref="http://aima.cs.berkeley.edu/" ext-link-type="uri" xlink:href="http://aima.cs.berkeley.edu/" hwp:id="ext-link-4">http://aima.cs.berkeley.edu/</ext-link> (accessed <date-in-citation content-type="access-date">Jun. 07, 2022</date-in-citation>).</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.29" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Katoch S."><given-names>S.</given-names> <surname>Katoch</surname></string-name>, <string-name name-style="western" hwp:sortable="Chauhan S. S."><given-names>S. S.</given-names> <surname>Chauhan</surname></string-name>, and <string-name name-style="western" hwp:sortable="Kumar V."><given-names>V.</given-names> <surname>Kumar</surname></string-name>, “<article-title hwp:id="article-title-30">A review on genetic algorithm: past, present, and future</article-title>,” <source hwp:id="source-26">Multimed Tools Appl</source>, vol. <volume>80</volume>, no. <issue>5</issue>, pp. <fpage>8091</fpage>–<lpage>8126</lpage>, <month>Feb</month>. <year>2021</year>, doi: <pub-id pub-id-type="doi">10.1007/s11042-020-10139-6</pub-id>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.30" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Levine R. A."><given-names>R. A.</given-names> <surname>Levine</surname></string-name>, <string-name name-style="western" hwp:sortable="Yu Z."><given-names>Z.</given-names> <surname>Yu</surname></string-name>, <string-name name-style="western" hwp:sortable="Hanley W. G."><given-names>W. G.</given-names> <surname>Hanley</surname></string-name>, and <string-name name-style="western" hwp:sortable="Nitao J. J."><given-names>J. J.</given-names> <surname>Nitao</surname></string-name>, “<article-title hwp:id="article-title-31">Implementing random scan Gibbs samplers</article-title>,” <source hwp:id="source-27">Computational Statistics</source>, vol. <volume>20</volume>, no. <issue>1</issue>, pp. <fpage>177</fpage>–<lpage>196</lpage>, <month>Mar</month>. <year>2005</year>, doi: <pub-id pub-id-type="doi">10.1007/BF02736129</pub-id>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>[31]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Tran T. D."><given-names>T. D.</given-names> <surname>Tran</surname></string-name>, <string-name name-style="western" hwp:sortable="Hofrichter J."><given-names>J.</given-names> <surname>Hofrichter</surname></string-name>, and <string-name name-style="western" hwp:sortable="Jost J."><given-names>J.</given-names> <surname>Jost</surname></string-name>, “<article-title hwp:id="article-title-32">An introduction to the mathematical structure of the Wright–Fisher model of population genetics</article-title>,” <source hwp:id="source-28">Theory Biosci</source>, vol. <volume>132</volume>, no. <issue>2</issue>, pp. <fpage>73</fpage>–<lpage>82</lpage>, <year>2013</year>, doi: <pub-id pub-id-type="doi">10.1007/s12064-012-0170-3</pub-id>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>[32]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Cock P. J. A."><given-names>P. J. A.</given-names> <surname>Cock</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-33">Biopython: freely available Python tools for computational molecular biology and bioinformatics</article-title>,” <source hwp:id="source-29">Bioinformatics</source>, vol. <volume>25</volume>, no. <issue>11</issue>, pp. <fpage>1422</fpage>–<lpage>1423</lpage>, <month>Jun</month>. <year>2009</year>, doi: <pub-id pub-id-type="doi">10.1093/bioinformatics/btp163</pub-id>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2"><label>[33]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.33" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Bjellqvist B."><given-names>B.</given-names> <surname>Bjellqvist</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-34">The focusing positions of polypeptides in immobilized pH gradients can be predicted from their amino acid sequences</article-title>,” <source hwp:id="source-30">ELECTROPHORESIS</source>, vol. <volume>14</volume>, no. <issue>1</issue>, pp. <fpage>1023</fpage>–<lpage>1031</lpage>, <year>1993</year>, doi: <pub-id pub-id-type="doi">10.1002/elps.11501401163</pub-id>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>[34]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Bjellqvist B."><given-names>B.</given-names> <surname>Bjellqvist</surname></string-name>, <string-name name-style="western" hwp:sortable="Basse B."><given-names>B.</given-names> <surname>Basse</surname></string-name>, <string-name name-style="western" hwp:sortable="Olsen E."><given-names>E.</given-names> <surname>Olsen</surname></string-name>, and <string-name name-style="western" hwp:sortable="Celis J. E."><given-names>J. E.</given-names> <surname>Celis</surname></string-name>, “<article-title hwp:id="article-title-35">Reference points for comparisons of two-dimensional maps of proteins from different human cell types defined in a pH scale where isoelectric points correlate with polypeptide compositions</article-title>,” <source hwp:id="source-31">Electrophoresis</source>, vol. <volume>15</volume>, no. <issue>3-4</issue>, pp. <fpage>529</fpage>–<lpage>539</lpage>, <month>Apr</month>. <year>1994</year>, doi: <pub-id pub-id-type="doi">10.1002/elps.1150150171</pub-id>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>[35]</label><citation publication-type="other" citation-type="journal" ref:id="2022.10.07.502662v1.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Tabb D. L."><given-names>D. L.</given-names> <surname>Tabb</surname></string-name>, “<article-title hwp:id="article-title-36">An algorithm for isoelectric point estimation</article-title>,” p. <fpage>5</fpage>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>[36]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Kyte J."><given-names>J.</given-names> <surname>Kyte</surname></string-name> and <string-name name-style="western" hwp:sortable="Doolittle R. F."><given-names>R. F.</given-names> <surname>Doolittle</surname></string-name>, “<article-title hwp:id="article-title-37">A simple method for displaying the hydropathic character of a protein</article-title>,” <source hwp:id="source-32">Journal of Molecular Biology</source>, vol. <volume>157</volume>, no. <issue>1</issue>, pp. <fpage>105</fpage>–<lpage>132</lpage>, <month>May</month> <year>1982</year>, doi: <pub-id pub-id-type="doi">10.1016/0022-2836(82)90515-0</pub-id>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>[37]</label><citation publication-type="journal" citation-type="journal" ref:id="2022.10.07.502662v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Virtanen P."><given-names>P.</given-names> <surname>Virtanen</surname></string-name> <etal>et al.</etal>, “<article-title hwp:id="article-title-38">SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>,” <source hwp:id="source-33">Nat Methods</source>, vol. <volume>17</volume>, no. <issue>3</issue>, Art. no. 3, <month>Mar</month>. <year>2020</year>, doi: <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>.</citation></ref></ref-list></back></article>
