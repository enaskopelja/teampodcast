<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2023.01.16.524265</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2023.01.16.524265</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2023.01.16.524265</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2023.01.16.524265</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2023.01.16.524265</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Bioinformatics" hwp:journal="biorxiv"><subject>Bioinformatics</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1"><label>*</label>Corresponding author(s). E-mail(s): <email hwp:id="email-1">ahmed.elnaggar@tum.de</email>;</corresp><fn fn-type="con" hwp:id="fn-1"><p hwp:id="p-1">Contributing authors: <email hwp:id="email-2">hazem@proteinea.com</email>; <email hwp:id="email-3">wafaa@proteinea.com</email>; <email hwp:id="email-4">walid@proteinea.com</email>; <email hwp:id="email-5">elkerdawy@proteinea.com</email>; <email hwp:id="email-6">cr3007@columbia.edu</email>; <email hwp:id="email-7">assistant@rostlab.org</email>;</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><name name-style="western" hwp:sortable="Elnaggar Ahmed"><surname>Elnaggar</surname><given-names>Ahmed</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">*</xref></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Essam Hazem"><surname>Essam</surname><given-names>Hazem</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Salah-Eldin Wafaa"><surname>Salah-Eldin</surname><given-names>Wafaa</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-3" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Moustafa Walid"><surname>Moustafa</surname><given-names>Walid</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-4" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><name name-style="western" hwp:sortable="Elkerdawy Mohamed"><surname>Elkerdawy</surname><given-names>Mohamed</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-5" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-6"><name name-style="western" hwp:sortable="Rochereau Charlotte"><surname>Rochereau</surname><given-names>Charlotte</given-names></name><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-7"><name name-style="western" hwp:sortable="Rost Burkhard"><surname>Rost</surname><given-names>Burkhard</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-2" hwp:rel-id="aff-1">1</xref></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1 xref-aff-1-2"><label>1</label><institution hwp:id="institution-1">Technical University of Munich (TUM)</institution></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2 xref-aff-2-3 xref-aff-2-4 xref-aff-2-5"><label>2</label><institution hwp:id="institution-2">Proteinea, Inc.</institution></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">Columbia University</institution></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2023"><year>2023</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2023-01-18T16:15:16-08:00">
    <day>18</day><month>1</month><year>2023</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2023-01-18T16:15:16-08:00">
    <day>18</day><month>1</month><year>2023</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2023-01-18T16:18:38-08:00">
    <day>18</day><month>1</month><year>2023</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2023-01-18T16:18:38-08:00">
    <day>18</day><month>1</month><year>2023</year>
  </pub-date><elocation-id>2023.01.16.524265</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2023-01-16"><day>16</day><month>1</month><year>2023</year></date>
<date date-type="rev-recd" hwp:start="2023-01-16"><day>16</day><month>1</month><year>2023</year></date>
<date date-type="accepted" hwp:start="2023-01-18"><day>18</day><month>1</month><year>2023</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2023, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2023</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-2">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-1">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="524265.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2023/pdf/2023.01.16.524265v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="524265.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2023/abstracts/2023.01.16.524265v1/2023.01.16.524265v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2023/fulltext/2023.01.16.524265v1/2023.01.16.524265v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-3">As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google’s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (&lt;10% for pre-training, &lt;7% for inference, and &lt;30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.</p></abstract><kwd-group kwd-group-type="author" hwp:id="kwd-group-1"><title hwp:id="title-2">Keywords</title><kwd hwp:id="kwd-1">Protein</kwd><kwd hwp:id="kwd-2">Language Model</kwd><kwd hwp:id="kwd-3">Transformer</kwd><kwd hwp:id="kwd-4">Deep Learning</kwd><kwd hwp:id="kwd-5">High-Performance Computing</kwd></kwd-group><counts><page-count count="29"/></counts></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-3">Competing Interest Statement</title><p hwp:id="p-4">The authors have declared no competing interest.</p></notes><fn-group content-type="external-links" hwp:id="fn-group-1"><fn fn-type="dataset" hwp:id="fn-2"><p hwp:id="p-5">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/agemagician/Ankh" ext-link-type="uri" xlink:href="https://github.com/agemagician/Ankh" hwp:id="ext-link-2">https://github.com/agemagician/Ankh</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1</label><title hwp:id="title-4">Introduction</title><p hwp:id="p-6">The analogy between the syntax-semantics of natural languages and the sequence-function of proteins has revolutionized the way humans investigate the language of life [<xref ref-type="bibr" rid="c1" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">1</xref>–<xref ref-type="bibr" rid="c8" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">8</xref>]. Although this analogy is intrinsically valuable when viewed as a precedent step in history leading to the adaptation of NLP’s advances on the domain of proteins (e.g., language models), conclusions from the field of NLP do not translate, fully, to protein language. Not only are NLP’s model sizes are pursued, it is even proposed that scaling-up protein language models may be significantly more impactful than scaling-up natural language models [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">9</xref>]. The proportionality between the model size and the richness of its learned representations is rather -falsely-encouraged by observing language models of a massive number of parameters trained on a massive number of steps still undergoing notable learning gradient and hence perceived as under-fitted [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">3</xref>, <xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-2" hwp:rel-id="ref-9">9</xref>, <xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">10</xref>]. As a result, opting for more meaningful protein representations or more accurate modeling has gradually shifted to opting for larger models and accordingly, more computational power - less accessibility. Notably, PLM sizes have jumped from ∼10<sup>6</sup> [<xref ref-type="bibr" rid="c4" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">4</xref>] to ∼10<sup>9</sup> [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-2" hwp:rel-id="ref-10">10</xref>] parameters recently. Shedding the light, chronologically, on protein language model state-of-the-art (SOTA), we baseline our size-performance benchmark using ProtTrans’s ProtT5-XL-U50, an encoder-decoder transformer pre-trained on UniRef50 database whose number of parameters is 3B for training and 1.5B for inference [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">3</xref>]. The evolution of model performance with respect to its size was then demonstrated via RITA, a family of language models taking a first step towards establishing scaling principles for protein sequence modeling. RITA showcases 4 different models with a performance-proportional increase in size from 85M, to 300M, to 680M, to 1.2B parameters [<xref ref-type="bibr" rid="c9" hwp:id="xref-ref-9-3" hwp:rel-id="ref-9">9</xref>]. The same trend was then reinforced by ProGen2, a suite of protein language models that are trained on different sequence datasets and whose number of parameters is scaled up to 6.4B [<xref ref-type="bibr" rid="c11" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">11</xref>]. Finally and up to the publication date of this manuscript, the latest contribution promoting model up-scaling is ESM-2, a poll of general-purpose protein language models that also showcase a performance-proportional increase in size from 650M, to 3B, to 15B parameters [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-3" hwp:rel-id="ref-10">10</xref>]. The simplified relation between bigger and seemingly-better PLMs, ignores several aspects, including computational costs, task-agnostic model design, and implementation. This raises the research innovation entry barrier and constrains it to scalability. Although model size is, without a doubt, a high impact attribute in pursuing the aforementioned objectives, it is not the only one. The same direction in up-scaling pre-training datasets has proven to be conditional (i.e., bigger datasets are not necessarily better than smaller datasets of higher quality) [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-3" hwp:rel-id="ref-3">3</xref>]. We build upon the same direction arguing that up-scaling language models is, too, conditional (i.e., bigger models are not necessarily better than smaller models of protein knowledge-guided means of optimization). In this work, our main objective is to integrate knowledge-guided optimization in an iterative empirical framework that promotes accessibility to research innovation via attainable resources. We title our work “Ankh” (i.e. an Ancient Egyptian symbol denoting the key of life) in analogy to how our model “unlocks” the language of life via learning superior representations of its “letters”, the amino acids. This is expanded into two pieces of evidence in evaluating Ankh in terms of optimization and generality. Firstly, surpassing the performance of the SOTA in a representative range of structure and function benchmarks combined with a generation analysis for protein engineering on High-N (family-based) and One-N (single sequence-based) applications, where N refers to the number of input sequences. Secondly, fulfilling this performance via a poll of optimized attributes that not only include the model design but also its development, training, and deployment software and hardware. We provide two pre-trained models referred to as <italic toggle="yes">Ankh large</italic> and <italic toggle="yes">Ankh base</italic>, offering two modes of computation depending on the application demands. For convenience, we refer to our main model, <italic toggle="yes">Ankh large</italic>, as <italic toggle="yes">Ankh</italic>.</p></sec><sec id="s2" hwp:id="sec-2"><label>2</label><title hwp:id="title-5">Results</title><sec id="s2a" hwp:id="sec-3"><label>2.1</label><title hwp:id="title-6">Utmost Computational Efficiency and Utmost Performance</title><p hwp:id="p-7">We promote visualizing the performance-size correlation as a trade-off also encompassing computational power. On average, <italic toggle="yes">Ankh</italic> improved the PLM SOTA performance by 4.8% while <italic toggle="yes">Ankh base</italic> improved it by 3.4% with &lt;10% &amp; 3% of the training parameters and 30% &amp; 15% of the embedding dimension, for <italic toggle="yes">Ankh</italic> and <italic toggle="yes">Ankh base</italic> respectively (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Fig. 1</xref>). Since feature extraction is the basis of any subsequent modeling, we compared the time needed in milliseconds to extract the features of a sequence with increasing length up to 1024 residues (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-2" hwp:rel-id="F1">Fig. 1</xref>). Although the Ankh models support sequence lengths even beyond the maximum length of the pre-defined relative positional embedding dimension, we chose the upper limit of 1024 to accommodate the maximum length supported by the ESM-1b model. We can see that ESM-2 (15B) takes minimally 2.2x &amp; 2.0x and maximally 11.7x &amp; 7.1x the feature extraction time for <italic toggle="yes">Ankh base</italic> and <italic toggle="yes">Ankh</italic>, respectively.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1 xref-fig-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Fig. 1:</label><caption hwp:id="caption-1"><title hwp:id="title-7">Performance-Size Trade-off Comparison:</title><p hwp:id="p-8">(a), we plot the number of parameters of different protein language models on the x-axis vs. the mean and median of the performance scores of seven different benchmarking tasks on the y-axis. (b), we plot the embedding dimensions of the investigated models on the x-axis vs. the same y-axis as (a). In diagram (c), we plot the increasing sequence length up to 1024 amino acids on the x-axis and on the y-axis, we plot the corresponding feature extraction time in ms for all of the investigated models.</p></caption><graphic xlink:href="524265v1_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><p hwp:id="p-9">As for the storage, we needed four A100 80GB Tensor Core GPUs for the feature extraction using ESM-2 (15B) compared to a single A100 40 GB Tensor Core GPU for each of the Ankh models. These computational attributes showcase that besides achieving the top average and median downstream performance, Ankh offers a significantly more accessible computational demand. For the reported results on the protein bench-marking tasks in <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref>, the contextualized embeddings of the protein sequences of each dataset are extracted from the last hidden states of all the investigated models. Our work promotes embedding extraction over attention extraction as means of transfer learning in light of promoting computational optimization. Therefore, we optimized our experimentation with respect to embedding-based predictions. However and as attention maps are reported as the better indicator of contact prediction for the ESM PLM family in [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">2</xref>] and [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-4" hwp:rel-id="ref-10">10</xref>], the two representations are tested, separately. To elaborate, the attention maps are extracted and compared with the contextualized embeddings as input for the contact prediction task per every model to opt for fair comparison and demonstrate the SOTA best indicator with what we deem as the best indicator. Indeed, the results show significant out-performance via embedding-based prediction against attention-based prediction (the full results of attention-based predictions can be found in <xref rid="tbl13" ref-type="table" hwp:id="xref-table-wrap-13-1" hwp:rel-id="T13">Table 13</xref>. As shown in 1, the Ankh suite consistently outperformed the rest of the investigated models. We also observe that ESM-2 (15B) did not outperform the smaller models from the ESM family in all of the tasks in addition to showing inconsistent results across different runs, affirming our hypothesis that bigger models are not necessarily better in all protein modeling tasks and that extensive model sizes bring out its own challenges.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1:</label><caption hwp:id="caption-2"><p hwp:id="p-10">Results Summary</p></caption><graphic xlink:href="524265v1_tbl1" position="float" orientation="portrait" hwp:id="graphic-2"/></table-wrap></sec><sec id="s2b" hwp:id="sec-4"><label>2.2</label><title hwp:id="title-8">Protein Generation Made Attainable</title><sec id="s2b1" hwp:id="sec-5"><label>2.2.1</label><title hwp:id="title-9">Auto-Regressive Fine-Tuning for High-N Generation</title><p hwp:id="p-11">We propose an auto-regressive fine-tuning generation framework for the High-N (protein familybased generation) scale as it offers an accessible approach for protein variant generation that can be easily scaled across different protein families. Moreover, the framework offers easy control over the generation’s exploration-exploitation trade-off by manipulating the logit warping temperature, a parameter used NLP models to increase/decrease the model’s confidence in its most likely response [<xref ref-type="bibr" rid="c12" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">12</xref>]. To validate the framework and select the best temperature value range, the same model, finetuned on malate dehydrogenase (MDH) natural variants, was utilized to generate three initial sets of 500 sequences with three different temperatures (1.0, 1.5, and 2.0). For the three sets, the Shan-non entropy variations between the generated set and a multi-sequence alignment (MSA) of the fine tuning data are reported. Shannon Entropy aims to characterize the generated sequences’ preservation of the evolutionary properties of the natural dataset by comparing their representative statistics of amino acid variation. Low entropy values reflect conserved regions governing retained functionality whereas high entropy values reflect less conserved regions with higher mutation rates. In <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-1" hwp:rel-id="F2">Figure 2 (a)</xref>, we can observe that the three generated sets show high similarity to the distribution of the natural sequences with almost identical positions for both peaks and valleys. The mean square error (MSE) between entropy values of natural and generated sets are quantified as 0.1, 0.09, and 0.08 for a generation with temperatures 1.0, 1,5, and 2.0 respectively. We emphasize that the reported similarity is calculated based on generated set of 500, which is less than 3% of the total sequences of the natural set (16,706). In other words, the model can mimic the distribution of the fine-tuning dataset with a small portion of its original sequences. To further investigate the effect of different temperatures on the exploration-exploration trade-off, we focus on the generated sequences via temperatures 1.0 and 2.0 due to their direct correlation to introducing functional diversity whilst maintaining conserved functional regions as observed. With this regard, we first report a comparison of the global alignment based identity between the generated sequences and the fine-tuning dataset. We compute the identity via BioPython’s pairwise2 module where we obtain the global alignment between the generated variants (gen) and the original ones (nat) [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">13</xref>]. We can observe in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-2" hwp:rel-id="F2">Figure 2</xref> (d) that the generated sequences span a wide range of global alignment-based identity scores with sequences as different as 70% and 55% for the generation with temperatures 1.0, and 2.0, respectively. Moreover, over 95% of the generated variants were unique (i.e., only 5% of the variants were duplicates of the fine-tuning dataset sequences). Furthermore, we report the internal identity of each set where we obtain the global alignment between the sequences of each set and themselves. We can observe in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-3" hwp:rel-id="F2">Figure 2 (e)</xref> that the generated set at 1.0 temperature show less internal variability than the natural sequences, while the generated set at 2.0 shows higher internal variability. Consequently, generation at temperature 1.0 tends to be more conservative, favoring similarity to the most dominating clusters in the fine-tuning dataset. On the other hand, generation at 2.0 tends to be less conservative, covering rare clusters and presenting more diversity in the generation. This allows the user control over the generation process according to the nature of the fine-tuning protein family, and the interest in the generation of global or local variants. Focusing on comparing natural and generated domains with known structural annotations, we observe that the CATH domains in natural variants dominantly belong to three homologous super-families. In all of the generated sets, a significant percentage of the generated sequences includes domains from the three major super-families as shown in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-4" hwp:rel-id="F2">Fig. 2 (f)</xref> and detailed numerically in <xref rid="tbl15" ref-type="table" hwp:id="xref-table-wrap-15-1" hwp:rel-id="T15">Table 15</xref>. The domains with known functional classifications are further investigated to compare the functional diversity of the generated sequences. Natural domains from only two of the three homologous super-families (3.90.110.10 and 3.40.50.720) are annotated with functional-family numbers. The functionally annotated CATH domains belonging to the 3.90.110.10 super-family are visualized in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-5" hwp:rel-id="F2">Fig 2 (g)</xref>. All of the generated sets contain domains belonging to the common functional-family numbers: 2, 11, and 3. However, domains belonging to the rare family number 1 (only 6 occurrences in the natural set) can be only observed in sequences generated at temperature 2 (155 and 95 occurrences in epochs 1, and 2 respectively). The same distribution trend is also conserved in the functionally annotated domains belonging to the super-family 3.40.50.720 as can be seen in <xref rid="tbl16" ref-type="table" hwp:id="xref-table-wrap-16-1" hwp:rel-id="T16">Table 16</xref></p><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2" hwp:rev-id="xref-fig-2-1 xref-fig-2-2 xref-fig-2-3 xref-fig-2-4 xref-fig-2-5 xref-fig-2-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Fig. 2:</label><caption hwp:id="caption-3"><title hwp:id="title-10">Auto-Regressive Fine-Tuning Generation of MDH variants:</title><p hwp:id="p-12">(a), (b), and (c) Shannon entropy curves between the MSA of natural and generated sets at logit warping temperatures of 1.0, 1.5, and 2.0, respectively: Manipulating the temperature during the generation affects the variability of the generated sequences. (d) Distribution of the Sequence Identity of the generated variants of temperatures 1.0 and 2.0 vs. natural variants: The generated sequences span a wide range of sequence identity scores going as low as 60%. (e) Internal identities of the natural/fine-tuning set vs. the generated sets of temperatures of 1.0 and 2.0: The generated variants span wide ranges of variability, with the set of temperature 2.0 showing the widest range. (f)CATH domain distribution of the three dominating homologous super-families in the natural set: All the generated variants retained CATH domains from the three dominating super-families. (g) The distribution of CATH domains with known functional annotations in the 3.90.110.10 super-family (responsible of the dehydrogenase function) in the natural and generated sets: The generated sets managed to maintain domains from the three functional families of the natural set while expanding to domains from functional family number 1, maltase dehydrogenase (MDH), with only 6 examples in the natural set. Abbr., Nat: Natural sequences; t1.e1: generated sequences at temperature 1 by the first epoch; t1.e2: Generated sequences at temperature 1 by the second epoch; t2.e1: Generated sequences at temperature 2 by the first epoch; t2.e2: Generated sequences at temperature 2 by the second epoch</p></caption><graphic xlink:href="524265v1_fig2" position="float" orientation="portrait" hwp:id="graphic-3"/></fig></sec><sec id="s2b2" hwp:id="sec-6"><label>2.2.2</label><title hwp:id="title-11">Masked Language Modeling (MLM) for One-N Generation</title><p hwp:id="p-13">Since we utilize the Masked Language Modeling (MLM) generation framework on the One-Shot (single-sequence generation) scale, it is infeasible to evaluate the generated sequences w.r.t. a specific dataset. Instead, we evaluate the retention of their experimental 3D structural -and accordingly functional-characteristics. To fulfill this, we use ColabFold’s <italic toggle="yes">colabfold batch</italic> with 2 models and 3 cycles to predict the 3D structure of the generated variants [<xref ref-type="bibr" rid="c14" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">14</xref>]. Per every generated sequence, we plot its identity to the original unmasked sequences vs. the root mean square deviation (RMSD) between its predicted structure and the experimental 3D structure of the original sequences. We compute the RMSD from the C-alpha atom via BioPython’s SVDSuperimposer [<xref ref-type="bibr" rid="c13" hwp:id="xref-ref-13-2" hwp:rel-id="ref-13">13</xref>]. Ideally, we would want our model to retain the semantics of the sequence while changing its syntax. In other words, we would want to have variants with low sequence identities as well as low RMSD. To test this assumption, we generate 179 synthetic variants utilizing two Masking Probability 40% and 50% of the input dataset. Indeed, we can see in <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> that the model was utilized to generate many sequences with low sequence similarity while maintaining high structural similarity compared with the input sequences. In the figure, we can notice that sequences generated by 50% masking probability maintain a steeper slope, meaning that the bigger the unmasked context, the better-generated variants, as expected.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1 xref-fig-3-2 xref-fig-3-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Fig. 3:</label><caption hwp:id="caption-4"><title hwp:id="title-12">Sequence Identities vs Structural Similarities of the Generated Sequences at Masking Probability of 40% (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-2" hwp:rel-id="F3">Fig 3.a</xref>.) and 50% (<xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-3" hwp:rel-id="F3">Fig 3.b</xref>.):</title><p hwp:id="p-14">The figures show the model was able to generate sequences with RMSD lower than 1.5 A withe sequence identity as low as 80% in both cases. The generated sequences at masking probability= 50% show more negative Pearson correlation, suggesting more sequence context facilitates the generation of similar structures with low sequence identities.</p></caption><graphic xlink:href="524265v1_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig></sec><sec id="s2b3" hwp:id="sec-7"><label>2.2.3</label><title hwp:id="title-13">Knowledge-Guided Optimization in Action</title><p hwp:id="p-15">To fulfill the demonstrated utmost computational efficiency coupled with the top downstream performance, our model design was preceded with knowledge-guided experimentation. We define knowledge-guided experimentation as protein-specific experimentation retaining a single independent variable traversing masking (strategy and probability), architecture, and pre-training dataset. This pre-design experimentation retained training each variation for 2 epochs while also abiding by approximately the same total number of parameters per experiment to avoid complexity biases. The definitions of the experimental independent variables, their values, evaluation metrics, and the experimentation baseline can be found in the Methods section. The design of the final models integrates the top-performing experimental versions of each independent variable’s set of experiments and sub-experiments. We fully pretrained two models, <italic toggle="yes">Ankh</italic> and <italic toggle="yes">Ankh base</italic>, that can be visualized in <xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>. The results of the masking strategy experiments promoted utilizing a 1-gram span de-masking strategy (elaborated on in <italic toggle="yes">Exp. 4</italic>). Regarding the masking probability, we used 20% as promoted by <italic toggle="yes">Exp. 8</italic>. For the number of layers, we chose 48 layers for the encoder and 24 layers for the decoder as per the results of <italic toggle="yes">Exp. 11</italic>. For the activation function, we proceeded with Gated-GELU as per the findings of <italic toggle="yes">Exp. 14</italic> and <italic toggle="yes">Exp. 15</italic>. We adopted relative positional embeddings with an embedding offset of 128 and an embedding dimension of 64 as per the results of <italic toggle="yes">Exp. 20</italic>. However, the only dimensions where the two models differ are the embedding dimension, number of attention heads, and number of feed-forward layers. <italic toggle="yes">Ankh base</italic> has an embedding dimension of 768 as recommended by <italic toggle="yes">Exp. 13</italic> while <italic toggle="yes">Ankh</italic> has an embedding of 1536 as we found that double dimensions are often better performing as verified by <italic toggle="yes">Exp. 20</italic>. Furthermore, the number of attention heads for <italic toggle="yes">Ankh base</italic> and <italic toggle="yes">Ankh</italic> is 12 and 16, respectively. Finally, the number of feed-forward layers for <italic toggle="yes">Ankh base</italic> is 3072 and for <italic toggle="yes">Ankh</italic> is 3840. The full configurations for the two models can be found in <xref rid="tbl11" ref-type="table" hwp:id="xref-table-wrap-11-1" hwp:rel-id="T11">Table 11</xref>.</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Fig. 4:</label><caption hwp:id="caption-5"><title hwp:id="title-14">Architecture of Ankh Models:</title><p hwp:id="p-16">Arrows show the information flow in the network starting from the input sequences, transformer input pre-processing, transformer, and then either a residue-level prediction network or a protein-level prediction network that only differs in being preceded by a global max pooling layer. Both <italic toggle="yes">Ankh base</italic> and <italic toggle="yes">Ankh large</italic> agree on the demonstrated architecture. However, they differ in the length of the context vector.</p></caption><graphic xlink:href="524265v1_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec></sec></sec><sec id="s3" hwp:id="sec-8"><label>3</label><title hwp:id="title-15">Discussion</title><sec id="s3a" hwp:id="sec-9"><title hwp:id="title-16">Results Summary - More Efficient Models Can Also Generalize Better</title><p hwp:id="p-17">To fulfill a holistic analysis of our model, we ensured an evaluation of Ankh that spans the most principal and valuable categories of protein modeling via deep learning. From a deep learning perspective, we have tested our model on regression, classification, and generation. From a biology perspective, we have tested our model on residue-level and protein-level where the measured attributes spanned structure and function. From a scale and sparsity perspective, we have tested our model on both High-N and One-N scales. In all of the aforementioned benchmarks, Ankh surpassed the performance of state-of-the-art with no exceptions while our base model Ankh base either reached the same or achieved very comparable with significantly less computational power, offering two modes of computation depending on the application demands. In our generation analysis, Ankh was able to learn the evolutionary conservation-mutation trends and introduce diversity while retaining key structural and functional characteristics in both High-N and One-N scales.</p></sec><sec id="s3b" hwp:id="sec-10"><title hwp:id="title-17">Results Interpretation</title><p hwp:id="p-18">Our results reinforce the compatibility of PLMs with the language of life but signify protein knowledge-guided model design. Moreover, our results fortify the use of sequence contextualized embeddings as a mere input to downstream models but shed light on task-specific architectures/layers.</p></sec><sec id="s3c" hwp:id="sec-11"><title hwp:id="title-18">Results Implications</title><p hwp:id="p-19">Our results suggest that state-of-the-art performance can be reached and surpassed by significantly less computational power. This suggestion implies the necessity of criticizing the highlighted correlation between model performance and needed computational power, embodied in either model or data sizes. Instead, our work suggests visualizing this correlation as a trade-off, highlighting the immense cost of directly scaling model/data size to improve model performance. On the other side of the trade-off, we propose knowledge-guided means of optimization whose prerequisites revolve around needed protein knowledge as well as the extra mile in optimizing both the software and hardware components of the model life cycle.</p></sec><sec id="s3d" hwp:id="sec-12"><title hwp:id="title-19">Results Limitation</title><p hwp:id="p-20">We report several limitations of our work. Firstly, changes of the activation function impacted the optima in the number of layers for encoder and decoder, as well as, the embedding dimension. However, this setting is traced to the nature of the Gated-GELU activation function denoting a significantly larger number of trainable parameters that, in turn, forced us to compensate increases in width by decreases in depth to retain the same total number of trainable parameters and avoid any possible complexity-bias [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">15</xref>]. Consequently, testing different combinations of dimensionality required utilizing an activation function needing fewer parameters. Secondly, our optimization propagated the top-performing model version for each prediction task to the next one. <italic toggle="yes">Top performance</italic> was not achieved through the version numerically performing top in all tasks, because none such version existed. Instead, we selected the version that outperformed others for most of the most standard data sets, and performed top for diverse objectives. The downside of this seemingly simple algorithm is that we failed to define any single formula to compute “best” that stands out from amongst the population of such formulas. Specifically, we documented the reasoning behind the choice of the top performer for each prediction task. Generally, we justified our choices by opting for holistic predictions via a general-purpose language model. This entailed promoting generalization across different tasks, especially when the difference amongst results remained numerically negligible, when acknowledging that task-specific customization is of impact, and when all the experimental versions are only trained for two epochs.</p></sec><sec id="s3e" hwp:id="sec-13"><title hwp:id="title-20">Recommendations</title><p hwp:id="p-21">Our results implicitly suggested that the choice of the data used to pre-train the pLMs might have to be coordinated with that of data sets used for testing subsequent protein prediction tasks. Although such an endeavor is beyond this work’s scope, we encourage efforts toward this end. For instance, we have reported the superiority of pre-training with UniRef50 over UniRef90, UniRef100, and BFD due to lower redundancy. Although the details of what exactly constitutes redundancy relate to the application (e.g. using all available human proteins constitutes less redundancy when wondering about the length distribution of human proteins than when trying to predict binding residues in these), too much redundancy is often easy to spot [<xref ref-type="bibr" rid="c16" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">16</xref>]. Furthermore, we shed light on the value of incorporating synthetically-generated-experimentallycharacterized sequences.</p></sec><sec id="s3f" hwp:id="sec-14"><title hwp:id="title-21">Future Work</title><p hwp:id="p-22">We present Ankh as an initial version of our optimized general-purpose protein language model. This version is meant to serve as a pre-training base that shall then be specialized into high-impact and high-value protein modeling tasks in our future work (e.g., full atomic resolution 3D-structure prediction, protein generation, etc.) with task-specific optimization and detailed analysis.</p></sec></sec><sec id="s4" hwp:id="sec-15"><label>4</label><title hwp:id="title-22">Methods</title><sec id="s4a" hwp:id="sec-16"><label>4.1</label><title hwp:id="title-23">Self-Supervised Learning</title><sec id="s4a1" hwp:id="sec-17"><label>4.1.1</label><title hwp:id="title-24">Pre-training Datasets</title><p hwp:id="p-23">Deep learning-based protein modeling, likewise NLP, is data-driven. However, this hunger for data is proving to be both constrained and constraining. We draw upon previous experimentation done in ProtTrans, in general, and Prot-T5, in particular, where the performance and associated computational power of three datasets ranging in size, identity-based clustering, and origin were analyzed. The analysis promoted utilizing UniRef50 [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">17</xref>] over UniRef100 [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">17</xref>] and BFD [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-4" hwp:rel-id="ref-3">3</xref>, <xref ref-type="bibr" rid="c18" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">18</xref>]. We build upon the same results by pre-training our baseline on UniRef50.</p><p hwp:id="p-24">UniRef (UniProt Reference Clusters) databases offer variable clustering of UniProtKB sequences (including isoforms) and selected UniParc records [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-3" hwp:rel-id="ref-17">17</xref>]. This variable clustering denotes different sequence similarity thresholds that can fulfill non-redundancy and intra-cluster homogeneity. In UniRef100, a single cluster/entry denotes identical sequences and sub-fragments originating from any organism. To build UniRef90 and UniRef50, UniRef100 sequences are clustered at 90% and 50% sequence identity thresholds, respectively [<xref ref-type="bibr" rid="c17" hwp:id="xref-ref-17-4" hwp:rel-id="ref-17">17</xref>]. Hence, the results in ProtTrans can be traced to having more variability and representation in UniRef50 [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-5" hwp:rel-id="ref-3">3</xref>]. Nevertheless, we wanted to test an intermediate value between the 50% and 100% thresholds that was not tested in ProtTrans, hence the justification for having the pre-training dataset amongst our experimentation independent variables later on with a value that is, UniRef90. We can refer to the pre-training data statistics in <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref> Sequences in both UniRef50 and UniRef90 are tokenized with a single space between each token in analogy to word boundaries and each sequence is stored on a separate line in analogy to sentences.</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2:</label><caption hwp:id="caption-6"><p hwp:id="p-25">Pre-training Data Statistics.</p></caption><graphic xlink:href="524265v1_tbl2" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap></sec><sec id="s4a2" hwp:id="sec-18"><label>4.1.2</label><title hwp:id="title-25">Pre-trained Model: Encoder-Decoder Transformer</title><p hwp:id="p-26">For our baseline model and throughout our experimentation, we utilize the encoder-decoder transformer originally proposed for machine translation and then for mapping an arbitrary input domain to a target domain [<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">19</xref>]. The encoder learns to project the input domain sequences into a laten-t/embedding space, representing the “context”. The decoder, however, learns to generate the target domain sequences given this context. Although later transformer releases abandoned the encoderdecoder combination and only utilized either of them, we draw upon ProtTrans’s experimentation promoting T5 [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">20</xref>] (the only encoder-decoder transformer analyzed) over encoder-only (e.g., BERT [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">21</xref>], ALBERT [<xref ref-type="bibr" rid="c22" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">22</xref>], and Electra [<xref ref-type="bibr" rid="c23" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">23</xref>]) and decoder-only transformers (e.g., TransformerXL [<xref ref-type="bibr" rid="c24" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">24</xref>] and XLNet [<xref ref-type="bibr" rid="c25" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">25</xref>]). However, the choice of encoder-decoder transformers in this work is not only motivated by the aforementioned top performance but also their compatibility with our experimentation independent variables such as, but not limited to, masking and architecture. Due to retaining both the encoder and decoder, T5-like transformers offer more compatibility with different masking strategies that are dependent on both masking and de-masking techniques. Furthermore and due to learning relative positional embeddings that are shared across layers for each attention head, T5-like transformers offer robustness against predictions surpassing the maximum length of the relative positional embedding dimension as it learns to combine the relative offset between lower layers’ amino acid subsets [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-2" hwp:rel-id="ref-20">20</xref>]. For the masking strategy adopted by the baseline, we performed a 1-gram random token masking according to the default probability of 15% (i.e., 15% of the sequence tokens are masked at random) and performed a full de-masking/reconstruction of the sequence (i.e., all tokens are reconstructed as individual tokens). For the number of encoderdecoder layers, we used 36 layers for each. For the activation function, we used Gated Gaussian Error Linear Unit (Gated-GELU) [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">15</xref>]. Regarding relative positional embedding, we have been using an embedding dimension of 32 and an embedding offset of 128. Finally, we pre-trained the baseline on UniRef50. The remainder of the baseline configurations can be found in <xref rid="tbl11" ref-type="table" hwp:id="xref-table-wrap-11-2" hwp:rel-id="T11">Table 11</xref>. Our baseline model represents <italic toggle="yes">Exp. 0</italic> that is later subjected to a single variable change at every class of experimentation.</p></sec></sec><sec id="s4b" hwp:id="sec-19"><label>4.2</label><title hwp:id="title-26">Downstream Tasks</title><p hwp:id="p-27">To provide a holistic analysis of Ankh’s performance -relatively and ultimately-, we conduct a protein downstream benchmark as well as a generation analysis for protein engineering applications. For the protein downstream benchmark, we measure the performance of Ankh in comparison to the top reported protein language models via selected downstream tasks covering various aspects of protein understanding and involving protein structure and function. For the generation analysis, we analyze the use of Ankh via two generation frameworks on two scales of data: High-N (Family-based) and One-Shot (single sequence-based) generation in terms of conservation-mutation trends, introducing diversity while retaining structural - and accordingly functional-identity, and sparsity-robustness. We unified the experimental training and testing settings and procedures of all the downstream tasks investigated in this study. Although we acknowledge task-specific optimization, this unification aims to specifically compare the level of protein understanding embodied in the protein representations generated by the studied models while avoiding any bias that can result from task-specific means of optimization. We can observe a demonstration of the adopted downstream tasks in <xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>. The following subsections are dedicated to the description of the tasks, task databases, and the predictive models/settings that utilized them.</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Fig. 5:</label><caption hwp:id="caption-7"><p hwp:id="p-28">A Demonstration of Structure &amp; Function Benchmarks: We showcase all the adopted downstream tasks (a) In SSP, the input is the protein sequence and the output is a per-residue classification that spans either 3 or 8 states. (b) In FolP, the input is the protein sequence and the output is a per-protein classification that spans 1194 possible folds. In CP, the input is the protein sequence that is processed as residue pairs and the output is a binary classification indicating whether or not the designated residues contact. (d) In FluP, the input is the protein sequence and the output is a regression score indicating the fluorescence intensity. (e) In SolP, the input is the protein sequence and the output is a binary classification indicating whether or not the protein is soluble. (f) In LocP, the input is the protein sequence and the output is a per-protein classification that spans 10 classes. (g) In EAT, the input is annotated query protein and the output is CATH annotation transferred from the best match in annotated lookup dataset (h) In Novel Generation, the input is a protein sequence and the output is a variant of the input protein that maintains the same desired function. (i) In GB1 Fitness Prediction, the input is a protein sequence corresponding to mutations at four possible positions and the output is a regression score indicating the fitness prediction</p></caption><graphic xlink:href="524265v1_fig5" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><sec id="s4b1" hwp:id="sec-20"><label>4.2.1</label><title hwp:id="title-27">Tasks and Datasets</title><p hwp:id="p-29">The emergence of transformers as powerful self-surprised models for proteins has motivated significant efforts in designing comprehensive bench-marking databases for protein sequence understanding, like TAPE [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">26</xref>], and PEER [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">27</xref>]. These efforts are necessary to drive the progress of transformers toward protein understanding, parallel to how comprehensive benchmarks have driven the progress of transformers in natural language understanding. We provide a summary of the downstream dataset statistics in <xref rid="tbl12" ref-type="table" hwp:id="xref-table-wrap-12-1" hwp:rel-id="T12">Table 12</xref>. We selected a set of commonly-utilized downstream tasks that fall into three groups: Protein Function Prediction, Protein Structure Prediction, and Protein Localization Prediction. We further extended the validity of the comparison by adding independent testing databases to some of the downstream tasks. Besides the bench-marking tasks, we also investigated the protein variant generation capabilities of Ankh in two settings: protein family-based generation and single protein-based generation.</p></sec><sec id="s4b2" hwp:id="sec-21"><label>4.2.2</label><title hwp:id="title-28">Protein Function Prediction</title><p hwp:id="p-30">This group of tasks aims to evaluate the ability of protein embeddings in capturing the functional scores of two critical design parameters of protein engineering, fluorescence, and solubility.</p><sec id="s4b2a" hwp:id="sec-22"><title hwp:id="title-29">Fluorescence Prediction (FluP)</title><p hwp:id="p-31">This regression task evaluates the fluorescence intensity of green fluorescent protein mutants annotated by Sarkisyan et al [<xref ref-type="bibr" rid="c28" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">28</xref>]. Fluorescence is an important biological function as it allows researchers to infer the existence of proteins in cell lines and living organisms [<xref ref-type="bibr" rid="c27" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">27</xref>]. Prediction of the effect of mutations on the green fluorescent protein is a common example to investigate, representing the protein genotype/phenotype fitness landscape prediction problem in protein engineering research. We follow the same splits of TAPE [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">26</xref>] as they are designed to test the model generalization ability from lower-order mutations to higher-order mutations. The training and evaluation datasets contain only mutants with three or fewer mutations while the testing contains mutants with four mutations or more.</p></sec><sec id="s4b2b" hwp:id="sec-23"><title hwp:id="title-30">Solubility Prediction (SolP)</title><p hwp:id="p-32">This classification task evaluates the binary label of a set of dissimilar proteins as soluble or not. Solubility is an indispensable design parameter for effective proteins, especially for pharmaceuticals [<xref ref-type="bibr" rid="c29" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">29</xref>]. We adopted the solubility database utilized in the development of DeepSol [<xref ref-type="bibr" rid="c30" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">30</xref>] with the same dataset splits, where any protein with ≥ 30% sequences identity to any protein in the testing subset is removed from the training and evaluation subsets.</p></sec><sec id="s4b2c" hwp:id="sec-24"><title hwp:id="title-31">GB1 Fitness Prediction (GB1)</title><p hwp:id="p-33">This regression task evaluates the fitness of GB1 binding following mutations in 4 specific positions curated in the FLIP benchmark [<xref ref-type="bibr" rid="c31" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">31</xref>]. GB1 is the binding domain of the immunoglobulin-binding Protein G, used in antibody purification. GB1 landscape is considered the gold standard in studying the non-additive interactions of mutations, termed epistasis [<xref ref-type="bibr" rid="c32" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">32</xref>]. Unlike the fluorescence mutants, the GB1 mutants are confined only to four positions, which is why it is selected to represent another common case in protein engineering research. In FLIP, 149,361 GB1 mutants with measured binding scores were down-sampled to 8,733 as 96% of the original data points were non-binder or poor-binder. The sampled split was utilized to evaluate the investigated PLMs as it provided the most stable results.</p></sec></sec><sec id="s4b3" hwp:id="sec-25"><label>4.2.3</label><title hwp:id="title-32">Protein Structure Prediction</title><p hwp:id="p-34">This group of tasks aims to evaluate the ability of the sequence-based embeddings of a protein to encompass accurate information about its structure. A sequential chain of amino acids folds into a set of predetermined stable three-dimensional structures. The large majority of biological parameters of a protein can be inferred from its structure [<xref ref-type="bibr" rid="c33" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">33</xref>]. Consequently, this group of tasks is esteemed due to its correlation to protein understanding, and the large set of applications it enables.</p><sec id="s4b3a" hwp:id="sec-26"><title hwp:id="title-33">Contact Prediction (CP)</title><p hwp:id="p-35">This is a binary classification task, where pairs of residues are predicted to be in contact in their 3D structure (commonly defined with an 8Å distance threshold) or not. Contact prediction, as a residue-level 3D structure prediction, provides significant global information about the protein structure. In literature, contact prediction is utilized as an intermediate prediction step toward atom-level 3D structure prediction. We utilized ProteinNet [<xref ref-type="bibr" rid="c34" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">34</xref>], a standardized dataset for structure learning, whose approach is to piggyback on the CASP competitions [<xref ref-type="bibr" rid="c35" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">35</xref>]. ProteinNet uses the CASP structures as a testing set and augments all the historical records of structures released before the CASP dates as training and evaluation sets. We utilized the latest version of ProteinNet that uses CASP12 as its test set with the same dataset splits as TAPE [<xref ref-type="bibr" rid="c26" hwp:id="xref-ref-26-3" hwp:rel-id="ref-26">26</xref>]. To further asset the robustness of the model training, we add independent testing based on the free modeling (FM) structures of the latest CASP competition, CASP14.</p></sec><sec id="s4b3b" hwp:id="sec-27"><title hwp:id="title-34">Fold Prediction (FolP)</title><p hwp:id="p-36">This is a classification task, where a full protein sequence is classified into 1194 possible folds. This task is utilized in the detection of emergent remote homologs of proteins of interest like new antibiotic-resistant genes, and industrial enzymes [<xref ref-type="bibr" rid="c36" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">36</xref>]. We adopted from Hou’s dataset [<xref ref-type="bibr" rid="c37" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">37</xref>], with its original splits. Entire clustered superfamilies are held for the testing dataset to affirm the models’ generalization ability to detect the structural similarity of drastically different sequences.</p></sec><sec id="s4b3c" hwp:id="sec-28"><title hwp:id="title-35">Secondary Structure Prediction (SSP)</title><p hwp:id="p-37">This is a classification task, where each residue in a protein is classified into its secondary structure fold with two levels of difficulty: 3-classes, and 8-classes. Secondary structures hold significant information about functional domains and are commonly utilized to capture evolutionary information through multiple sequence alignment. We utilized the training and evaluation set from NetSurfP-2.0 [<xref ref-type="bibr" rid="c38" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">38</xref>] and used a variety of testing sets to affirm the robustness of the model including CB513 [<xref ref-type="bibr" rid="c39" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">39</xref>], TS115 [<xref ref-type="bibr" rid="c40" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">40</xref>], and CASP12 [<xref ref-type="bibr" rid="c41" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">41</xref>], and CASP14 [<xref ref-type="bibr" rid="c42" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">42</xref>].</p></sec><sec id="s4b3d" hwp:id="sec-29"><title hwp:id="title-36">Embedding-based Annotation Transfer (EAT)</title><p hwp:id="p-38">Protein annotation transfer from labeled (experimentally-annotated) proteins to unlabeled proteins traditionally employed Homology-based inference (HBI) in sequence space. Recently, embedding-based annotation transfer has emerged as an alternative faster approach, as it does not require multiple sequence alignment (MSA) calculations. In the new framework, the distances between query proteins and a lookup set of annotated proteins are calculated to transfer annotation from the most similar known match through k-Nearest Neighbors (k-NN) in embedding space [<xref ref-type="bibr" rid="c43" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">43</xref>, <xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">44</xref>]. This task evaluates the ability of raw embeddings to capture meaningful information about the proteins without the need for supervised training. We utilized a bench-marking set of 69k look-up sequences and 219 test sequences that were developed for ProTucke evaluation [<xref ref-type="bibr" rid="c44" hwp:id="xref-ref-44-2" hwp:rel-id="ref-44">44</xref>]. This evaluation dataset is curated from CATH v4.3 dataset, where proteins are classified into four levels of structural annotations: Class, Architecture, Topology, and Homologous super-family [<xref ref-type="bibr" rid="c45" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">45</xref>]. For simplicity, we report the mean of the four accuracy scores as a performance measure for this task in <xref ref-type="table" rid="tbl1" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table. 1</xref>. The performance over the four classes was consistent with the mean as seen in <xref rid="tbl16" ref-type="table" hwp:id="xref-table-wrap-16-2" hwp:rel-id="T16">Table 16</xref>.</p></sec></sec><sec id="s4b4" hwp:id="sec-30"><label>4.2.4</label><title hwp:id="title-37">Protein Localization Prediction</title><p hwp:id="p-39">This task aims to evaluate the ability of protein embeddings to capture where a protein is expected to accumulate in the cell, known as protein localization prediction [<xref ref-type="bibr" rid="c46" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">46</xref>]. This attribute is significant for understanding protein functions, especially in disease target identification studies.</p><sec id="s4b4a" hwp:id="sec-31"><title hwp:id="title-38">Sub-cellular localization prediction (LocP)</title><p hwp:id="p-40">This classification task evaluates the localization of a protein into 10 sub-cellular classes. We utilize the DeepLoc [<xref ref-type="bibr" rid="c47" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">47</xref>] dataset with the same dataset splits described in their paper.</p></sec></sec><sec id="s4b5" hwp:id="sec-32"><label>4.2.5</label><title hwp:id="title-39">Generation of Novel Protein Sequences</title><p hwp:id="p-41">Following the comparative study, we utilized the <italic toggle="yes">Ankh</italic> model to generate synthetic variants of natural proteins to affirm its applicability in a crucial protein engineering task, computational variant generation. We evaluate Ankh with two input datasets representing two different settings and scales in protein engineering: family-based and single sequence-based variant generation.</p><sec id="s4b5a" hwp:id="sec-33"><title hwp:id="title-40">High-N (Family-Based Variant Generation)</title><p hwp:id="p-42">For the family-based use case, we utilized a curated dataset of malate dehydrogenase (MDH), which was utilized as the training dataset of ProteinGAN [<xref ref-type="bibr" rid="c48" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">48</xref>]. ProteinGAN is a recent deep learning-based generative model that showed superior performance compared with experimental variant generation in the case of MDH. This choice of this protein family is convenient given its diverse 16,706 unique training sequences as well as the complexity of enzyme catalysis as it requires binding to both its substrate and the NAD+ cofactor, which add complexity to the generation process</p></sec><sec id="s4b5b" hwp:id="sec-34"><title hwp:id="title-41">One-Shot (Single Sequence Variant Generation)</title><p hwp:id="p-43">For the single sequence use case, we used single chain SARS-Cov-2 nanobody that was added to the CoV-AbDab dataset after June 2022 [<xref ref-type="bibr" rid="c49" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">49</xref>]. This is to ensure that they are new sequences the model did not see in its unsupervised training. This one-shot generation use case specifically challenges the model’s generalization capability by demanding it to generate variants of a small-scale dataset without over-fitting on the dataset in question. We conducted seven independent virtual generation experiments, utilizing seven different nanobodies Cov-AbDab identified by the following names: Nb-007, F6, Nb 1-23, Nb 1-25, Nb 2-62, Nb 2-65, and Nb 2-67. All of the selected nanobodies have experimentally validated structures to facilitate their comparison with the predicted structures of the generated candidate nanobodies.</p></sec></sec></sec><sec id="s4c" hwp:id="sec-35"><label>4.3</label><title hwp:id="title-42">Downstream Model: ConvBERT</title><p hwp:id="p-44">For our implementation of the top/supervised models mapping the pre-trained embeddings to the designated supervised targets, we utilized the same supervised network with very few modifications to account for the differences in protein processing levels (e.g., residue-level and entire protein-level) and output distributions (e.g., binary classification, multi-class classification, and regression). In all cases, our top/supervised models consist of two types of layers. The choice of the first type draws-upon previous experimentation done in ProtTrans promoting CNNs as top/downstream models/layers that are proven to perform better when coupled with self-attention [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-6" hwp:rel-id="ref-3">3</xref>]. We utilize a ConvBERT layer with the same embedding dimension as the pre-trained model, a feed-forward network dimension of pre-trained embeddings divided by 2 (i.e., if we’re benchmarking with ESM-1b whose embedding dimension is 1280 then the feed-forward network dimension will be 640), 4 attention heads, a dropout rate of 0.2, convolutional kernel size of 7, and a Gated-GELU activation [<xref ref-type="bibr" rid="c50" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">50</xref>]. The second type is linear layers whose activation varies between None, Sigmoid, and Softmax for regression, binary classification, and multi-class classification, respectively. The third type of layers, however, is not shared across all cases. In fact, we only used a global max pooling layer only in regression and binary classification tasks prior to/at the beginning of the aforementioned supervised network. Generally, we acknowledge -and promote-task-specific optimization. Accordingly, we acknowledge that different top models with different set of hyper-parameters and configurations can result in a better downstream performance. Nevertheless, we unify the setting of this top model believed to achieve the better generalized performance as the core of the downstream bench-marking is to evaluate and compare the level of protein understanding embodied each model’s learned protein representations.</p></sec><sec id="s4d" hwp:id="sec-36"><label>4.4</label><title hwp:id="title-43">Variant Generation Model</title><p hwp:id="p-45">The proposed auto-regressive fine-tuning generation framework for the High-N (protein familybased generation) scale offers an accessible approach for variant generation, that can be easily scaled across different protein families. Moreover, the framework offers easy control over the exploration-exploitation trade-off of the generation, by manipulating the logit warping temperature parameter. To validate the framework, the same fine-tuned model was first utilized to generate three datasets of 500 sequences, utilizing three different temperatures (1.0, 1.5, and 2.0). For the three datasets, the Shannon entropy curves between the generated set and a multi-sequence alignment (MSA)of the natural set are reported. Shannon Entropy aims to characterize if the generated sets preserve the evolutionary sequence properties of the natural MDH set by comparing their representative statistics of amino acid variation. Low entropy values reflect conserved residues governing retained functionality whereas high entropy values reflect less conserved regions with higher mutation rates. We can observe in <xref rid="fig2" ref-type="fig" hwp:id="xref-fig-2-6" hwp:rel-id="F2">Figure 2</xref>, the three generated sets show high similarity to the distribution of the natural sequences with almost identical positions for both peaks and valleys. MSE between entropy values of natural and generated sets are quantified as 0.1, 0.09, and 0.08 for a generation with temperatures 1.0, 1,5, and 2.0 respectively. We emphasize that the reported similarity is calculated based on generated set of 500, which is less than 3% of the total sequences of the natural set (16,700). In other words, the model is able to mimic the distribution of the natural set with a small portion of its original sequences.</p><p hwp:id="p-46">Since variant generation tasks adopt a framework that is different from the aforementioned pretrained-top model framework and evaluation settings, we present it solely. We adopt two frameworks for variant generation, Auto-Regressive Fine-Tuning for High-N (family-based generation) and Masked Language Modeling (MLM) for One-Shot (single protein-based generation).</p><p hwp:id="p-47">Fine-tuning denotes specializing the pre-trained model on a specific dataset/task(s) [<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">51</xref>]. We define auto-regressive fine-tuning, however, by adding a constraint to the classic fine-tuning setting that denotes completely freezing the encoder and only allowing for the decoder’s parameter to change. We allow this for the entirety of the decoder’s layers and initialize the fine-tuning by the original decoder parameters. We train each experiment for 2 epochs, shifting it from an masked language modeling prediction into an auto-regressive prediction. We set a maximum sequence length of 256 tokens and a maximum prompt length of 20 tokens. We utilize a learning rate of 3<italic toggle="yes">e</italic> −4 and an epsilon value of <italic toggle="yes">e</italic> − 8 for the Adam optimizer. Moreover, we use a train batch size of 4 and an evaluation batch size of 8. For the auto-regressive sampling, we use beam search with a number of beams of 10. For the auto-regressive logit warping, we use temperature with three different temperature values of 1, 1.5, and 2 to observe the behavior of the model under different temperatures. The mixture of beam search sampling and temperature logit warping works as follows: Firstly, temperature changes the logit distribution, preserving the order of the tokens but smoothing/sharpening the distribution. Secondly, sampling occurs where the beams are scored (greedily, in our case).</p><p hwp:id="p-48">MLM denotes a pre-training objective that guides the model’s learning -and accordingly its inference-of token representations by requiring it to predict a random sample of input tokens that is usually replaced by a [<italic toggle="yes">MASK</italic>] place-holder(s) in a multi-class setting over the entire vocabulary [<xref ref-type="bibr" rid="c51" hwp:id="xref-ref-51-2" hwp:rel-id="ref-51">51</xref>]. However and for fine-tuning purposes, we use MLM in the context of inference only (i.e., no training or change of parameters of any kind has been done). Furthermore and unlike the commonly-used small values of masking probabilities for pre-training purposes, we perform two experiments where we use larger masking probabilities corresponding to a bigger range of variations in the original sequences. The two experiments try different variations of the exploration-exploitation trade-offs. The first experiment attempted a higher masking probability of 50% and a lower temperature logit warping of 1.0. The second experiment utilized a lower masking probability of 40% and a higher temperature logit warping of 1.5. Both experiments utilized a beam search sampling with 30 beams.</p></sec><sec id="s4e" hwp:id="sec-37"><label>4.5</label><title hwp:id="title-44">Computational Power (Software &amp; Hardware)</title><p hwp:id="p-49">Processing a limited vocabulary that incorporates unlimited life within its tokens, we had to also reach out for the limits of up-scaling the efficiency of our computational power in terms of both software and hardware.</p><sec id="s4e1" hwp:id="sec-38"><label>4.5.1</label><title hwp:id="title-45">Flax &amp; JAX</title><p hwp:id="p-50">Flax is an end-to-end high-performance library and an ecosystem for JAX that is designed for flexibility and tailored for neural networks. JAX opts for a plausible range of composable function transformations, allowing just-in-time (JIT) compilation, automatic differentiation, CPU/GPU/TPU compatibility, and automatic vectorization and parallelization [<xref ref-type="bibr" rid="c52" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">52</xref>]. JAX, for example, can deliver approximately 1.4x speed-up for language model training on TPU Pods compared to PyTorch. We used Flax to build our source codes to leverage the aforementioned advances during training the PLMs on Google TPU Pods but made our models available on HuggingFace to support a wider range of researchers, which use any of the three top deep learning libraries: JAX, TensorFlow, and PyTorch.</p></sec><sec id="s4e2" hwp:id="sec-39"><label>4.5.2</label><title hwp:id="title-46">TPUs</title><p hwp:id="p-51">We were fortunate enough to be amongst the first manuscripts, in general, and to be the first protein modeling work, in particular, utilizing Google’s latest TPU v4 and unleashing unseen capabilities of supercomputers. This means all the pre-trained models on this work were trained using Google TPU v4 Pods with either 64 or 128 cores. The single TPU v4 VM host has 8 TPU cores, and each core has 16 GiB of high-bandwidth memory, 120 CPU cores, and 400 GB of main memory. At first glance, it seems TPU V4 is similar to TPU V3; however, it has two main advantages over TPU V3. First, the new mega core features allow the virtual merge of 2 cores, making deep learning libraries like JAX sees every two cores as one core. This gives each of the four merged cores per host access to 32 GiB of memory, which leads to fit- ting bigger models up to 3 billion on a single host without the need to use model parallelism. Second, TPU V4 can deliver approximately 2.2x speedup compared to TPU V3.</p></sec></sec><sec id="s4f" hwp:id="sec-40"><label>4.6</label><title hwp:id="title-47">Data &amp; Model Experimentation</title><sec id="s4f1" hwp:id="sec-41"><label>4.6.1</label><title hwp:id="title-48">Masking</title><p hwp:id="p-52">Masking is a pre-training objective that guides the model’s learning of token representations by demanding it to predict a random sample of input tokens that is usually replaced by a [<italic toggle="yes">MASK</italic>] placeholder(s) in a multi-class setting over the entire vocabulary [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-2" hwp:rel-id="ref-21">21</xref>]. This class of model experimentation aimed to investigate the impact of two masking-related parameters, masking strategy and masking probability.</p><sec id="s4f1a" hwp:id="sec-42"><label>a.</label><title hwp:id="title-49">Masking Strategy</title><p hwp:id="p-53">Masking strategy indicates the means by which we decide which tokens to mask and which to keep unmasked [<xref ref-type="bibr" rid="c53" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">53</xref>–<xref ref-type="bibr" rid="c55" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">55</xref>]. Motivated by the skewed distribution of amino acid tokens in protein sequences in addition to the redundancy in the database, we have tested different means of masking strategies to ensure protein-specific adoption. For that purpose, we tested out five variations/experiments.</p><p hwp:id="p-54"><bold>Exp. 0:</bold> This experiment represents our baseline model whose details can be found in Sub-Section 3.1.2.</p><p hwp:id="p-55"><bold>Exp. 1:</bold> We masked every unique 1-gram token (i.e., every unique amino acid) in the sequence at least once. We did so by repeatedly iterating the sequence and randomly masking an amino acid at a time providing that neither the desired masking probability has yet been reached nor that the amino acid in question is the one with the highest count (i.e., if we are using a 15% masking probability for the sequence “<italic toggle="yes">ABCAAAAAAA. A</italic>” whose length is 20 amino acids: token “<italic toggle="yes">A</italic>” will always be masked one time and the remaining two masks will always be for tokens “<italic toggle="yes">B</italic>” and “<italic toggle="yes">C</italic>”). Finally, we performed a full de-masking/reconstruction of the whole sequence tokens. This has been found to achieve higher performance w.r.t. the baseline and accordingly was proceeded with.</p><p hwp:id="p-56"><bold>Exp. 2:</bold> We have also masked every unique token in the sequence at least once but instead of masking one token at a time, we masked its precedent and subsequent tokens (i.e., in the sequence “<italic toggle="yes">ABCDEFG</italic>”: if token “<italic toggle="yes">D</italic>” is masked, then tokens “<italic toggle="yes">C</italic>” and “<italic toggle="yes">E</italic>” are also masked), turning it from a 1-gram token masking to 3-gram token masking. We also retained the full de-masking/reconstruction of the whole sequence tokens. This, however, reduced the performance in all the downstream tasks and accordingly was not proceeded with.</p><p hwp:id="p-57"><bold>Exp. 3:</bold> We replicated <italic toggle="yes">Exp. 1</italic> indicating the masking of every unique 1-gram token in the sequence at least once. However, as opposed to the default scenario where all tokens’ de-masking/reconstruction is incorporated in the calculation of the loss function, we have only incorporated the reconstruction of the masked tokens even though the output will contain the entirety of the sequence tokens (i.e., in the sequence “<italic toggle="yes">ABCDEFG</italic>”: if tokens “<italic toggle="yes">D</italic>” and “<italic toggle="yes">F</italic> “ are masked, then only the reconstruction of tokens “<italic toggle="yes">D</italic>” and “<italic toggle="yes">F</italic> “ is accounted for the calculation of the loss function). However, this reduced the performance in all the downstream tasks and deducing that we should reconstruct the entire input, even if it is already known, and accordingly was not proceeded with.</p><p hwp:id="p-58"><bold>Exp. 4:</bold> This experiment reflected a change in the input-target mapping of masking and de-masking. To elaborate, every input token is reconstructed as a single target token in the default 1-gram token masking case. The change we performed was to reconstruct all the consecutive unmasked tokens as a single merged token (i.e., if the input sequence was “<italic toggle="yes">ABCDEFG</italic>” and tokens “<italic toggle="yes">C</italic>” and “<italic toggle="yes">G</italic>” were masked then the sequence was inputted as “<italic toggle="yes">A, B</italic>, [<italic toggle="yes">MASK</italic><sub>0</sub>], <italic toggle="yes">D, E, F</italic>, [<italic toggle="yes">MASK</italic><sub>1</sub>]” and reconstructed as “[<italic toggle="yes">MASK</italic><sub>0</sub>], <italic toggle="yes">C</italic>, [<italic toggle="yes">MASK</italic><sub>1</sub>], <italic toggle="yes">G</italic>” where [<italic toggle="yes">MASK</italic><sub>0</sub>] is a single target token mapping two input tokens (AB) and [<italic toggle="yes">MASK</italic><sub>1</sub>] is a single target token mapping three input tokens (DEF), turning it from a 1-gram token masking into a 1-gram span masking. The change was motivated by reducing the computational cost of the unneeded computations associated with the reconstruction of unmasked tokens of the output. We refer to this masking strategy as “1-Gram Span Partial Demasking/Reconstruction”. It is important to note that this partial reconstruction is done only on the output as the input tokens are left as is. This direction has proven to be a valid one, corresponding to higher performance w.r.t. the first experiment and accordingly was proceeded with for the upcoming experiments.</p><p hwp:id="p-59"><bold>Exp. 5:</bold> We applied the 1-gram span partial reconstruction -introduced in <italic toggle="yes">Exp. 4</italic> - on the approach of <italic toggle="yes">Exp. 1</italic>, indicating the masking of every unique 1-gram token in the sequence at least once and the reconstruction of all the unmasked tokens as a single merged token (i.e., if we are using a 15% masking probability for the sequence “<italic toggle="yes">ABCAAAAAAA</italic>….<italic toggle="yes">A</italic>” whose length is 20 amino acids and if the first random index was the zeroth index then the sequence will be inputted as “[<italic toggle="yes">MASK</italic><sub>0</sub>], [<italic toggle="yes">MASK</italic><sub>1</sub>], [<italic toggle="yes">MASK</italic><sub>2</sub>], <italic toggle="yes">A, A, A, A</italic>,, <italic toggle="yes">A</italic>” and reconstructed as “<italic toggle="yes">A, B, C</italic>, [<italic toggle="yes">MASK</italic><sub>0</sub>]” where “[<italic toggle="yes">MASK</italic><sub>0</sub>]” is a single token mapping seventeen input tokens). This, however, was shown to reduce the performance, consistently, and was therefore discarded.</p><p hwp:id="p-60"><bold>Exp. 6:</bold> We have tried a variant of <italic toggle="yes">Exp. 4</italic> in terms of the partial reconstruction where we mapped all the consecutively-masked tokens into a single token upon reconstruction (i.e., if the input sequence was “<italic toggle="yes">ABCDEFG</italic>” and the tokens “<italic toggle="yes">C</italic>”, “<italic toggle="yes">D</italic>”, and “<italic toggle="yes">E</italic>” were masked then the sequence was inputted as “<italic toggle="yes">A, B</italic>, [<italic toggle="yes">MASK</italic><sub>0</sub>], <italic toggle="yes">F, G</italic>” and reconstructed as “[<italic toggle="yes">MASK</italic><sub>0</sub>], [<italic toggle="yes">MASK</italic><sub>1</sub>], [<italic toggle="yes">MASK</italic><sub>2</sub>]”, where “[<italic toggle="yes">MASK</italic><sub>1</sub>]” is a single token mapping three masked tokens, [<italic toggle="yes">MASK</italic><sub>0</sub>] and [<italic toggle="yes">MASK</italic><sub>2</sub>] are single tokens mapping two unmasked tokens) turning it into a 3-gram span masking. This change, too, was motivated by reducing the computational cost but has shown to be an invalid direction and accordingly was not proceeded with.</p><p hwp:id="p-61">Hence, it can be deduced that the top performing version of the six tested versions was the version of <italic toggle="yes">Exp. 4</italic>, where we reconstruct all the consecutive unmasked tokens as a single merged token. Therefore, this was the version that carried on to the following sub-set of experiments, masking probability. The results for this set of experiments can be found in <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3</xref>.</p><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1 xref-table-wrap-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3:</label><caption hwp:id="caption-8"><p hwp:id="p-62">Masking Strategy Experiment Statistics</p></caption><graphic xlink:href="524265v1_tbl3" position="float" orientation="portrait" hwp:id="graphic-8"/></table-wrap></sec><sec id="s4f1b" hwp:id="sec-43"><label>b.</label><title hwp:id="title-50">Masking Probability</title><p hwp:id="p-63">Masking probability indicates the ratio of tokens to be masked out of the entire sequence length. As indicated in the baseline model’s configurations, the default masking probability is 15% [<xref ref-type="bibr" rid="c21" hwp:id="xref-ref-21-3" hwp:rel-id="ref-21">21</xref>]. Here, we have experimented with three additional values on the top-performing version of <italic toggle="yes">Exp. 4</italic>.</p><p hwp:id="p-64"><bold>Exp. 7:</bold> The first tested masking probability was 10%.</p><p hwp:id="p-65"><bold>Exp. 8:</bold> The second tested masking probability was 20%.</p><p hwp:id="p-66"><bold>Exp. 9:</bold> The third tested masking probability was 30%.</p><p hwp:id="p-67">Out of the four values, 10% was the worst masking probability and was accordingly disregarded. Interestingly, it was found that the default value of <italic toggle="yes">Exp. 4</italic> (15% masking probability) was outperforming for the sub-cellular localization prediction, fold prediction, as well as some of the secondary structure prediction tasks for datasets such as <italic toggle="yes">CB513</italic> and TS115. Nevertheless, it was found that the value of <italic toggle="yes">Exp. 9</italic> (30% masking probability) was outperforming for the entirety of the contact prediction tasks in addition to the secondary structure prediction tasks for CASP12 dataset. Given the inconsistency amongst the secondary structure prediction dataset results, we referred to the results on CASP12 -being a domain standard-that promotes the higher masking probability. Furthermore and as we are opting for holistic predictions via a general-purpose language model, we promoted generalization across different types of tasks, especially when the difference amongst results is of such a small magnitude and when acknowledging that task-specific customization is of impact. Finally, given that all the experimental variations were trained for only two epochs, we proceeded with the intermediate value of <italic toggle="yes">Exp. 8</italic> (20% masking probability) for the post-experimentation long-term training to fulfill the semi-comprehensive inclusion of different polls of tasks anticipated from a general-purpose protein language model that can then be customized per downstream task. The results for this set of experiments can be found in <xref rid="tbl4" ref-type="table" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Table 4</xref>.</p><table-wrap id="tbl4" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1 xref-table-wrap-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tbl4</object-id><label>Table 4:</label><caption hwp:id="caption-9"><p hwp:id="p-68">Masking Probability Experiment Statistics</p></caption><graphic xlink:href="524265v1_tbl4" position="float" orientation="portrait" hwp:id="graphic-9"/></table-wrap></sec><sec id="s4f1c" hwp:id="sec-44"><title hwp:id="title-51">Architecture</title><p hwp:id="p-69">Since we are utilizing an encoder-decoder transformer architecture, the architecture variations we target correspond to the number of encoder and decoder layers, different combinations of depth and width variations, and the means by which the model learns the order of tokens (i.e., positional embeddings).</p></sec><sec id="s4f1d" hwp:id="sec-45"><label>a.</label><title hwp:id="title-52">Number of Encoder-Decoder layers</title><p hwp:id="p-70">Although the presence of a decoder is essential in improving the representations produced by the encoder, previous runs of Prot-T5, deduced that the decoder did not provide any notable difference in most of the downstream tasks and was accordingly eliminated which, in turn, cut down the inference cost to almost half [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-7" hwp:rel-id="ref-3">3</xref>]. This motivated experimenting with an encoder whose number of layers is larger than the decoder’s with varying extents as well as testing out the opposite to exclude any refutations. It is noteworthy to mention that we maintained the same total number of layers of the encoder and decoder combined, that is, 72 layers corresponding to 36 layers each in the previous set of experiments. Proceeding from the top-performing version so far of <italic toggle="yes">Exp. 7</italic>, we tested three variations of encoder-decoder’s relative number of layers.</p><p hwp:id="p-71"><bold>Exp. 10:</bold> We initially experimented with having an encoder with 54 layers and a decoder with 18 layers.</p><p hwp:id="p-72"><bold>Exp. 11:</bold> We then experimented with having an encoder with 48 layers and a decoder with 24 layers.</p><p hwp:id="p-73"><bold>Exp. 12:</bold> We finally experimented with having a decoder with 48 layers and an encoder with 24 layers.</p><p hwp:id="p-74">It was found that the version with a 48-layer encoder and 24-layer decoder -<italic toggle="yes">Exp. 11</italic> - out- performed the version of <italic toggle="yes">Exp. 8</italic> in all of the secondary structure prediction tasks (8-states), the fold and sub-cellular localization prediction, and the overall mean and median whilst the other two versions demonstrated a fluctuating performance depending on the task. Eventually, we proceeded with <italic toggle="yes">Exp. 11</italic> (48-layer encoder and 24-layer decoder) to the subsequent set of experiments, unlocking a gain in extracting richer embeddings -as a result of a bigger encoder-with the same total cost of equal-sized encoder-decoder. Our choice was mainly motivated by promoting generalization embodied in the need to retain an adequate number of decoder layers due to their importance in a broad class of generation tasks as well as pooling the majority of task datasets. Moreover, our choice was also motivated by computational efficiency embodied in the smaller number of encoder layers resulting in faster feature extraction. The results for this set of experiments can be found in <xref rid="tbl5" ref-type="table" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">Table 5</xref>.</p><table-wrap id="tbl5" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1 xref-table-wrap-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tbl5</object-id><label>Table 5:</label><caption hwp:id="caption-10"><p hwp:id="p-75">Architecture Experiments: No. of Encoder and Decoder Layers</p></caption><graphic xlink:href="524265v1_tbl5" position="float" orientation="portrait" hwp:id="graphic-10"/></table-wrap></sec><sec id="s4f1e" hwp:id="sec-46"><label>b.</label><title hwp:id="title-53">Depth vs. Width Variation layers</title><p hwp:id="p-76">Depth corresponds to the number of layers which, as demonstrated, corresponds to the encoder and decoder’s layers in the case of the transformer. Width, however, corresponds to the embedding dimension in the transformer’s context [<xref ref-type="bibr" rid="c19" hwp:id="xref-ref-19-2" hwp:rel-id="ref-19">19</xref>].</p><p hwp:id="p-77"><bold>Exp. 13:</bold> The only experiment conducted w.r.t depth-width variations corresponds to increasing the embedding layer dimension from 768 to 1024 and accordingly reducing the depth from a 48-layered encoder &amp; 24-layered decoder to a a 24-layered encoder &amp; a 12-layered decoder to retain, approximately, similar or smaller number of parameters. This, however, corresponded to fluctuating results and accordingly was not proceeded with. We refer to the version with an embedding dimension of 768 as the base model or, <italic toggle="yes">Ankh base</italic>. The results for this set of experiments can be found in <xref rid="tbl6" ref-type="table" hwp:id="xref-table-wrap-6-1" hwp:rel-id="T6">Table 6</xref>.</p><table-wrap id="tbl6" orientation="portrait" position="float" hwp:id="T6" hwp:rev-id="xref-table-wrap-6-1 xref-table-wrap-6-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T6</object-id><object-id pub-id-type="publisher-id">tbl6</object-id><label>Table 6:</label><caption hwp:id="caption-11"><p hwp:id="p-78">Architecture Experiments: Depth vs. Width Variations</p></caption><graphic xlink:href="524265v1_tbl6" position="float" orientation="portrait" hwp:id="graphic-11"/></table-wrap></sec><sec id="s4f1f" hwp:id="sec-47"><label>c.</label><title hwp:id="title-54">Activation Function</title><p hwp:id="p-79">The Activation function is the function introducing non-linearity to the forward layer [<xref ref-type="bibr" rid="c56" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">56</xref>]. So far and throughout all the previous experiments, the activation function we have been using was Gated-GELU. It is important to note that Gated-GELU denotes a significantly larger trainable parameter size that, in turn, forces us to neutralize increases in width by decreases in depth, which we have done in <italic toggle="yes">Exp. 12</italic> [<xref ref-type="bibr" rid="c15" hwp:id="xref-ref-15-3" hwp:rel-id="ref-15">15</xref>]. To overcome the high parameter demand of the Gated-GELU forcing us to reduce the model’s depth, we have ought to change the activation function altogether so that we are not mandated to compromise significantly large depths. Thus, the remaining two experiments conducted w.r.t depth-width variations correspond to a change of the activation function to classic ReLU and testing two combos of depth and width [<xref ref-type="bibr" rid="c57" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">57</xref>]. Now, it may come to mind that varying depth/width alongside the activation function corresponds to two independent variables. However, it is important to note that this is traced to the nature of the Gated-GELU proposing a demand that no longer exists when the Gated-GELU is omitted as well as the constraint of maintaining the same number of parameters to opt for a computationally-fair comparison.</p><p hwp:id="p-80"><bold>Exp. 14:</bold> The first combo we tested is a depth of 62-layer encoder and 11-layer decoder with an embedding dimension of 768.</p><p hwp:id="p-81"><bold>Exp. 15:</bold> The second combo is a depth of 48-layer encoder and 24-layer decoder, also with an embedding dimension of 768.</p><p hwp:id="p-82">It was found that none of the combos pursued in the depth-width variation set of experiments consistently surpassed the top performer version, <italic toggle="yes">Exp. 11</italic>. Hence, none of this set’s combos were proceeded with and we reverted back to <italic toggle="yes">Exp. 11</italic>. The results for this set of experiments can be found in <xref rid="tbl7" ref-type="table" hwp:id="xref-table-wrap-7-1" hwp:rel-id="T7">Table 7</xref>.</p><table-wrap id="tbl7" orientation="portrait" position="float" hwp:id="T7" hwp:rev-id="xref-table-wrap-7-1 xref-table-wrap-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T7</object-id><object-id pub-id-type="publisher-id">tbl7</object-id><label>Table 7:</label><caption hwp:id="caption-12"><p hwp:id="p-83">Architecture Experiments: Activation Function</p></caption><graphic xlink:href="524265v1_tbl7" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap></sec><sec id="s4f1g" hwp:id="sec-48"><label>d.</label><title hwp:id="title-55">Relative Positional Embedding</title><p hwp:id="p-84">Positional embedding describes the location of sequence tokens so that each position is assigned a unique fixed representation, an essential assignment in the case of transformers as their fundamental idea is the replacement of sequential units with attention. [<xref ref-type="bibr" rid="c58" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">58</xref>]. Using a fixed positional embedding has several limitations such as not being able to extract embeddings for tokens exceeding the maximum length of the pre-defined positional embedding dimension, correlated to the embedding dimension [<xref ref-type="bibr" rid="c59" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">59</xref>]. To overcome this limitation, Relative Positional Embedding was introduced. Instead of utilizing a fixed embedding per position, Relative Positional Embedding assigns the sequence tokens variable and relative positional representations whose variability derives from the offset between the “key” and “query” compared in the self-attention setting. We utilize a simplified variation of the Relative Positional Embedding introduced in [<xref ref-type="bibr" rid="c20" hwp:id="xref-ref-20-3" hwp:rel-id="ref-20">20</xref>] where every representation is merely a scalar added to the denoting logit utilized for computing the attention weights. In this setting, all representations’ parameters are shared across all layers in our model although each attention head uses a different learned positional representation within a given layer. Although the learned embeddings/representations are relative and variable, the embedding dimension learned is fixed in the same experiment as it corresponds to a range of possible key-query offsets. So far and throughout the aforementioned experiments, we have been using 32 embeddings for all of our models. Furthermore, we have been using an offset of 128 tokens throughout which we assign all relative positions to the same embedding. It is noteworthy that a given layer is insensitive to a relative position beyond 128 tokens, but the following layers can still be sensitive to larger offsets by combining local information from previous layers, enabling the model to accommodate sequence lengths larger than the maximum predetermined length. This set of experiments corresponds to different combos of the embedding dimension as well as the embedding offset to ultimately test whether it is better to have a few large-sized embeddings or many small-sized ones, or something in between. The first two experiments in this set retained the default embedding dimension of 32 but varied the embedding offset.</p><p hwp:id="p-85"><bold>Exp. 16:</bold> The first experiment in this set retained the default embedding dimension, that is, 32 but increased the embedding offset to 256.</p><p hwp:id="p-86"><bold>Exp. 17:</bold> The second experiment in this set also retained the default embedding dimension of 32 but decreased the embedding offset to 64. It was found that the smaller embedding offset of 64 exceeded the performance of both 256 as well as the default value of 128. The following two experiments in this set retained the top-performing embedding offset of 64 but varied the embedding dimension.</p><p hwp:id="p-87"><bold>Exp. 18:</bold> The third experiment in this set retained the embedding offset of 64 but increased the embedding dimension to 64.</p><p hwp:id="p-88"><bold>Exp. 19:</bold> The fourth experiment in this set retained the embedding offset of 64 but decreased the embedding dimension to 16.</p><p hwp:id="p-89">Nevertheless, none of those variations consistently exceeded the performance of the embedding dimension of 32 and embedding offset of 64. As it was shown that doubles are performing better, the final two experiments in this set tried out two combos of doubles.</p><p hwp:id="p-90"><bold>Exp. 20:</bold> The fifth experiment tested an embedding offset of 128 and an embedding dimension of 64.</p><p hwp:id="p-91"><bold>Exp. 21:</bold> The final experiment of this set tested an embedding offset of 256 and an embedding dimension of 128.</p><p hwp:id="p-92">It was found that the combo with the most consistent and general results was that of <italic toggle="yes">Exp. 20</italic> (embedding offset of 128 and an embedding dimension of 64) and accordingly proceeded with as we refer to fold prediction when classification tasks are inconsistent and refer to <italic toggle="yes">CASP12</italic> when secondary structure dataset results are inconsistent. The results for this set of experiments can be found in <xref rid="tbl8" ref-type="table" hwp:id="xref-table-wrap-8-1" hwp:rel-id="T8">Table 8</xref>.</p><table-wrap id="tbl8" orientation="portrait" position="float" hwp:id="T8" hwp:rev-id="xref-table-wrap-8-1 xref-table-wrap-8-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL8</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T8</object-id><object-id pub-id-type="publisher-id">tbl8</object-id><label>Table 8:</label><caption hwp:id="caption-13"><p hwp:id="p-93">Architecture Experiments: Relative Positional Encoding</p></caption><graphic xlink:href="524265v1_tbl8" position="float" orientation="portrait" hwp:id="graphic-13"/></table-wrap></sec><sec id="s4f1h" hwp:id="sec-49"><label>e.</label><title hwp:id="title-56">Weight Tying</title><p hwp:id="p-94">Weight Tying originates from the motive of reducing the number of parameters associated with the training of the language models and accordingly, training and convergence time, and results consistency [<xref ref-type="bibr" rid="c60" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">60</xref>].</p><p hwp:id="p-95"><bold>Exp. 22:</bold> The mechanism by which we pursue the aforementioned motives is tying/sharing the weights and biases of the embedding and the decoder. However, it was found that this did not consistently surpass the results of the so-far top-performing model. This is traced to the difference between the input and output types due to how we masked and damask the input and output tokens, respectively, denoting higher prediction abilities from a setting with fewer parameters. The results for this set of experiments can be found in <xref rid="tbl9" ref-type="table" hwp:id="xref-table-wrap-9-1" hwp:rel-id="T9">Table 9</xref>.</p><table-wrap id="tbl9" orientation="portrait" position="float" hwp:id="T9" hwp:rev-id="xref-table-wrap-9-1 xref-table-wrap-9-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL9</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T9</object-id><object-id pub-id-type="publisher-id">tbl9</object-id><label>Table 9:</label><caption hwp:id="caption-14"><p hwp:id="p-96">Architecture Experiments: Weight Tying</p></caption><graphic xlink:href="524265v1_tbl9" position="float" orientation="portrait" hwp:id="graphic-14"/></table-wrap></sec></sec><sec id="s4f2" hwp:id="sec-50"><label>4.6.3</label><title hwp:id="title-57">Dataset</title><p hwp:id="p-97">In the experimentation of Prot-T5, it was found that <italic toggle="yes">UniRef50</italic> outperformed larger datasets such as <italic toggle="yes">UniRef100</italic> and <italic toggle="yes">BFD</italic> [<xref ref-type="bibr" rid="c3" hwp:id="xref-ref-3-8" hwp:rel-id="ref-3">3</xref>]. This was traced to the high quality of <italic toggle="yes">UniRef50</italic> in means of the lack of duplication and sequence diversity. Throughout all the previous experiments, the pre-training was also conducted on UniRef50. Yet, we wanted to test a dataset of intermediate size between UniRef50 and the bigger datasets which have proven less efficient.</p><p hwp:id="p-98"><bold>Exp. 23:</bold> We have conducted a single experiment on <italic toggle="yes">UniRef90</italic>. Nonetheless, the experiment has encouraged the initial direction of proceeding with <italic toggle="yes">UniRef50</italic> as representative for efficient high-quality attributes. It is noteworthy to mention that all previous experiments with <italic toggle="yes">UniRef50</italic> were trained for 2 full epochs in contrast to the experiment with <italic toggle="yes">UniRef90</italic> which was trained for only one epoch that is arguably equivalent. The results for this set of experiments can be found in <xref rid="tbl10" ref-type="table" hwp:id="xref-table-wrap-10-1" hwp:rel-id="T10">Table 10</xref>.</p><table-wrap id="tbl10" orientation="portrait" position="float" hwp:id="T10" hwp:rev-id="xref-table-wrap-10-1 xref-table-wrap-10-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL10</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T10</object-id><object-id pub-id-type="publisher-id">tbl10</object-id><label>Table 10:</label><caption hwp:id="caption-15"><p hwp:id="p-99">Dataset Experiments</p></caption><graphic xlink:href="524265v1_tbl10" position="float" orientation="portrait" hwp:id="graphic-15"/></table-wrap></sec></sec></sec></body><back><sec id="s5" hwp:id="sec-51"><label>5</label><title hwp:id="title-58">Availability</title><p hwp:id="p-100">Both the Ankh and <italic toggle="yes">Ankh base</italic> models are publicly available for research innovation at our Ankh repository “<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/agemagician/Ankh" ext-link-type="uri" xlink:href="https://github.com/agemagician/Ankh" hwp:id="ext-link-3">https://github.com/agemagician/Ankh</ext-link>“, under an Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. The repository also contains extensive python and Jupyter python notebook tutorials for various examples, including embedding extractions and supervised learning of several downstream tasks using freely available online resources (GoogleCo-lab). For commercial usage and licensing, please visit “<ext-link l:rel="related" l:ref-type="uri" l:ref="https://www.proteinea.com/" ext-link-type="uri" xlink:href="https://www.proteinea.com/" hwp:id="ext-link-4">https://www.proteinea.com/</ext-link>”</p></sec><ack hwp:id="ack-1"><label>6</label><title hwp:id="title-59">Acknowledgments</title><p hwp:id="p-101">The authors would like to thank the deep learning and bioinformatics teams in Proteinea for their invaluable help with hardware, software, and with many other aspects of this work. Mohammed AlQuraishi, Columbia University, for their feedback. From Google, the authors would like to thank Jonathan Caton, Shira Genauer, Astitva Chopra, and all Google cloud, Google innovator, JAX, and TRC Teams for helping to set up the project on Google Cloud and solving Google cloud issues. The models trained in this work could not be easily publicly available without support from the HuggingFace team, including Patrick von Platen, Julien Chaumond, and Clement Delangue. Google supported this project through Google research innovator and Google TPU Cloud Research Credits Program. We would like to thank all researchers worldwide who made all the datasets used in this research publicly available. Finally, ElNaggar and Essam would like to thank Allah for giving the strength, knowledge, and courage to finish this project and share it with the rest of humanity.</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-60">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><label>[1]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Vig Jesse"><surname>Vig</surname>, <given-names>Jesse</given-names></string-name> and <string-name name-style="western" hwp:sortable="Madani Ali"><surname>Madani</surname>, <given-names>Ali</given-names></string-name> and <string-name name-style="western" hwp:sortable="Varshney Lav R"><surname>Varshney</surname>, <given-names>Lav R</given-names></string-name> and <string-name name-style="western" hwp:sortable="Xiong Caiming"><surname>Xiong</surname>, <given-names>Caiming</given-names></string-name> and <string-name name-style="western" hwp:sortable="Socher Richard"><surname>Socher</surname>, <given-names>Richard</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rajani Nazneen Fatema."><surname>Rajani</surname>, <given-names>Nazneen Fatema.</given-names></string-name> <article-title hwp:id="article-title-2">BERTology meets biology: interpreting attention in protein language models</article-title>. <source hwp:id="source-1">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2006.15222</pub-id>, <year>2020</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1 xref-ref-2-2"><label>[2]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.2" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Rao Roshan"><surname>Rao</surname>, <given-names>Roshan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Meier Joshua"><surname>Meier</surname>, <given-names>Joshua</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sercu Tom"><surname>Sercu</surname>, <given-names>Tom</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ovchinnikov Sergey"><surname>Ovchinnikov</surname>, <given-names>Sergey</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rives Alexander."><surname>Rives</surname>, <given-names>Alexander.</given-names></string-name> <article-title hwp:id="article-title-3">Transformer protein language models are unsupervised structure learners</article-title>. <source hwp:id="source-2">Biorxiv</source>, <year>2020</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2 xref-ref-3-3 xref-ref-3-4 xref-ref-3-5 xref-ref-3-6 xref-ref-3-7 xref-ref-3-8"><label>[3]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.3" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Elnaggar Ahmed"><surname>Elnaggar</surname>, <given-names>Ahmed</given-names></string-name> and <string-name name-style="western" hwp:sortable="Heinzinger Michael"><surname>Heinzinger</surname>, <given-names>Michael</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dallago Christian"><surname>Dallago</surname>, <given-names>Christian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rihawi Ghalia"><surname>Rihawi</surname>, <given-names>Ghalia</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wang Yu"><surname>Wang</surname>, <given-names>Yu</given-names></string-name> and <string-name name-style="western" hwp:sortable="Jones Llion"><surname>Jones</surname>, <given-names>Llion</given-names></string-name> and <string-name name-style="western" hwp:sortable="Gibbs Tom"><surname>Gibbs</surname>, <given-names>Tom</given-names></string-name> and <string-name name-style="western" hwp:sortable="Feher Tamas"><surname>Feher</surname>, <given-names>Tamas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Angerer Christoph"><surname>Angerer</surname>, <given-names>Christoph</given-names></string-name> and <string-name name-style="western" hwp:sortable="Steinegger Martin"><surname>Steinegger</surname>, <given-names>Martin</given-names></string-name> and <collab hwp:id="collab-1">others</collab>. <article-title hwp:id="article-title-4">Prot-Trans: towards cracking the language of Life’s code through self-supervised deep learning and high performance computing</article-title>. <source hwp:id="source-3">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2007.06225</pub-id>, <year>2020</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1"><label>[4]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.4" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Rives Alexander"><surname>Rives</surname>, <given-names>Alexander</given-names></string-name> and <string-name name-style="western" hwp:sortable="Meier Joshua"><surname>Meier</surname>, <given-names>Joshua</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sercu Tom"><surname>Sercu</surname>, <given-names>Tom</given-names></string-name> and <string-name name-style="western" hwp:sortable="Goyal Siddharth"><surname>Goyal</surname>, <given-names>Siddharth</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lin Zeming"><surname>Lin</surname>, <given-names>Zeming</given-names></string-name> and <string-name name-style="western" hwp:sortable="Liu Jason"><surname>Liu</surname>, <given-names>Jason</given-names></string-name> and <string-name name-style="western" hwp:sortable="Guo Demi"><surname>Guo</surname>, <given-names>Demi</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ott Myle"><surname>Ott</surname>, <given-names>Myle</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zitnick C Lawrence"><surname>Zitnick</surname>, <given-names>C Lawrence</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ma Jerry"><surname>Ma</surname>, <given-names>Jerry</given-names></string-name> and <collab hwp:id="collab-2">others</collab>. <article-title hwp:id="article-title-5">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source hwp:id="source-4">Proceedings of the National Academy of Sciences</source>, vol. <volume>118</volume>, no. <issue>15</issue>, <year>2021</year>.</citation></ref><ref id="c5" hwp:id="ref-5"><label>[5]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Heinzinger Michael"><surname>Heinzinger</surname>, <given-names>Michael</given-names></string-name> and <string-name name-style="western" hwp:sortable="Elnaggar Ahmed"><surname>Elnaggar</surname>, <given-names>Ahmed</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wang Yu"><surname>Wang</surname>, <given-names>Yu</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dallago Christian"><surname>Dallago</surname>, <given-names>Christian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nechaev Dmitrii"><surname>Nechaev</surname>, <given-names>Dmitrii</given-names></string-name> and <string-name name-style="western" hwp:sortable="Matthes Florian"><surname>Matthes</surname>, <given-names>Florian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rost Burkhard."><surname>Rost</surname>, <given-names>Burkhard.</given-names></string-name> <article-title hwp:id="article-title-6">Modeling aspects of the language of life through transfer-learning protein sequences</article-title>. <source hwp:id="source-5">BMC bioinformatics</source>, vol. <volume>20</volume>, no. <issue>1</issue>, <year>2019</year>.</citation></ref><ref id="c6" hwp:id="ref-6"><label>[6]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.6" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Alley Ethan C"><surname>Alley</surname>, <given-names>Ethan C</given-names></string-name> and <string-name name-style="western" hwp:sortable="Khimulya Grigory"><surname>Khimulya</surname>, <given-names>Grigory</given-names></string-name> and <string-name name-style="western" hwp:sortable="Biswas Surojit"><surname>Biswas</surname>, <given-names>Surojit</given-names></string-name> and <string-name name-style="western" hwp:sortable="AlQuraishi Mohammed"><surname>AlQuraishi</surname>, <given-names>Mohammed</given-names></string-name> and <string-name name-style="western" hwp:sortable="Church George M."><surname>Church</surname>, <given-names>George M.</given-names></string-name> <article-title hwp:id="article-title-7">Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source hwp:id="source-6">Nature methods</source>, vol. <volume>16</volume>, no. <issue>12</issue>, <year>2019</year>.</citation></ref><ref id="c7" hwp:id="ref-7"><label>[7]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Madani Ali"><surname>Madani</surname>, <given-names>Ali</given-names></string-name> and <string-name name-style="western" hwp:sortable="McCann Bryan"><surname>McCann</surname>, <given-names>Bryan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Naik Nikhil"><surname>Naik</surname>, <given-names>Nikhil</given-names></string-name> and <string-name name-style="western" hwp:sortable="Keskar Nitish Shirish"><surname>Keskar</surname>, <given-names>Nitish Shirish</given-names></string-name> and <string-name name-style="western" hwp:sortable="Anand Namrata"><surname>Anand</surname>, <given-names>Namrata</given-names></string-name> and <string-name name-style="western" hwp:sortable="Eguchi Raphael R"><surname>Eguchi</surname>, <given-names>Raphael R</given-names></string-name> and <string-name name-style="western" hwp:sortable="Huang Po-Ssu"><surname>Huang</surname>, <given-names>Po-Ssu</given-names></string-name> and <string-name name-style="western" hwp:sortable="Socher Richard."><surname>Socher</surname>, <given-names>Richard.</given-names></string-name> <article-title hwp:id="article-title-8">Progen: Language modeling for protein generation</article-title>. <source hwp:id="source-7">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2004.03497</pub-id>, <year>2020</year>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><label>[8]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.8" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Ofer Dan"><surname>Ofer</surname>, <given-names>Dan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Brandes Nadav"><surname>Brandes</surname>, <given-names>Nadav</given-names></string-name> and <string-name name-style="western" hwp:sortable="Linial Michal."><surname>Linial</surname>, <given-names>Michal.</given-names></string-name> <article-title hwp:id="article-title-9">The language of proteins: NLP, machine learning &amp; protein sequences</article-title>. <source hwp:id="source-8">Computational and Structural Biotechnology Journal</source>, vol. <volume>19</volume>, <year>2021</year>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1 xref-ref-9-2 xref-ref-9-3"><label>[9]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Hesslow Daniel"><surname>Hesslow</surname>, <given-names>Daniel</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zanichelli Niccoló"><surname>Zanichelli</surname>, <given-names>Niccoló</given-names></string-name> and <string-name name-style="western" hwp:sortable="Notin Pascal"><surname>Notin</surname>, <given-names>Pascal</given-names></string-name> and <string-name name-style="western" hwp:sortable="Poli Iacopo"><surname>Poli</surname>, <given-names>Iacopo</given-names></string-name> and <string-name name-style="western" hwp:sortable="Marks Debora."><surname>Marks</surname>, <given-names>Debora.</given-names></string-name> <article-title hwp:id="article-title-10">RITA: a Study on Scaling Up Generative Protein Sequence Models</article-title>. <source hwp:id="source-9">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2205.05789</pub-id>, <year>2022</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1 xref-ref-10-2 xref-ref-10-3 xref-ref-10-4 xref-ref-10-5"><label>[10]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Lin Zeming"><surname>Lin</surname>, <given-names>Zeming</given-names></string-name> and <string-name name-style="western" hwp:sortable="Akin Halil"><surname>Akin</surname>, <given-names>Halil</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rao Roshan"><surname>Rao</surname>, <given-names>Roshan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Hie Brian"><surname>Hie</surname>, <given-names>Brian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhu Zhongkai"><surname>Zhu</surname>, <given-names>Zhongkai</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lu Wenting"><surname>Lu</surname>, <given-names>Wenting</given-names></string-name> and <string-name name-style="western" hwp:sortable="dos Santos Costa Allan"><surname>dos Santos Costa</surname>, <given-names>Allan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Fazel-Zarandi Maryam"><surname>Fazel-Zarandi</surname>, <given-names>Maryam</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sercu Tom"><surname>Sercu</surname>, <given-names>Tom</given-names></string-name> and <string-name name-style="western" hwp:sortable="Candido Sal"><surname>Candido</surname>, <given-names>Sal</given-names></string-name> and <collab hwp:id="collab-3">others</collab>. <article-title hwp:id="article-title-11">Language models of protein sequences at the scale of evolution enable accurate structure prediction</article-title>. <source hwp:id="source-10">bioRxiv</source>, <year>2022</year>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><label>[11]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.11" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Nijkamp Erik"><surname>Nijkamp</surname>, <given-names>Erik</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ruffolo Jeffrey"><surname>Ruffolo</surname>, <given-names>Jeffrey</given-names></string-name> and <string-name name-style="western" hwp:sortable="Weinstein Eli N"><surname>Weinstein</surname>, <given-names>Eli N</given-names></string-name> and <string-name name-style="western" hwp:sortable="Naik Nikhil"><surname>Naik</surname>, <given-names>Nikhil</given-names></string-name> and <string-name name-style="western" hwp:sortable="Madani Ali."><surname>Madani</surname>, <given-names>Ali.</given-names></string-name> <article-title hwp:id="article-title-12">ProGen2: exploring the boundaries of protein language models</article-title>. <source hwp:id="source-11">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2206.13517</pub-id>, <year>2022</year>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1"><label>[12]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Gardner Matt"><surname>Gardner</surname>, <given-names>Matt</given-names></string-name> and <string-name name-style="western" hwp:sortable="Grus Joel"><surname>Grus</surname>, <given-names>Joel</given-names></string-name> and <string-name name-style="western" hwp:sortable="Neumann Mark"><surname>Neumann</surname>, <given-names>Mark</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tafjord Oyvind"><surname>Tafjord</surname>, <given-names>Oyvind</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dasigi Pradeep"><surname>Dasigi</surname>, <given-names>Pradeep</given-names></string-name> and <string-name name-style="western" hwp:sortable="Liu Nelson"><surname>Liu</surname>, <given-names>Nelson</given-names></string-name> and <string-name name-style="western" hwp:sortable="Peters Matthew"><surname>Peters</surname>, <given-names>Matthew</given-names></string-name> and <string-name name-style="western" hwp:sortable="Schmitz Michael"><surname>Schmitz</surname>, <given-names>Michael</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zettlemoyer Luke."><surname>Zettlemoyer</surname>, <given-names>Luke.</given-names></string-name> <article-title hwp:id="article-title-13">Allennlp: A deep semantic natural language processing platform</article-title>. <source hwp:id="source-12">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1803.07640</pub-id>, <year>2018</year>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1 xref-ref-13-2"><label>[13]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.13" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Cock Peter JA"><surname>Cock</surname>, <given-names>Peter JA</given-names></string-name> and <string-name name-style="western" hwp:sortable="Antao Tiago"><surname>Antao</surname>, <given-names>Tiago</given-names></string-name> and <string-name name-style="western" hwp:sortable="Chang Jeffrey T"><surname>Chang</surname>, <given-names>Jeffrey T</given-names></string-name> and <string-name name-style="western" hwp:sortable="Chapman Brad A"><surname>Chapman</surname>, <given-names>Brad A</given-names></string-name> and <string-name name-style="western" hwp:sortable="Cox Cymon J"><surname>Cox</surname>, <given-names>Cymon J</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dalke Andrew"><surname>Dalke</surname>, <given-names>Andrew</given-names></string-name> and <string-name name-style="western" hwp:sortable="Friedberg Iddo"><surname>Friedberg</surname>, <given-names>Iddo</given-names></string-name> and <string-name name-style="western" hwp:sortable="Hamelryck Thomas"><surname>Hamelryck</surname>, <given-names>Thomas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kauff Frank"><surname>Kauff</surname>, <given-names>Frank</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wilczynski Bartek"><surname>Wilczynski</surname>, <given-names>Bartek</given-names></string-name> and <collab hwp:id="collab-4">others</collab>. <article-title hwp:id="article-title-14">Biopython: freely available Python tools for computational molecular biology and bioinformatics</article-title>. <source hwp:id="source-13">Bioinformatics</source>, vol. <volume>25</volume>, no. <issue>11</issue>, <year>2009</year>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><label>[14]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.14" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Mirdita Milot"><surname>Mirdita</surname>, <given-names>Milot</given-names></string-name> and <string-name name-style="western" hwp:sortable="Schütze Konstantin"><surname>Schütze</surname>, <given-names>Konstantin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Moriwaki Yoshitaka"><surname>Moriwaki</surname>, <given-names>Yoshitaka</given-names></string-name> and <string-name name-style="western" hwp:sortable="Heo Lim"><surname>Heo</surname>, <given-names>Lim</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ovchinnikov Sergey"><surname>Ovchinnikov</surname>, <given-names>Sergey</given-names></string-name> and <string-name name-style="western" hwp:sortable="Steinegger Martin."><surname>Steinegger</surname>, <given-names>Martin.</given-names></string-name> <article-title hwp:id="article-title-15">ColabFold: making protein folding accessible to all</article-title>. <source hwp:id="source-14">Nature Methods</source>, <year>2022</year>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2 xref-ref-15-3"><label>[15]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.15" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Hendrycks Dan"><surname>Hendrycks</surname>, <given-names>Dan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Gimpel Kevin."><surname>Gimpel</surname>, <given-names>Kevin.</given-names></string-name> <article-title hwp:id="article-title-16">Gaussian error linear units (gelus)</article-title>. <source hwp:id="source-15">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1606.08415</pub-id>, <year>2016</year>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><label>[16]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.16" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="Rost Burkhard."><surname>Rost</surname>, <given-names>Burkhard.</given-names></string-name> <article-title hwp:id="article-title-17">Enzyme function less conserved than anticipated</article-title>. <source hwp:id="source-16">Journal of molecular biology</source>, vol. <volume>318</volume>, no. <issue>2</issue>, <year>2002</year>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2 xref-ref-17-3 xref-ref-17-4"><label>[17]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.17" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Suzek Baris E"><surname>Suzek</surname>, <given-names>Baris E</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wang Yuqi"><surname>Wang</surname>, <given-names>Yuqi</given-names></string-name> and <string-name name-style="western" hwp:sortable="Huang Hongzhan"><surname>Huang</surname>, <given-names>Hongzhan</given-names></string-name> and <string-name name-style="western" hwp:sortable="McGarvey Peter B"><surname>McGarvey</surname>, <given-names>Peter B</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wu Cathy H"><surname>Wu</surname>, <given-names>Cathy H</given-names></string-name> and <string-name name-style="western" hwp:sortable="Consortium UniProt"><given-names>UniProt</given-names> <surname>Consortium</surname></string-name>. <article-title hwp:id="article-title-18">UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>. <source hwp:id="source-17">Bioinformatics</source>, vol. <volume>31</volume>, no. <issue>6</issue>, <year>2015</year>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><label>[18]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Steinegger Martin"><surname>Steinegger</surname>, <given-names>Martin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Mirdita Milot"><surname>Mirdita</surname>, <given-names>Milot</given-names></string-name> and <string-name name-style="western" hwp:sortable="Söding Johannes."><surname>Söding</surname>, <given-names>Johannes.</given-names></string-name> <article-title hwp:id="article-title-19">Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</article-title>. <source hwp:id="source-18">Nature methods</source>, vol. <volume>16</volume>, no. <issue>7</issue>, <year>2019</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1 xref-ref-19-2"><label>[19]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.19" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Vaswani Ashish"><surname>Vaswani</surname>, <given-names>Ashish</given-names></string-name> and <string-name name-style="western" hwp:sortable="Shazeer Noam"><surname>Shazeer</surname>, <given-names>Noam</given-names></string-name> and <string-name name-style="western" hwp:sortable="Parmar Niki"><surname>Parmar</surname>, <given-names>Niki</given-names></string-name> and <string-name name-style="western" hwp:sortable="Uszkoreit Jakob"><surname>Uszkoreit</surname>, <given-names>Jakob</given-names></string-name> and <string-name name-style="western" hwp:sortable="Jones Llion"><surname>Jones</surname>, <given-names>Llion</given-names></string-name> and <string-name name-style="western" hwp:sortable="Gomez Aidan N"><surname>Gomez</surname>, <given-names>Aidan N</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kaiser L ukasz"><surname>Kaiser</surname>, <given-names>L ukasz</given-names></string-name> and <string-name name-style="western" hwp:sortable="Polosukhin Illia."><surname>Polosukhin</surname>, <given-names>Illia.</given-names></string-name> <article-title hwp:id="article-title-20">Attention is all you need</article-title>. <source hwp:id="source-19">Advances in neural information processing systems</source>, vol. <volume>30</volume>, <year>2017</year>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1 xref-ref-20-2 xref-ref-20-3"><label>[20]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Raffel Colin"><surname>Raffel</surname>, <given-names>Colin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Shazeer Noam"><surname>Shazeer</surname>, <given-names>Noam</given-names></string-name> and <string-name name-style="western" hwp:sortable="Roberts Adam"><surname>Roberts</surname>, <given-names>Adam</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lee Katherine"><surname>Lee</surname>, <given-names>Katherine</given-names></string-name> and <string-name name-style="western" hwp:sortable="Narang Sharan"><surname>Narang</surname>, <given-names>Sharan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Matena Michael"><surname>Matena</surname>, <given-names>Michael</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhou Yanqi"><surname>Zhou</surname>, <given-names>Yanqi</given-names></string-name> and <string-name name-style="western" hwp:sortable="Li Wei"><surname>Li</surname>, <given-names>Wei</given-names></string-name> and <string-name name-style="western" hwp:sortable="Liu Peter J"><surname>Liu</surname>, <given-names>Peter J</given-names></string-name> and <collab hwp:id="collab-5">others</collab>. <article-title hwp:id="article-title-21">Exploring the limits of transfer learning with a unified text-to-text transformer</article-title>. <source hwp:id="source-20">J. Mach. Learn. Res</source>., vol. <volume>21</volume>, no. <issue>140</issue>, <year>2020</year>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1 xref-ref-21-2 xref-ref-21-3"><label>[21]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Devlin Jacob"><surname>Devlin</surname>, <given-names>Jacob</given-names></string-name> and <string-name name-style="western" hwp:sortable="Chang Ming-Wei"><surname>Chang</surname>, <given-names>Ming-Wei</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lee Kenton"><surname>Lee</surname>, <given-names>Kenton</given-names></string-name> and <string-name name-style="western" hwp:sortable="Toutanova Kristina."><surname>Toutanova</surname>, <given-names>Kristina.</given-names></string-name> <article-title hwp:id="article-title-22">Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source hwp:id="source-21">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1810.04805</pub-id>, <year>2018</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><label>[22]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Lan Zhenzhong"><surname>Lan</surname>, <given-names>Zhenzhong</given-names></string-name> and <string-name name-style="western" hwp:sortable="Chen Mingda"><surname>Chen</surname>, <given-names>Mingda</given-names></string-name> and <string-name name-style="western" hwp:sortable="Goodman Sebastian"><surname>Goodman</surname>, <given-names>Sebastian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Gimpel Kevin"><surname>Gimpel</surname>, <given-names>Kevin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sharma Piyush"><surname>Sharma</surname>, <given-names>Piyush</given-names></string-name> and <string-name name-style="western" hwp:sortable="Soricut Radu."><surname>Soricut</surname>, <given-names>Radu.</given-names></string-name> <article-title hwp:id="article-title-23">Albert: A lite bert for self-supervised learning of language representations</article-title>. <source hwp:id="source-22">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1909.11942</pub-id>, <year>2019</year>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><label>[23]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Clark Kevin"><surname>Clark</surname>, <given-names>Kevin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Luong Minh-Thang"><surname>Luong</surname>, <given-names>Minh-Thang</given-names></string-name> and <string-name name-style="western" hwp:sortable="Le Quoc V"><surname>Le</surname>, <given-names>Quoc V</given-names></string-name> and <string-name name-style="western" hwp:sortable="Manning Christopher D."><surname>Manning</surname>, <given-names>Christopher D.</given-names></string-name> <article-title hwp:id="article-title-24">Electra: Pre-training text encoders as discriminators rather than generators</article-title>. <source hwp:id="source-23">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2003.10555</pub-id>, <year>2020</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><label>[24]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Dai Zihang"><surname>Dai</surname>, <given-names>Zihang</given-names></string-name> and <string-name name-style="western" hwp:sortable="Yang Zhilin"><surname>Yang</surname>, <given-names>Zhilin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Yang Yiming"><surname>Yang</surname>, <given-names>Yiming</given-names></string-name> and <string-name name-style="western" hwp:sortable="Carbonell Jaime"><surname>Carbonell</surname>, <given-names>Jaime</given-names></string-name> and <string-name name-style="western" hwp:sortable="Le Quoc V"><surname>Le</surname>, <given-names>Quoc V</given-names></string-name> and <string-name name-style="western" hwp:sortable="Salakhutdinov Ruslan."><surname>Salakhutdinov</surname>, <given-names>Ruslan.</given-names></string-name> <article-title hwp:id="article-title-25">Transformerxl: Attentive language models beyond a fixed-length context</article-title>. <source hwp:id="source-24">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1901.02860</pub-id>, <year>2019</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><label>[25]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.25" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Yang Zhilin"><surname>Yang</surname>, <given-names>Zhilin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dai Zihang"><surname>Dai</surname>, <given-names>Zihang</given-names></string-name> and <string-name name-style="western" hwp:sortable="Yang Yiming"><surname>Yang</surname>, <given-names>Yiming</given-names></string-name> and <string-name name-style="western" hwp:sortable="Carbonell Jaime"><surname>Carbonell</surname>, <given-names>Jaime</given-names></string-name> and <string-name name-style="western" hwp:sortable="Salakhutdinov Russ R"><surname>Salakhutdinov</surname>, <given-names>Russ R</given-names></string-name> and <string-name name-style="western" hwp:sortable="Le Quoc V."><surname>Le</surname>, <given-names>Quoc V.</given-names></string-name> <article-title hwp:id="article-title-26">Xlnet: Generalized autoregressive pretraining for language understanding</article-title>. <source hwp:id="source-25">Advances in neural information processing systems</source>, vol. <volume>32</volume>, <year>2019</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2 xref-ref-26-3"><label>[26]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Rao Roshan"><surname>Rao</surname>, <given-names>Roshan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bhattacharya Nicholas"><surname>Bhattacharya</surname>, <given-names>Nicholas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Thomas Neil"><surname>Thomas</surname>, <given-names>Neil</given-names></string-name> and <string-name name-style="western" hwp:sortable="Duan Yan"><surname>Duan</surname>, <given-names>Yan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Chen Peter"><surname>Chen</surname>, <given-names>Peter</given-names></string-name> and <string-name name-style="western" hwp:sortable="Canny John"><surname>Canny</surname>, <given-names>John</given-names></string-name> and <string-name name-style="western" hwp:sortable="Abbeel Pieter"><surname>Abbeel</surname>, <given-names>Pieter</given-names></string-name> and <string-name name-style="western" hwp:sortable="Song Yun."><surname>Song</surname>, <given-names>Yun.</given-names></string-name> <article-title hwp:id="article-title-27">Evaluating protein transfer learning with TAPE</article-title>. <source hwp:id="source-26">Advances in neural information processing systems</source>, vol. <volume>32</volume>, <year>2019</year>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><label>[27]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Xu Minghao"><surname>Xu</surname>, <given-names>Minghao</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhang Zuobai"><surname>Zhang</surname>, <given-names>Zuobai</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lu Jiarui"><surname>Lu</surname>, <given-names>Jiarui</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhu Zhaocheng"><surname>Zhu</surname>, <given-names>Zhaocheng</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhang Yangtian"><surname>Zhang</surname>, <given-names>Yangtian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ma Chang"><surname>Ma</surname>, <given-names>Chang</given-names></string-name> and <string-name name-style="western" hwp:sortable="Liu Runcheng"><surname>Liu</surname>, <given-names>Runcheng</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tang Jian."><surname>Tang</surname>, <given-names>Jian.</given-names></string-name> <article-title hwp:id="article-title-28">PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding</article-title>. <source hwp:id="source-27">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2206.02096</pub-id>, <year>2022</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><label>[28]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.28" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Sarkisyan Karen S"><surname>Sarkisyan</surname>, <given-names>Karen S</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bolotin Dmitry A"><surname>Bolotin</surname>, <given-names>Dmitry A</given-names></string-name> and <string-name name-style="western" hwp:sortable="Meer Margarita V"><surname>Meer</surname>, <given-names>Margarita V</given-names></string-name> and <string-name name-style="western" hwp:sortable="Usmanova Dinara R"><surname>Usmanova</surname>, <given-names>Dinara R</given-names></string-name> and <string-name name-style="western" hwp:sortable="Mishin Alexander S"><surname>Mishin</surname>, <given-names>Alexander S</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sharonov George V"><surname>Sharonov</surname>, <given-names>George V</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ivankov Dmitry N"><surname>Ivankov</surname>, <given-names>Dmitry N</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bozhanova Nina G"><surname>Bozhanova</surname>, <given-names>Nina G</given-names></string-name> and <string-name name-style="western" hwp:sortable="Baranov Mikhail S"><surname>Baranov</surname>, <given-names>Mikhail S</given-names></string-name> and <string-name name-style="western" hwp:sortable="Soylemez Onuralp"><surname>Soylemez</surname>, <given-names>Onuralp</given-names></string-name> and <collab hwp:id="collab-6">others</collab>. <article-title hwp:id="article-title-29">Local fitness landscape of the green fluorescent protein</article-title>. <source hwp:id="source-28">Nature</source>, vol. <volume>533</volume>, no. <issue>7603</issue>, <year>2016</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1"><label>[29]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.29" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Sormanni Pietro"><surname>Sormanni</surname>, <given-names>Pietro</given-names></string-name> and <string-name name-style="western" hwp:sortable="Amery Leanne"><surname>Amery</surname>, <given-names>Leanne</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ekizoglou Sofia"><surname>Ekizoglou</surname>, <given-names>Sofia</given-names></string-name> and <string-name name-style="western" hwp:sortable="Vendruscolo Michele"><surname>Vendruscolo</surname>, <given-names>Michele</given-names></string-name> and <string-name name-style="western" hwp:sortable="Popovic Bojana."><surname>Popovic</surname>, <given-names>Bojana.</given-names></string-name> <article-title hwp:id="article-title-30">Rapid and accurate in silico solubility screening of a monoclonal antibody library</article-title>. <source hwp:id="source-29">Scientific reports</source>, vol. <volume>7</volume>, no. <issue>1</issue>, <year>2017</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1"><label>[30]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.30" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Khurana Sameer"><surname>Khurana</surname>, <given-names>Sameer</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rawi Reda"><surname>Rawi</surname>, <given-names>Reda</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kunji Khalid"><surname>Kunji</surname>, <given-names>Khalid</given-names></string-name> and <string-name name-style="western" hwp:sortable="Chuang Gwo-Yu"><surname>Chuang</surname>, <given-names>Gwo-Yu</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bensmail Halima"><surname>Bensmail</surname>, <given-names>Halima</given-names></string-name> and <string-name name-style="western" hwp:sortable="Mall Raghvendra."><surname>Mall</surname>, <given-names>Raghvendra.</given-names></string-name> <article-title hwp:id="article-title-31">DeepSol: a deep learning framework for sequence-based protein solubility prediction</article-title>. <source hwp:id="source-30">Bioinformatics</source>, vol. <volume>34</volume>, no. <issue>15</issue>, <year>2018</year>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1"><label>[31]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.31" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Wu Nicholas C"><surname>Wu</surname>, <given-names>Nicholas C</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dai Lei"><surname>Dai</surname>, <given-names>Lei</given-names></string-name> and <string-name name-style="western" hwp:sortable="Olson C Anders"><surname>Olson</surname>, <given-names>C Anders</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lloyd-Smith James O"><surname>Lloyd-Smith</surname>, <given-names>James O</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sun Ren."><surname>Sun</surname>, <given-names>Ren.</given-names></string-name> <article-title hwp:id="article-title-32">Adaptation in protein fitness landscapes is facilitated by indirect paths</article-title>. <source hwp:id="source-31">Elife</source>, vol. <volume>5</volume>, <year>2016</year>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1"><label>[32]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.32" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Dallago Christian"><surname>Dallago</surname>, <given-names>Christian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Mou Jody"><surname>Mou</surname>, <given-names>Jody</given-names></string-name> and <string-name name-style="western" hwp:sortable="Johnston Kadina E"><surname>Johnston</surname>, <given-names>Kadina E</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wittmann Bruce J"><surname>Wittmann</surname>, <given-names>Bruce J</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bhattacharya Nicholas"><surname>Bhattacharya</surname>, <given-names>Nicholas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Goldman Samuel"><surname>Goldman</surname>, <given-names>Samuel</given-names></string-name> and <string-name name-style="western" hwp:sortable="Madani Ali"><surname>Madani</surname>, <given-names>Ali</given-names></string-name> and <string-name name-style="western" hwp:sortable="Yang Kevin K."><surname>Yang</surname>, <given-names>Kevin K.</given-names></string-name> <article-title hwp:id="article-title-33">FLIP: Benchmark tasks in fitness landscape inference for proteins</article-title>. <source hwp:id="source-32">bioRxiv</source>, <year>2021</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1"><label>[33]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="McLachlan Andrew D."><surname>McLachlan</surname>, <given-names>Andrew D.</given-names></string-name> <article-title hwp:id="article-title-34">Rapid comparison of protein structures</article-title>. <source hwp:id="source-33">Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography</source>, vol. <volume>38</volume>, no. <issue>6</issue>, <year>1982</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><label>[34]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.34" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="AlQuraishi Mohammed."><surname>AlQuraishi</surname>, <given-names>Mohammed.</given-names></string-name> <article-title hwp:id="article-title-35">ProteinNet: a standardized data set for machine learning of protein structure</article-title>. <source hwp:id="source-34">BMC bioinformatics</source>, vol. <volume>20</volume>, no. <issue>1</issue>, <year>2019</year>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1"><label>[35]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.35" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Kryshtafovych Andriy"><surname>Kryshtafovych</surname>, <given-names>Andriy</given-names></string-name> and <string-name name-style="western" hwp:sortable="Schwede Torsten"><surname>Schwede</surname>, <given-names>Torsten</given-names></string-name> and <string-name name-style="western" hwp:sortable="Topf Maya"><surname>Topf</surname>, <given-names>Maya</given-names></string-name> and <string-name name-style="western" hwp:sortable="Fidelis Krzysztof"><surname>Fidelis</surname>, <given-names>Krzysztof</given-names></string-name> and <string-name name-style="western" hwp:sortable="Moult John."><surname>Moult</surname>, <given-names>John.</given-names></string-name> <article-title hwp:id="article-title-36">Critical assessment of methods of protein structure prediction (CASP)—Round XIII</article-title>. <source hwp:id="source-35">Proteins: Structure, Function, and Bioinformatics</source>, vol. <volume>87</volume>, no. <issue>12</issue>, <year>2019</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><label>[36]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.36" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Chen Daozheng"><surname>Chen</surname>, <given-names>Daozheng</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tian Xiaoyu"><surname>Tian</surname>, <given-names>Xiaoyu</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhou Bo"><surname>Zhou</surname>, <given-names>Bo</given-names></string-name> and <string-name name-style="western" hwp:sortable="Gao Jun."><surname>Gao</surname>, <given-names>Jun.</given-names></string-name> <article-title hwp:id="article-title-37">Profold: Protein fold classification with additional structural features and a novel ensemble classifier</article-title>. <source hwp:id="source-36">BioMed research international</source>, vol. <volume>2016</volume>, <year>2016</year>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1"><label>[37]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.37" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Hou Jie"><surname>Hou</surname>, <given-names>Jie</given-names></string-name> and <string-name name-style="western" hwp:sortable="Adhikari Badri"><surname>Adhikari</surname>, <given-names>Badri</given-names></string-name> and <string-name name-style="western" hwp:sortable="Cheng Jianlin."><surname>Cheng</surname>, <given-names>Jianlin.</given-names></string-name> <article-title hwp:id="article-title-38">DeepSF: deep convolutional neural network for mapping protein sequences to folds</article-title>. <source hwp:id="source-37">Bioinformatics</source>, vol. <volume>34</volume>, no. <issue>8</issue>, <year>2018</year>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><label>[38]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.38" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Klausen Michael Schantz"><surname>Klausen</surname>, <given-names>Michael Schantz</given-names></string-name> and <string-name name-style="western" hwp:sortable="Jespersen Martin Closter"><surname>Jespersen</surname>, <given-names>Martin Closter</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nielsen Henrik"><surname>Nielsen</surname>, <given-names>Henrik</given-names></string-name> and <string-name name-style="western" hwp:sortable="Jensen Kamilla Kjaergaard"><surname>Jensen</surname>, <given-names>Kamilla Kjaergaard</given-names></string-name> and <string-name name-style="western" hwp:sortable="Jurtz Vanessa Isabell"><surname>Jurtz</surname>, <given-names>Vanessa Isabell</given-names></string-name> and <string-name name-style="western" hwp:sortable="Soenderby Casper Kaae"><surname>Soenderby</surname>, <given-names>Casper Kaae</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sommer Morten Otto Alexander"><surname>Sommer</surname>, <given-names>Morten Otto Alexander</given-names></string-name> and <string-name name-style="western" hwp:sortable="Winther Ole"><surname>Winther</surname>, <given-names>Ole</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nielsen Morten"><surname>Nielsen</surname>, <given-names>Morten</given-names></string-name> and <string-name name-style="western" hwp:sortable="Petersen Bent"><surname>Petersen</surname>, <given-names>Bent</given-names></string-name> and <collab hwp:id="collab-7">others</collab>. <article-title hwp:id="article-title-39">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</article-title>. <source hwp:id="source-38">Proteins: Structure, Function, and Bioinformatics</source>, vol. <volume>87</volume>, no. <issue>6</issue>, <year>2019</year>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><label>[39]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.39" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Yang Yuedong"><surname>Yang</surname>, <given-names>Yuedong</given-names></string-name> and <string-name name-style="western" hwp:sortable="Gao Jianzhao"><surname>Gao</surname>, <given-names>Jianzhao</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wang Jihua"><surname>Wang</surname>, <given-names>Jihua</given-names></string-name> and <string-name name-style="western" hwp:sortable="Heffernan Rhys"><surname>Heffernan</surname>, <given-names>Rhys</given-names></string-name> and <string-name name-style="western" hwp:sortable="Hanson Jack"><surname>Hanson</surname>, <given-names>Jack</given-names></string-name> and <string-name name-style="western" hwp:sortable="Paliwal Kuldip"><surname>Paliwal</surname>, <given-names>Kuldip</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhou Yaoqi."><surname>Zhou</surname>, <given-names>Yaoqi.</given-names></string-name> <article-title hwp:id="article-title-40">Sixtyfive years of the long march in protein secondary structure prediction: the final stretch?</article-title>. <source hwp:id="source-39">Briefings in bioinformatics</source>, vol. <volume>19</volume>, no. <issue>3</issue>, <year>2018</year>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><label>[40]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.40" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Cuff James A"><surname>Cuff</surname>, <given-names>James A</given-names></string-name> and <string-name name-style="western" hwp:sortable="Barton Geoffrey J."><surname>Barton</surname>, <given-names>Geoffrey J.</given-names></string-name> <article-title hwp:id="article-title-41">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</article-title>. <source hwp:id="source-40">Proteins: Structure, Function, and Bioinformatics</source>, vol. <volume>34</volume>, no. <issue>4</issue>, <year>1999</year>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1"><label>[41]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Abriata Luciano A"><surname>Abriata</surname>, <given-names>Luciano A</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tamò Giorgio E"><surname>Tamò</surname>, <given-names>Giorgio E</given-names></string-name> and <string-name name-style="western" hwp:sortable="Monastyrskyy Bohdan"><surname>Monastyrskyy</surname>, <given-names>Bohdan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kryshtafovych Andriy"><surname>Kryshtafovych</surname>, <given-names>Andriy</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dal Peraro Matteo."><surname>Dal Peraro</surname>, <given-names>Matteo.</given-names></string-name> <article-title hwp:id="article-title-42">Assessment of hard target modeling in CASP12 reveals an emerging role of alignment-based contact prediction methods</article-title>. <source hwp:id="source-41">Proteins: Structure, Function, and Bioinformatics</source>, vol. <volume>86</volume>, <year>2018</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1"><label>[42]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Moult J"><surname>Moult</surname>, <given-names>J</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kryshtafovych A."><surname>Kryshtafovych</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-43">Special Issue: CASP14: Critical Assessment of methods of protein Structure Prediction, 14th round</article-title>. <source hwp:id="source-42">Proteins</source>, vol. <volume>89</volume>, <year>2021</year>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><label>[43]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.43" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Littmann Maria"><surname>Littmann</surname>, <given-names>Maria</given-names></string-name> and <string-name name-style="western" hwp:sortable="Heinzinger Michael"><surname>Heinzinger</surname>, <given-names>Michael</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dallago Christian"><surname>Dallago</surname>, <given-names>Christian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Olenyi Tobias"><surname>Olenyi</surname>, <given-names>Tobias</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rost Burkhard."><surname>Rost</surname>, <given-names>Burkhard.</given-names></string-name> <article-title hwp:id="article-title-44">Embeddings from deep learning transfer GO annotations beyond homology</article-title>. <source hwp:id="source-43">Scientific reports</source>, vol. <volume>11</volume>, no. <issue>1</issue>, <year>2021</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1 xref-ref-44-2"><label>[44]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Heinzinger Michael"><surname>Heinzinger</surname>, <given-names>Michael</given-names></string-name> and <string-name name-style="western" hwp:sortable="Littmann Maria"><surname>Littmann</surname>, <given-names>Maria</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sillitoe Ian"><surname>Sillitoe</surname>, <given-names>Ian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bordin Nicola"><surname>Bordin</surname>, <given-names>Nicola</given-names></string-name> and <string-name name-style="western" hwp:sortable="Orengo Christine"><surname>Orengo</surname>, <given-names>Christine</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rost Burkhard."><surname>Rost</surname>, <given-names>Burkhard.</given-names></string-name> <article-title hwp:id="article-title-45">Contrastive learning on protein embeddings enlightens midnight zone</article-title>. <source hwp:id="source-44">NAR genomics and bioinformatics</source>, vol. <volume>4</volume>, no. <issue>2</issue>, <year>2022</year>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><label>[45]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.45" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="Sillitoe Ian"><surname>Sillitoe</surname>, <given-names>Ian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bordin Nicola"><surname>Bordin</surname>, <given-names>Nicola</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dawson Natalie"><surname>Dawson</surname>, <given-names>Natalie</given-names></string-name> and <string-name name-style="western" hwp:sortable="Waman Vaishali P"><surname>Waman</surname>, <given-names>Vaishali P</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ashford Paul"><surname>Ashford</surname>, <given-names>Paul</given-names></string-name> and <string-name name-style="western" hwp:sortable="Scholes Harry M"><surname>Scholes</surname>, <given-names>Harry M</given-names></string-name> and <string-name name-style="western" hwp:sortable="Pang Camilla SM"><surname>Pang</surname>, <given-names>Camilla SM</given-names></string-name> and <string-name name-style="western" hwp:sortable="Woodridge Laurel"><surname>Woodridge</surname>, <given-names>Laurel</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rauer Clemens"><surname>Rauer</surname>, <given-names>Clemens</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sen Neeladri"><surname>Sen</surname>, <given-names>Neeladri</given-names></string-name> and <collab hwp:id="collab-8">others</collab>. <article-title hwp:id="article-title-46">CATH: increased structural coverage of functional space</article-title>. <source hwp:id="source-45">Nucleic acids research</source>, vol. <volume>49</volume>, no. <issue>D1</issue>, <year>2021</year>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1"><label>[46]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.46" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Silhavy THOMAS J"><surname>Silhavy</surname>, <given-names>THOMAS J</given-names></string-name> and <string-name name-style="western" hwp:sortable="Benson SPENCER A"><surname>Benson</surname>, <given-names>SPENCER A</given-names></string-name> and <string-name name-style="western" hwp:sortable="Emr SCOTT D."><surname>Emr</surname>, <given-names>SCOTT D.</given-names></string-name> <article-title hwp:id="article-title-47">Mechanisms of protein localization</article-title>. <source hwp:id="source-46">Microbiological Reviews</source>, vol. <volume>47</volume>, no. <issue>3</issue>, <year>1983</year>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1"><label>[47]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.47" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Armenteros Almagro"><given-names>Almagro</given-names> <surname>Armenteros</surname></string-name>, <string-name name-style="western" hwp:sortable="Juan Josá"><given-names>Josá</given-names> <surname>Juan</surname></string-name> and <string-name name-style="western" hwp:sortable="Sønderby Casper Kaae"><surname>Sønderby</surname>, <given-names>Casper Kaae</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sønderby Søren Kaae"><surname>Sønderby</surname>, <given-names>Søren Kaae</given-names></string-name> and <string-name name-style="western" hwp:sortable="Nielsen Henrik"><surname>Nielsen</surname>, <given-names>Henrik</given-names></string-name> and <string-name name-style="western" hwp:sortable="Winther Ole."><surname>Winther</surname>, <given-names>Ole.</given-names></string-name> <article-title hwp:id="article-title-48">DeepLoc: prediction of protein subcellular localization using deep learning</article-title>. <source hwp:id="source-47">Bioinformatics</source>, vol. <volume>33</volume>, no. <issue>21</issue>, <year>2017</year>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><label>[48]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.48" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Repecka Donatas"><surname>Repecka</surname>, <given-names>Donatas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Jauniskis Vykintas"><surname>Jauniskis</surname>, <given-names>Vykintas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Karpus Laurynas"><surname>Karpus</surname>, <given-names>Laurynas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rembeza Elzbieta"><surname>Rembeza</surname>, <given-names>Elzbieta</given-names></string-name> and <string-name name-style="western" hwp:sortable="Rokaitis Irmantas"><surname>Rokaitis</surname>, <given-names>Irmantas</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zrimec Jan"><surname>Zrimec</surname>, <given-names>Jan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Poviloniene Simona"><surname>Poviloniene</surname>, <given-names>Simona</given-names></string-name> and <string-name name-style="western" hwp:sortable="Laurynenas Audrius"><surname>Laurynenas</surname>, <given-names>Audrius</given-names></string-name> and <string-name name-style="western" hwp:sortable="Viknander Sandra"><surname>Viknander</surname>, <given-names>Sandra</given-names></string-name> and <string-name name-style="western" hwp:sortable="Abuajwa Wissam"><surname>Abuajwa</surname>, <given-names>Wissam</given-names></string-name> and <collab hwp:id="collab-9">others</collab>. <article-title hwp:id="article-title-49">Expanding functional protein sequence spaces using generative adversarial networks</article-title>. <source hwp:id="source-48">Nature Machine Intelligence</source>, vol. <volume>3</volume>, no. <issue>4</issue>, <year>2021</year>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><label>[49]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.49" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Raybould Matthew IJ"><surname>Raybould</surname>, <given-names>Matthew IJ</given-names></string-name> and <string-name name-style="western" hwp:sortable="Kovaltsuk Aleksandr"><surname>Kovaltsuk</surname>, <given-names>Aleksandr</given-names></string-name> and <string-name name-style="western" hwp:sortable="Marks Claire"><surname>Marks</surname>, <given-names>Claire</given-names></string-name> and <string-name name-style="western" hwp:sortable="Deane Charlotte M."><surname>Deane</surname>, <given-names>Charlotte M.</given-names></string-name> <article-title hwp:id="article-title-50">CoV-AbDab: the coronavirus antibody database</article-title>. <source hwp:id="source-49">Bioinformatics</source>, vol. <volume>37</volume>, no. <issue>5</issue>, <year>2021</year>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><label>[50]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.50" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Jiang Zi-Hang"><surname>Jiang</surname>, <given-names>Zi-Hang</given-names></string-name> and <string-name name-style="western" hwp:sortable="Yu Weihao"><surname>Yu</surname>, <given-names>Weihao</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhou Daquan"><surname>Zhou</surname>, <given-names>Daquan</given-names></string-name> and <string-name name-style="western" hwp:sortable="Chen Yunpeng"><surname>Chen</surname>, <given-names>Yunpeng</given-names></string-name> and <string-name name-style="western" hwp:sortable="Feng Jiashi"><surname>Feng</surname>, <given-names>Jiashi</given-names></string-name> and <string-name name-style="western" hwp:sortable="Yan Shuicheng."><surname>Yan</surname>, <given-names>Shuicheng.</given-names></string-name> <article-title hwp:id="article-title-51">Convbert: Improving bert with span-based dynamic convolution</article-title>. <source hwp:id="source-50">Advances in Neural Information Processing Systems</source>, vol. <volume>33</volume>, <year>2020</year>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1 xref-ref-51-2"><label>[51]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.51" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Goodfellow Ian"><surname>Goodfellow</surname>, <given-names>Ian</given-names></string-name> and <string-name name-style="western" hwp:sortable="Bengio Yoshua"><surname>Bengio</surname>, <given-names>Yoshua</given-names></string-name> and <string-name name-style="western" hwp:sortable="Courville Aaron."><surname>Courville</surname>, <given-names>Aaron.</given-names></string-name> <source hwp:id="source-51">Deep learning</source>, <year>2016</year>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1"><label>[52]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.52" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Frostig Roy"><surname>Frostig</surname>, <given-names>Roy</given-names></string-name> and <string-name name-style="western" hwp:sortable="Johnson Matthew James"><surname>Johnson</surname>, <given-names>Matthew James</given-names></string-name> and <string-name name-style="western" hwp:sortable="Leary Chris."><surname>Leary</surname>, <given-names>Chris.</given-names></string-name> <article-title hwp:id="article-title-52">Compiling machine learning programs via high-level tracing</article-title>. <source hwp:id="source-52">Systems for Machine Learning</source>, vol. <volume>4</volume>, no. <issue>9</issue>, <year>2018</year>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1"><label>[53]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.53" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Zeng Changchang"><surname>Zeng</surname>, <given-names>Changchang</given-names></string-name> and <string-name name-style="western" hwp:sortable="Li Shaobo."><surname>Li</surname>, <given-names>Shaobo.</given-names></string-name> <article-title hwp:id="article-title-53">Analyzing the effect of masking length distribution of MLM: an evaluation framework and case study on Chinese MRC datasets</article-title>. <source hwp:id="source-53">Wireless Communications and Mobile Computing</source>, vol. <volume>2021</volume>, <year>2021</year>.</citation></ref><ref id="c54" hwp:id="ref-54"><label>[54]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Xiao Dongling"><surname>Xiao</surname>, <given-names>Dongling</given-names></string-name> and <string-name name-style="western" hwp:sortable="Li Yu-Kun"><surname>Li</surname>, <given-names>Yu-Kun</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhang Han"><surname>Zhang</surname>, <given-names>Han</given-names></string-name> and <string-name name-style="western" hwp:sortable="Sun Yu"><surname>Sun</surname>, <given-names>Yu</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tian Hao"><surname>Tian</surname>, <given-names>Hao</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wu Hua"><surname>Wu</surname>, <given-names>Hua</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wang Haifeng."><surname>Wang</surname>, <given-names>Haifeng.</given-names></string-name> <article-title hwp:id="article-title-54">AERNIE-Gram: pre-training with explicitly n-gram masked language modeling for natural language understanding</article-title>. <source hwp:id="source-54">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2010.12148</pub-id>, <year>2020</year>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1"><label>[55]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Levine Yoav"><surname>Levine</surname>, <given-names>Yoav</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lenz Barak"><surname>Lenz</surname>, <given-names>Barak</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lieber Opher"><surname>Lieber</surname>, <given-names>Opher</given-names></string-name> and <string-name name-style="western" hwp:sortable="Abend Omri"><surname>Abend</surname>, <given-names>Omri</given-names></string-name> and <string-name name-style="western" hwp:sortable="Leyton-Brown Kevin"><surname>Leyton-Brown</surname>, <given-names>Kevin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Tennenholtz Moshe"><surname>Tennenholtz</surname>, <given-names>Moshe</given-names></string-name> and <string-name name-style="western" hwp:sortable="Shoham Yoav."><surname>Shoham</surname>, <given-names>Yoav.</given-names></string-name> <article-title hwp:id="article-title-55">Pmi-masking: Principled masking of correlated spans</article-title>. arXiv:<pub-id pub-id-type="arxiv">2010.01825</pub-id>, <year>2020</year>. <source hwp:id="source-55">arXiv preprint</source></citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1"><label>[56]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.56" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Szandala Tomasz."><surname>Szandala</surname>, <given-names>Tomasz.</given-names></string-name> <article-title hwp:id="article-title-56">Review and comparison of commonly used activation functions for deep neural networks</article-title>. <source hwp:id="source-56">Bio-inspired neurocomputing</source>, <year>2021</year>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1"><label>[57]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Agarap Abien Fred."><surname>Agarap</surname>, <given-names>Abien Fred.</given-names></string-name> <article-title hwp:id="article-title-57">Deep learning using rectified linear units (relu)</article-title>. <source hwp:id="source-57">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1803.08375</pub-id>, <year>2018</year>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><label>[58]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Ke Guolin"><surname>Ke</surname>, <given-names>Guolin</given-names></string-name> and <string-name name-style="western" hwp:sortable="He Di"><surname>He</surname>, <given-names>Di</given-names></string-name> and <string-name name-style="western" hwp:sortable="Liu"><surname>Liu</surname></string-name>, <collab hwp:id="collab-10">Tie-Yan</collab>. <article-title hwp:id="article-title-58">Rethinking positional encoding in language pre-training</article-title>. <source hwp:id="source-58">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2006.15595</pub-id>, <year>2020</year>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><label>[59]</label><citation publication-type="other" citation-type="journal" ref:id="2023.01.16.524265v1.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Su Jianlin"><surname>Su</surname>, <given-names>Jianlin</given-names></string-name> and <string-name name-style="western" hwp:sortable="Lu Yu"><surname>Lu</surname>, <given-names>Yu</given-names></string-name> and <string-name name-style="western" hwp:sortable="Pan Shengfeng"><surname>Pan</surname>, <given-names>Shengfeng</given-names></string-name> and <string-name name-style="western" hwp:sortable="Wen Bo"><surname>Wen</surname>, <given-names>Bo</given-names></string-name> and <string-name name-style="western" hwp:sortable="Liu Yunfeng."><surname>Liu</surname>, <given-names>Yunfeng.</given-names></string-name> <article-title hwp:id="article-title-59">Roformer: Enhanced transformer with rotary position embedding</article-title>. <source hwp:id="source-59">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">2104.09864</pub-id>, <year>2021</year>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1"><label>[60]</label><citation publication-type="journal" citation-type="journal" ref:id="2023.01.16.524265v1.60" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Denil Misha"><surname>Denil</surname>, <given-names>Misha</given-names></string-name> and <string-name name-style="western" hwp:sortable="Shakibi Babak"><surname>Shakibi</surname>, <given-names>Babak</given-names></string-name> and <string-name name-style="western" hwp:sortable="Dinh Laurent"><surname>Dinh</surname>, <given-names>Laurent</given-names></string-name> and <string-name name-style="western" hwp:sortable="Ranzato Marc’Aurelio"><surname>Ranzato</surname>, <given-names>Marc’Aurelio</given-names></string-name> and <string-name name-style="western" hwp:sortable="De Freitas Nando."><surname>De Freitas</surname>, <given-names>Nando.</given-names></string-name> <article-title hwp:id="article-title-60">Predicting parameters in deep learning</article-title>. <source hwp:id="source-60">Advances in neural information processing systems</source>, vol. <volume>26</volume>, <year>2013</year>.</citation></ref></ref-list><sec id="s6" hwp:id="sec-52"><label>7</label><title hwp:id="title-61">Supplementary Materials</title><sec id="s6a" hwp:id="sec-53"><label>7.1</label><title hwp:id="title-62">Pre-Training Data &amp; Model Experimentation</title><p hwp:id="p-102">In this subsection, we report the statistics of the pre-training data as well as the results of every class of experimentation. In <xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Table 2</xref>, we show the statistics of the pre-training data.</p><p hwp:id="p-103">In <xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-2" hwp:rel-id="T3">Table 3</xref> we report the results of the experimentation on the masking strategy. It can be deduced that the top performing version of the six tested versions was the version of <italic toggle="yes">Exp. 4</italic>, where we reconstruct all the consecutive unmasked tokens as a single merged token. Therefore, this was the version that carried on to the following sub-set of experiments, masking probability.</p><p hwp:id="p-104">In <xref rid="tbl4" ref-type="table" hwp:id="xref-table-wrap-4-2" hwp:rel-id="T4">Table 4</xref>, we report the results of the experimentation on the masking probability. Out of the four values, 10% was the worst masking probability and was accordingly disregarded. Interestingly, it was found that the default value of Exp. 4 (15% masking probability) was outperforming for the sub-cellular localization prediction, fold prediction, as well as some of the secondary structure prediction tasks for datasets such as CB513 and TS115. Nevertheless, it was found that the value of Exp. 9 (30% masking probability) was outperforming for the entirety of the contact prediction tasks in addition to the secondary structure prediction tasks for CASP12 dataset. Given the inconsistency amongst the secondary structure prediction dataset results, we referred to the results on CASP12 -being a domain standardthat promotes the higher masking probability. Furthermore and as we are opting for holistic predictions via a general-purpose language model, we promoted generalization across different types of tasks, especially when the difference amongst results is of such a small magnitude and when acknowledging that taskspecific customization is of impact. Finally, given that all the experimental variations were trained for only two epochs, we proceeded with the intermediate value of Exp. 8 (20% masking probability) for the post-experimentation long-term training to fulfill the semi-comprehensive inclusion of different polls of tasks anticipated from a general-purpose protein language model that can then be customized per downstream task.</p><p hwp:id="p-105">In <xref rid="tbl5" ref-type="table" hwp:id="xref-table-wrap-5-2" hwp:rel-id="T5">Table 5</xref>, we report the results of the experimentation on the number of encoder and decoder layers. It was found that the version with a 48-layer encoder and 24-layer decoder -<italic toggle="yes">Exp. 11</italic> - outperformed the version of <italic toggle="yes">Exp. 8</italic> in all of the secondary structure prediction tasks (8-states), the fold and sub-cellular localization prediction, and the overall mean and median whilst the other two versions demonstrated a fluctuating performance depending on the task. Eventually, we proceeded with <italic toggle="yes">Exp. 11</italic> (48-layer encoder and 24-layer decoder) to the subsequent set of experiments, unlocking a gain in extracting richer embeddings -as a result of a bigger encoderwith the same total cost of equal-sized encoder-decoder. Our choice was mainly motivated by promoting generalization embodied in the need to retain an adequate number of decoder layers due to their importance in a broad class of generation tasks as well as pooling the majority of task datasets. Moreover, our choice was also motivated by computational efficiency embodied in the smaller number of encoder layers resulting in faster feature extraction.</p><p hwp:id="p-106">In <xref rid="tbl6" ref-type="table" hwp:id="xref-table-wrap-6-2" hwp:rel-id="T6">Table 6</xref>, we report the results of the experimentation on the depth vs. width variations. We can see that the tested variation corresponded to fluctuating results and accordingly was not proceeded with. We refer to the version with an embedding dimension of 768 as the base model or, <italic toggle="yes">Ankh base</italic>.</p><p hwp:id="p-107">In <xref rid="tbl7" ref-type="table" hwp:id="xref-table-wrap-7-2" hwp:rel-id="T7">Table 7</xref>, we report the results of the experimentation on the activation function.</p><p hwp:id="p-108">It was found that none of the combos pursued in the depth-width variation set of experiments consistently surpassed the top performer version, <italic toggle="yes">Exp. 11</italic>. Hence, none of this set’s combos were proceeded with and we reverted back to <italic toggle="yes">Exp. 11</italic>.</p><p hwp:id="p-109">In <xref rid="tbl8" ref-type="table" hwp:id="xref-table-wrap-8-2" hwp:rel-id="T8">Table 8</xref>, we report the results of the experimentation on the relative positional encoding. It was found that the combo with the most consistent and general results was that of <italic toggle="yes">Exp. 20</italic> (embedding offset of 128 and a number of embeddings of 64) and accordingly proceeded with as we refer to fold prediction when classification tasks are inconsistent and refer to <italic toggle="yes">CASP12</italic> when secondary structure dataset results are inconsistent.</p><p hwp:id="p-110">In <xref rid="tbl9" ref-type="table" hwp:id="xref-table-wrap-9-2" hwp:rel-id="T9">Table 9</xref>, we report the results of the experimentation on the weight tying. It can be seen that this experiment did not consistently surpass the results of the so-far top-performing model. This is traced to the difference between the input and output types due to how we masked and damask the input and output tokens, respectively, denoting higher prediction abilities from a setting with fewer parameters.</p><p hwp:id="p-111">In <xref rid="tbl10" ref-type="table" hwp:id="xref-table-wrap-10-2" hwp:rel-id="T10">Table 10</xref>, we report the results of the experimentation on the dataset. Nonetheless, the experiment has encouraged the initial direction of proceeding with <italic toggle="yes">UniRef50</italic> as representative for efficient high-quality attributes. It is noteworthy to mention that all previous experiments with <italic toggle="yes">UniRef50</italic> were trained for 2 full epochs in contrast to the experiment with <italic toggle="yes">UniRef90</italic> which was trained for only one epoch that is arguably equivalent.</p></sec><sec id="s6b" hwp:id="sec-54"><label>7.2</label><title hwp:id="title-63">Model Pre-training</title><p hwp:id="p-112">In this subsection, we report the configurations and hyper-parameters associated with the pre-training of the two Ankh models in comparison to the baseline as well as the downstream model in <xref rid="tbl11" ref-type="table" hwp:id="xref-table-wrap-11-3" hwp:rel-id="T11">Table 11</xref>.</p><table-wrap id="tbl11" orientation="portrait" position="float" hwp:id="T11" hwp:rev-id="xref-table-wrap-11-1 xref-table-wrap-11-2 xref-table-wrap-11-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL11</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T11</object-id><object-id pub-id-type="publisher-id">tbl11</object-id><label>Table 11:</label><caption hwp:id="caption-16"><p hwp:id="p-113">Pre-training Configurations.</p></caption><graphic xlink:href="524265v1_tbl11" position="float" orientation="portrait" hwp:id="graphic-16"/></table-wrap></sec><sec id="s6c" hwp:id="sec-55"><label>7.3</label><title hwp:id="title-64">Data &amp; Model Testing</title><sec id="s6c1" hwp:id="sec-56"><label>7.3.1</label><title hwp:id="title-65">Downstream Dataset</title><p hwp:id="p-114">In this subsection, we report the statistics of the multiple datasets used in the downstream tasks. The statistics can be observed in <xref rid="tbl12" ref-type="table" hwp:id="xref-table-wrap-12-2" hwp:rel-id="T12">Table 12</xref>.</p><table-wrap id="tbl12" orientation="portrait" position="float" hwp:id="T12" hwp:rev-id="xref-table-wrap-12-1 xref-table-wrap-12-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL12</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T12</object-id><object-id pub-id-type="publisher-id">tbl12</object-id><label>Table 12:</label><caption hwp:id="caption-17"><p hwp:id="p-115">Description of Supervised Downstream Datasets.</p></caption><graphic xlink:href="524265v1_tbl12" position="float" orientation="portrait" hwp:id="graphic-17"/></table-wrap></sec><sec id="s6c2" hwp:id="sec-57"><label>7.3.2</label><title hwp:id="title-66">Full Results of Embedding-Based Predictions</title><p hwp:id="p-116">Besides the domain standard datasets presented in our main pages, this section shows in <xref ref-type="table" rid="tbl13" hwp:id="xref-table-wrap-13-2" hwp:rel-id="T13">Table 13</xref> the full results of the embedding-based predictions spanning a variety of additional datasets and also confirming the same results.</p><table-wrap id="tbl13" orientation="portrait" position="float" hwp:id="T13" hwp:rev-id="xref-table-wrap-13-1 xref-table-wrap-13-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL13</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T13</object-id><object-id pub-id-type="publisher-id">tbl13</object-id><label>Table 13:</label><caption hwp:id="caption-18"><p hwp:id="p-117">Results Summary.</p></caption><graphic xlink:href="524265v1_tbl13" position="float" orientation="portrait" hwp:id="graphic-18"/></table-wrap></sec><sec id="s6c3" hwp:id="sec-58"><label>7.3.3</label><title hwp:id="title-67">Attention-Based Predictions</title><p hwp:id="p-118">In this subsection, we report the results of the attention-based prediction done on the contact prediction task in <xref rid="tbl14" ref-type="table" hwp:id="xref-table-wrap-14-1" hwp:rel-id="T14">Table 14</xref>. As attention maps are reported as the better indicator of contact prediction for the ESM PLM family in [<xref ref-type="bibr" rid="c2" hwp:id="xref-ref-2-2" hwp:rel-id="ref-2">2</xref>] and [<xref ref-type="bibr" rid="c10" hwp:id="xref-ref-10-5" hwp:rel-id="ref-10">10</xref>], the two representations are tested. To elaborate, the attention maps are extracted and compared with the contextualized embeddings as input for the contact prediction task per every model to opt for fair comparison and demonstrate the SoA’s best indicator with what we deem as the best indicator.</p><table-wrap id="tbl14" orientation="portrait" position="float" hwp:id="T14" hwp:rev-id="xref-table-wrap-14-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL14</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T14</object-id><object-id pub-id-type="publisher-id">tbl14</object-id><label>Table 14:</label><caption hwp:id="caption-19"><p hwp:id="p-119">Attention-Based Predictions.</p></caption><graphic xlink:href="524265v1_tbl14" position="float" orientation="portrait" hwp:id="graphic-19"/></table-wrap></sec><sec id="s6c4" hwp:id="sec-59"><label>7.3.4</label><title hwp:id="title-68">Embedding-Based Annotation Transfer</title><p hwp:id="p-120">In this subsection, we report the full results of the embedding-based annotation transfer for the CATH domains as seen in <xref rid="tbl15" ref-type="table" hwp:id="xref-table-wrap-15-2" hwp:rel-id="T15">Table 15</xref>.</p><table-wrap id="tbl15" orientation="portrait" position="float" hwp:id="T15" hwp:rev-id="xref-table-wrap-15-1 xref-table-wrap-15-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL15</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T15</object-id><object-id pub-id-type="publisher-id">tbl15</object-id><label>Table 15:</label><caption hwp:id="caption-20"><p hwp:id="p-121">The accuracy scores of the embedding-based attention transfer</p></caption><graphic xlink:href="524265v1_tbl15" position="float" orientation="portrait" hwp:id="graphic-20"/></table-wrap></sec><sec id="s6c5" hwp:id="sec-60"><label>7.3.5</label><title hwp:id="title-69">CATH Domains</title><p hwp:id="p-122">In this subsection, we report the results of the CATH domain analysis on the natural and generated data. In <xref rid="tbl16" ref-type="table" hwp:id="xref-table-wrap-16-3" hwp:rel-id="T16">Table 16</xref>, we observe the CATH super-domain analysis.</p><p hwp:id="p-123">In <xref rid="tbl17" ref-type="table" hwp:id="xref-table-wrap-17-1" hwp:rel-id="T17">Table 17</xref>, we observe the CATH functional-domain analysis.</p><table-wrap id="tbl16" orientation="portrait" position="float" hwp:id="T16" hwp:rev-id="xref-table-wrap-16-1 xref-table-wrap-16-2 xref-table-wrap-16-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL16</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T16</object-id><object-id pub-id-type="publisher-id">tbl16</object-id><label>Table 16:</label><caption hwp:id="caption-21"><p hwp:id="p-124">A summary of the CATH Super-Family Domains in Generated vs. Natural (Fine-Tuning) Data.</p></caption><graphic xlink:href="524265v1_tbl16" position="float" orientation="portrait" hwp:id="graphic-21"/></table-wrap><table-wrap id="tbl17" orientation="portrait" position="float" hwp:id="T17" hwp:rev-id="xref-table-wrap-17-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2023.01.16.524265v1/TBL17</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T17</object-id><object-id pub-id-type="publisher-id">tbl17</object-id><label>Table 17:</label><caption hwp:id="caption-22"><p hwp:id="p-125">A summary of the CATH Functional-Family Domains in Generated vs. Natural (Fine-Tuning) Data.</p></caption><graphic xlink:href="524265v1_tbl17" position="float" orientation="portrait" hwp:id="graphic-22"/></table-wrap></sec></sec></sec></back></article>
