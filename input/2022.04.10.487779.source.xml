<article article-type="article" specific-use="production" xml:lang="en" xmlns:hw="org.highwire.hpp" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:ref="http://schema.highwire.org/Reference" xmlns:hwp="http://schema.highwire.org/Journal" xmlns:l="http://schema.highwire.org/Linking" xmlns:r="http://schema.highwire.org/Revision" xmlns:x="http://www.w3.org/1999/xhtml" xmlns:app="http://www.w3.org/2007/app" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:nlm="http://schema.highwire.org/NLM/Journal" xmlns:a="http://www.w3.org/2005/Atom" xmlns:c="http://schema.highwire.org/Compound" xmlns:hpp="http://schema.highwire.org/Publishing"><front><journal-meta><journal-id journal-id-type="hwp">biorxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title>bioRxiv</journal-title><abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1101/2022.04.10.487779</article-id><article-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1</article-id><article-id pub-id-type="other" hwp:sub-type="pisa-master">biorxiv;2022.04.10.487779</article-id><article-id pub-id-type="other" hwp:sub-type="slug">2022.04.10.487779</article-id><article-id pub-id-type="other" hwp:sub-type="atom-slug">2022.04.10.487779</article-id><article-id pub-id-type="other" hwp:sub-type="tag">2022.04.10.487779</article-id><article-version>1.1</article-version><article-categories><subj-group subj-group-type="author-type"><subject>Regular Article</subject></subj-group><subj-group subj-group-type="heading"><subject>New Results</subject></subj-group><subj-group subj-group-type="hwp-journal-coll" hwp:journal-coll-id="Systems Biology" hwp:journal="biorxiv"><subject>Systems Biology</subject></subj-group></article-categories><title-group><article-title hwp:id="article-title-1">Learning inverse folding from millions of predicted structures</article-title></title-group><author-notes hwp:id="author-notes-1"><corresp id="cor1" hwp:id="corresp-1" hwp:rev-id="xref-corresp-1-1 xref-corresp-1-2 xref-corresp-1-3"><label>†</label>Correspondence to: Chloe Hsu &lt;<email hwp:id="email-1">chloehsu@berkeley.edu</email>&gt;, Adam Lerer &lt;<email hwp:id="email-2">alerer@fb.com</email>&gt;, Alexander Rives &lt;<email hwp:id="email-3">arives@fb.com</email>&gt;.</corresp><fn id="n1" fn-type="equal" hwp:id="fn-1" hwp:rev-id="xref-fn-1-1 xref-fn-1-2"><label>*</label><p hwp:id="p-1">Equal contribution</p></fn></author-notes><contrib-group hwp:id="contrib-group-1"><contrib contrib-type="author" corresp="yes" hwp:id="contrib-1"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-7743-3168</contrib-id><name name-style="western" hwp:sortable="Hsu Chloe"><surname>Hsu</surname><given-names>Chloe</given-names></name><xref ref-type="aff" rid="a1" hwp:id="xref-aff-1-1" hwp:rel-id="aff-1">1</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-1" hwp:rel-id="corresp-1">†</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-7743-3168"/></contrib><contrib contrib-type="author" hwp:id="contrib-2"><name name-style="western" hwp:sortable="Verkuil Robert"><surname>Verkuil</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-1" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-3"><name name-style="western" hwp:sortable="Liu Jason"><surname>Liu</surname><given-names>Jason</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-2" hwp:rel-id="aff-2">2</xref></contrib><contrib contrib-type="author" hwp:id="contrib-4"><name name-style="western" hwp:sortable="Lin Zeming"><surname>Lin</surname><given-names>Zeming</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-3" hwp:rel-id="aff-2">2</xref><xref ref-type="aff" rid="a3" hwp:id="xref-aff-3-1" hwp:rel-id="aff-3">3</xref></contrib><contrib contrib-type="author" hwp:id="contrib-5"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3224-8142</contrib-id><name name-style="western" hwp:sortable="Hie Brian"><surname>Hie</surname><given-names>Brian</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-4" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-3224-8142"/></contrib><contrib contrib-type="author" hwp:id="contrib-6"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2947-6064</contrib-id><name name-style="western" hwp:sortable="Sercu Tom"><surname>Sercu</surname><given-names>Tom</given-names></name><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-5" hwp:rel-id="aff-2">2</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-2947-6064"/></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-7"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5216-6542</contrib-id><name name-style="western" hwp:sortable="Lerer Adam"><surname>Lerer</surname><given-names>Adam</given-names></name><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-1" hwp:rel-id="fn-1">*</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-6" hwp:rel-id="aff-2">2</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-2" hwp:rel-id="corresp-1">†</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0002-5216-6542"/></contrib><contrib contrib-type="author" corresp="yes" hwp:id="contrib-8"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2208-0796</contrib-id><name name-style="western" hwp:sortable="Rives Alexander"><surname>Rives</surname><given-names>Alexander</given-names></name><xref ref-type="fn" rid="n1" hwp:id="xref-fn-1-2" hwp:rel-id="fn-1">*</xref><xref ref-type="aff" rid="a2" hwp:id="xref-aff-2-7" hwp:rel-id="aff-2">2</xref><xref ref-type="corresp" rid="cor1" hwp:id="xref-corresp-1-3" hwp:rel-id="corresp-1">†</xref><object-id pub-id-type="other" hwp:sub-type="orcid" xlink:href="http://orcid.org/0000-0003-2208-0796"/></contrib><aff id="a1" hwp:id="aff-1" hwp:rev-id="xref-aff-1-1"><label>1</label><institution hwp:id="institution-1">University of California, Berkeley. Work performed during internship at Facebook AI Research</institution></aff><aff id="a2" hwp:id="aff-2" hwp:rev-id="xref-aff-2-1 xref-aff-2-2 xref-aff-2-3 xref-aff-2-4 xref-aff-2-5 xref-aff-2-6 xref-aff-2-7"><label>2</label><institution hwp:id="institution-2">Facebook AI Research</institution></aff><aff id="a3" hwp:id="aff-3" hwp:rev-id="xref-aff-3-1"><label>3</label><institution hwp:id="institution-3">New York University</institution>. Code and weights available at <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/facebookresearch/esm" ext-link-type="uri" xlink:href="https://github.com/facebookresearch/esm" hwp:id="ext-link-1">https://github.com/facebookresearch/esm</ext-link></aff></contrib-group><pub-date pub-type="epub-original" date-type="pub" publication-format="electronic" hwp:start="2022"><year>2022</year></pub-date><pub-date pub-type="hwp-created" hwp:start="2022-04-10T15:27:37-07:00">
    <day>10</day><month>4</month><year>2022</year>
  </pub-date><pub-date pub-type="hwp-received" hwp:start="2022-04-10T15:27:37-07:00">
    <day>10</day><month>4</month><year>2022</year>
  </pub-date><pub-date pub-type="epub" hwp:start="2022-04-10T15:42:16-07:00">
    <day>10</day><month>4</month><year>2022</year>
  </pub-date><pub-date pub-type="epub-version" hwp:start="2022-04-10T15:42:16-07:00">
    <day>10</day><month>4</month><year>2022</year>
  </pub-date><elocation-id>2022.04.10.487779</elocation-id><history hwp:id="history-1">
<date date-type="received" hwp:start="2022-04-10"><day>10</day><month>4</month><year>2022</year></date>
<date date-type="rev-recd" hwp:start="2022-04-10"><day>10</day><month>4</month><year>2022</year></date>
<date date-type="accepted" hwp:start="2022-04-10"><day>10</day><month>4</month><year>2022</year></date>
</history><permissions><copyright-statement hwp:id="copyright-statement-1">© 2022, Posted by Cold Spring Harbor Laboratory</copyright-statement><copyright-year>2022</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="license-1"><p hwp:id="p-2">This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at <ext-link l:rel="related" l:ref-type="uri" l:ref="http://creativecommons.org/licenses/by-nc-nd/4.0/" ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/" hwp:id="ext-link-2">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link></p></license></permissions><self-uri xlink:href="487779.pdf" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/abstract" xlink:role="abstract" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/external-links" xlink:role="external-links" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:href="file:/content/biorxiv/vol0/issue2022/pdf/2022.04.10.487779v1.pdf" hwp:variant="yes" content-type="pdf" xlink:role="full-text"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/full-text" xlink:role="full-text" content-type="xhtml+xml" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/source" xlink:role="source" content-type="xml" xlink:show="none" hwp:variant="yes"/><self-uri l:ref="forthcoming:yes" c:role="http://schema.highwire.org/variant/original" xlink:role="original" content-type="xml" xlink:show="none" hwp:variant="yes" xlink:href="487779.xml"/><self-uri content-type="abstract" xlink:href="file:/content/biorxiv/vol0/issue2022/abstracts/2022.04.10.487779v1/2022.04.10.487779v1.htslp"/><self-uri content-type="fulltext" xlink:href="file:/content/biorxiv/vol0/issue2022/fulltext/2022.04.10.487779v1/2022.04.10.487779v1.htslp"/><abstract hwp:id="abstract-1"><title hwp:id="title-1">Abstract</title><p hwp:id="p-3">We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51% native sequence recovery on structurally held-out backbones with 72% recovery for buried residues, an overall improvement of almost 10 percentage points over existing methods. The model generalizes to a variety of more complex tasks including design of protein complexes, partially masked structures, binding interfaces, and multiple states.</p></abstract><counts><page-count count="22"/></counts></article-meta><notes hwp:id="notes-1"><notes notes-type="competing-interest-statement" hwp:id="notes-2"><title hwp:id="title-2">Competing Interest Statement</title><p hwp:id="p-4">The authors have declared no competing interest.</p></notes><fn-group content-type="external-links" hwp:id="fn-group-1"><fn fn-type="dataset" hwp:id="fn-2"><p hwp:id="p-5">
<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/facebookresearch/esm" ext-link-type="uri" xlink:href="https://github.com/facebookresearch/esm" hwp:id="ext-link-3">https://github.com/facebookresearch/esm</ext-link>
</p></fn></fn-group></notes></front><body><sec id="s1" hwp:id="sec-1"><label>1.</label><title hwp:id="title-3">Introduction</title><p hwp:id="p-6">Designing novel amino acid sequences that encode proteins with desired properties, known as <italic toggle="yes">de novo protein design</italic>, is a central challenge in bioengineering (<xref rid="c25" ref-type="bibr" hwp:id="xref-ref-25-1" hwp:rel-id="ref-25">Huang et al., 2016</xref>). The most well-established approaches to this problem use an energy function which directly models the physical basis of a protein’s folded state (<xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-1" hwp:rel-id="ref-17">Alford et al., 2017</xref>).</p><p hwp:id="p-7">Recently a new class of deep learning based approaches has been proposed, using generative models to predict sequences for structures (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-1" hwp:rel-id="ref-26">Ingraham et al., 2019</xref>; <xref rid="c63" ref-type="bibr" hwp:id="xref-ref-63-1" hwp:rel-id="ref-63">Strokach et al., 2020</xref>; <xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-1" hwp:rel-id="ref-4">Anand-Achim et al., 2021</xref>; <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-1" hwp:rel-id="ref-30">Jing et al., 2021b</xref>), generate backbone structures (<xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-1" hwp:rel-id="ref-3">Anand &amp; Huang, 2018</xref>; <xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-2" hwp:rel-id="ref-4">Eguchi et al., 2020</xref>), jointly generate structures and sequences (<xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-1" hwp:rel-id="ref-6">Anishchenko et al., 2021</xref>; <xref rid="c71" ref-type="bibr" hwp:id="xref-ref-71-1" hwp:rel-id="ref-71">Wang et al., 2021</xref>), or model sequences directly (<xref rid="c53" ref-type="bibr" hwp:id="xref-ref-53-1" hwp:rel-id="ref-53">Rives et al., 2021</xref>; <xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-1" hwp:rel-id="ref-41">Madani et al., 2021</xref>; <xref rid="c57" ref-type="bibr" hwp:id="xref-ref-57-1" hwp:rel-id="ref-57">Shin et al., 2021</xref>; <xref rid="c20" ref-type="bibr" hwp:id="xref-ref-20-1" hwp:rel-id="ref-20">Gligorijevic et al., 2021</xref>; <xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-1" hwp:rel-id="ref-12">Bryant et al., 2021</xref>; <xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-1" hwp:rel-id="ref-15">Dallago et al., 2021</xref>). The potential to learn the rules of protein design directly from data makes deep generative models a promising alternative to current physics-based energy functions.</p><p hwp:id="p-8">However, the relatively small number of experimentally determined protein structures places a limit on deep learning approaches. Experimentally determined structures cover less than 0.1% of the known space of protein sequences. While the UniRef sequence database (<xref rid="c64" ref-type="bibr" hwp:id="xref-ref-64-1" hwp:rel-id="ref-64">Suzek et al., 2015</xref>) has over 50 million clusters at 50% sequence identity; as of January 2022, the Protein Data Bank (PDB) (<xref rid="c9" ref-type="bibr" hwp:id="xref-ref-9-1" hwp:rel-id="ref-9">Berman et al., 2000</xref>) contains structures for fewer than 53,000 unique sequences clustered at the same level of identity.</p><p hwp:id="p-9">Here we explore whether predicted structures can be used to overcome the limitation of experimental data. With progress in protein structure prediction (<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-1" hwp:rel-id="ref-32">Jumper et al., 2021</xref>; <xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-1" hwp:rel-id="ref-7">Baek et al., 2021</xref>), it is now possible to consider learning from predicted structures at scale. Predicting structures for the sequences in large databases can expand the structural coverage of protein sequences by orders of magnitude. To train an inverse model for protein design, we predict structures for 12 million sequences in UniRef50 using AlphaFold2.</p><p hwp:id="p-10">We focus on the problem of predicting sequences from back-bone structures, known as <italic toggle="yes">inverse folding</italic> or fixed back-bone design. We approach inverse folding as a sequence-to-sequence problem (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-2" hwp:rel-id="ref-26">Ingraham et al., 2019</xref>), using an au-toregressive encoder-decoder architecture, where the model is tasked with recovering the native sequence of a protein from the coordinates of its backbone atoms.</p><p hwp:id="p-11">We make use of the large number of sequences with un-known structures by adding them as additional training data, conditioning the model on predicted structures when the experimental structures are unknown (<xref rid="fig1" ref-type="fig" hwp:id="xref-fig-1-1" hwp:rel-id="F1">Figure 1</xref>). This approach parallels back-translation (<xref rid="c56" ref-type="bibr" hwp:id="xref-ref-56-1" hwp:rel-id="ref-56">Sennrich et al., 2015</xref>; <xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-2" hwp:rel-id="ref-17">Edunov et al., 2018</xref>) in machine translation, where predicted translations in one direction are used to improve a model in the opposite direction. Back-translation has been found to effectively learn from extra target data (i.e. sequences) even when the predicted inputs (i.e. structures) are of low quality.</p><fig id="fig1" position="float" fig-type="figure" orientation="portrait" hwp:id="F1" hwp:rev-id="xref-fig-1-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIG1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F1</object-id><object-id pub-id-type="publisher-id">fig1</object-id><label>Figure 1.</label><caption hwp:id="caption-1"><p hwp:id="p-12">Augmenting inverse folding with predicted structures. To evaluate the potential for training protein design models with predicted structures, we predict structures for 12 million UniRef50 protein sequences using AlphaFold2 (<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-2" hwp:rel-id="ref-32">Jumper et al., 2021</xref>). An autoregressive inverse folding model is trained to perform fixed-backbone protein sequence design. Train and test sets are partitioned at the topology level, so that the model is evaluated on structurally held-out backbones. We compare transformer models having invariant geometric input processing layers, with fully geometric models used in prior work. Span masking and noise is applied to the input coordinates.</p></caption><graphic xlink:href="487779v1_fig1" position="float" orientation="portrait" hwp:id="graphic-1"/></fig><fig id="fig2" position="float" fig-type="figure" orientation="portrait" hwp:id="F2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIG2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F2</object-id><object-id pub-id-type="publisher-id">fig2</object-id><label>Figure 2.</label><caption hwp:id="caption-2"><p hwp:id="p-13">Illustration of the protein design tasks considered.</p></caption><graphic xlink:href="487779v1_fig2" position="float" orientation="portrait" hwp:id="graphic-2"/></fig><p hwp:id="p-14">We find that existing approaches have been limited by data. While current state-of-the-art inverse folding models degrade when training is augmented with predicted structures, much larger models and different model architectures can effectively learn from the additional data, leading to an improvement of nearly 10 percentage points in the recovery of sequences for structurally held out native backbones.</p><p hwp:id="p-15">We evaluate models on fixed backbone design benchmarks from prior work, and assess the generalization capabilities across a series of tasks including design of complexes and binding sites, partially masked backbones, and multiple conformations. We further consider the use of the models for zero-shot prediction of mutational effects on protein function and stability, complex stability, and binding affinity.</p></sec><sec id="s2" hwp:id="sec-2"><label>2.</label><title hwp:id="title-4">Learning inverse folding from predicted structures</title><p hwp:id="p-16">The goal of inverse folding is to design sequences that fold to a desired structure. In this work, we focus on the backbone structure without considering side chains. While each of the 20 amino acid has a specific side chain, they share a common set of atoms that make up the amino acid backbone. Among the backbone atoms, we choose the N, C<italic toggle="yes">α</italic> (alpha Carbon), and C atom coordinates to represent the backbone.</p><p hwp:id="p-17">Using the structures of naturally existing proteins we can train a model for this task by supervising it to predict the protein’s native sequence from the coordinates of its backbone atoms in three-dimensional space. Formally we represent this problem as one of learning the conditional distribution <italic toggle="yes">p</italic>(<italic toggle="yes">Y</italic>|<italic toggle="yes">X</italic>), where for a protein of length <italic toggle="yes">n</italic>, given a sequence <italic toggle="yes">X</italic> of spatial coordinates (<italic toggle="yes">x</italic><sub>1</sub>, <italic toggle="yes">…, x<sub>i</sub>, …, x</italic><sub>3<italic toggle="yes">n</italic></sub>) for each of the backbone atoms <italic toggle="yes">N, Cα, C</italic> in the structure, the objective is to predict <italic toggle="yes">Y</italic> the native sequence (<italic toggle="yes">y</italic><sub>1</sub>, <italic toggle="yes">…, y<sub>i</sub>, …, y<sub>n</sub></italic>) of amino acids. This density is modeled autoregressively through a sequence-to-sequence encoder-decoder:
<disp-formula id="eqn1" hwp:id="disp-formula-1" hwp:rev-id="xref-disp-formula-1-1">
<alternatives hwp:id="alternatives-1"><graphic xlink:href="487779v1_eqn1.gif" position="float" orientation="portrait" hwp:id="graphic-3"/></alternatives>
</disp-formula>
We train a model by minimizing the negative log likelihood of the data. We can design sequences by sampling, or by finding sequences that maximize the conditional probability given the desired structure.</p><sec id="s2a" hwp:id="sec-3"><label>2.1</label><title hwp:id="title-5">Data</title><sec id="s2a1" hwp:id="sec-4"><title hwp:id="title-6">Predicted structures</title><p hwp:id="p-18">We generate 12 million structures for sequences in UniRef50 to explore how predicted structures can improve inverse folding models. To select sequences for structure prediction we first use MSA Transformer (<xref rid="c52" ref-type="bibr" hwp:id="xref-ref-52-1" hwp:rel-id="ref-52">Rao et al., 2021</xref>) to predict distograms for MSAs of all UniRef50 sequences. We rank the sequences by distogram LDDT scores (<xref rid="c55" ref-type="bibr" hwp:id="xref-ref-55-1" hwp:rel-id="ref-55">Senior et al., 2020</xref>) as a proxy for the quality of the predictions. We take the top 12 million sequences not longer than five hundred amino acids and forward fold them using the AlphaFold2 model with a final Amber (<xref rid="c23" ref-type="bibr" hwp:id="xref-ref-23-1" hwp:rel-id="ref-23">Hornak et al., 2006</xref>) relaxation. This results in a predicted dataset approximately 750 times the size of the training set of experimental structures (<xref ref-type="sec" rid="s6a" hwp:id="xref-sec-30-1" hwp:rel-id="sec-30">Appendix A.1</xref>).</p></sec><sec id="s2a2" hwp:id="sec-5"><title hwp:id="title-7">Training and evaluation data</title><p hwp:id="p-19">We evaluate models on a structurally held-out subset of CATH (<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-1" hwp:rel-id="ref-46">Orengo et al., 1997</xref>). We partition CATH at the topology level with an 80/10/10 split resulting in 16153 structures assigned to the training set, 1457 to the validation set, and 1797 to the test set. Particular care is required to prevent leakage of information in the test set via the predicted structures. We use Gene3D topology classification (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-1" hwp:rel-id="ref-37">Lees et al., 2012</xref>) to filter both the sequences used for supervision in training, as well as the MSAs used as inputs for AlphaFold2 predictions (<xref ref-type="sec" rid="s6a" hwp:id="xref-sec-30-2" hwp:rel-id="sec-30">Appendix A.1</xref>). We also perform evaluations on a smaller subset of the CATH test set that has been additionally filtered by TM-score using Foldseek (<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-1" hwp:rel-id="ref-33">Kim et al., 2021</xref>) to exclude any structures with similarity to those in the training set (<xref ref-type="sec" rid="s7" hwp:id="xref-sec-39-1" hwp:rel-id="sec-39">Appendix B</xref>).</p></sec></sec><sec id="s2b" hwp:id="sec-6"><label>2.2</label><title hwp:id="title-8">Model architectures</title><p hwp:id="p-20">We study model architectures using Geometric Vector Perceptron (GVP) layers (<xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-2" hwp:rel-id="ref-30">Jing et al., 2021b</xref>) that learn rotationequivariant transformations of vector features and rotationinvariant transformations of scalar features.</p><p hwp:id="p-21">We present results for three model architectures: (1) GVP-GNN from <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-3" hwp:rel-id="ref-30">Jing et al. (2021b)</xref> which is currently state-of-the-art on inverse folding; (2) a GVP-GNN with increased width and depth (GVP-GNN-large); and (3) a hybrid model consisting of a GVP-GNN structural encoder followed by a generic transformer (GVP-Transformer). All models used in evaluations are trained to convergence, with detailed hy-perparameters listed in <xref rid="tblA1" ref-type="table" hwp:id="xref-table-wrap-4-1" hwp:rel-id="T4">Table A.1</xref>.</p><p hwp:id="p-22">In inverse folding, the predicted sequence should be independent of the reference frame of the structural coordinates. For any rotation and translation <italic toggle="yes">T</italic> of the input coordinates, we would like for the model’s output to be invariant under these transformations, i.e., <italic toggle="yes">p</italic>(<italic toggle="yes">Y</italic>|<italic toggle="yes">X</italic>) = <italic toggle="yes">p</italic>(<italic toggle="yes">Y</italic>|<italic toggle="yes">TX</italic>). Both the GVP-GNN and GVP-Transformer inverse folding models studied in this work are invariant (<xref ref-type="sec" rid="s6c" hwp:id="xref-sec-36-1" hwp:rel-id="sec-36">Appendix A.3</xref>).</p><sec id="s2b1" hwp:id="sec-7"><title hwp:id="title-9">GVP-GNN</title><p hwp:id="p-23">We start with the GVP-GNN architecture with 3 encoder layers and 3 decoder layers as described in (<xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-4" hwp:rel-id="ref-30">Jing et al., 2021b</xref>), with the vector gates described in (<xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-1" hwp:rel-id="ref-29">Jing et al., 2021a</xref>) (GVP-GNN, 1M parameters). When trained on predicted structures, we find a deeper and wider version of GVP-GNN with 8 encoder layers and 8 decoder layers (GVP-GNN-large, 21M parameters) performs better. Scaling GVP-GNN further did not improve model performance in preliminary experiments (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-1" hwp:rel-id="F6">Figure 6c</xref>).</p></sec><sec id="s2b2" hwp:id="sec-8"><title hwp:id="title-10">GVP-Transformer</title><p hwp:id="p-24">We use GVP-GNN encoder layers to extract geometric features, followed by a generic autoregressive encoder-decoder Transformer (<xref rid="c68" ref-type="bibr" hwp:id="xref-ref-68-1" hwp:rel-id="ref-68">Vaswani et al., 2017</xref>). In GVP-GNN, the input features are translation-invariant and each layer is rotation-equivariant. We perform a change ofbasis on the vector features from GVP-GNN into local reference frames defined for each amino acid to derive rotationinvariant features (<xref ref-type="sec" rid="s6c" hwp:id="xref-sec-36-2" hwp:rel-id="sec-36">Appendix A.3</xref>). In ablation studies increasing the number of GVP-GNN encoder layers improves the overall model performance (<xref rid="figC1" ref-type="fig" hwp:id="xref-fig-10-1" hwp:rel-id="F10">Figure C.1</xref>), indicating that the geometric reasoning capability in GVP-GNN is complementary to the Transformer layers. Scaling improves performance up to a 142M-parameter GVP-Transformer model with 4 GVP-GNN encoder layers, 8 generic Transformer encoder layers, and 8 generic Transformer decoder layers (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-2" hwp:rel-id="F6">Figure 6c</xref>).</p></sec><sec id="s2b3" hwp:id="sec-9"><title hwp:id="title-11">Training</title><sec id="s2b3a" hwp:id="sec-10"><title hwp:id="title-12">Combining experimental and predicted data</title><p hwp:id="p-25">During training, in each epoch we mix the training set of experimentally derived structures (<italic toggle="yes">∼</italic>16K structures) with a 10% random sample of the AlphaFold2-predicted training set (10% of 12M), resulting in a 1:80 experimental:predicted data ratio. For larger models, a high ratio of predicted data during training helps prevent overfitting on the smaller experimental train set (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-3" hwp:rel-id="F6">Figure 6b</xref>).</p><p hwp:id="p-26">The loss is equally weighted for each amino acid in target sequences. We mask out predicted input coordinates with AlphaFold2 confidence score (pLDDT) below 90, around 25% of the predicted coordinates. See <xref rid="fig3" ref-type="fig" hwp:id="xref-fig-3-1" hwp:rel-id="F3">Figure 3</xref> for visualization of the pLDDT confidence score. Most often these low confidence regions are at the start and the end of sequences and may correspond to disordered regions. We prepend one token at the beginning of each sequence to indicate whether the structure is experimental or predicted. For each residue we provide the pLDDT confidence score from AlphaFold2 as a feature encoded by Gaussian radial basis functions.</p><fig id="fig3" position="float" fig-type="figure" orientation="portrait" hwp:id="F3" hwp:rev-id="xref-fig-3-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIG3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F3</object-id><object-id pub-id-type="publisher-id">fig3</object-id><label>Figure 3.</label><caption hwp:id="caption-3"><p hwp:id="p-27">Example AlphaFold prediction compared with experimental structure for a UniRef50 sequence (UniRef50: P07260; PDB: 1AP8). The experimental structure is shown as pink with transparency. The prediction is coloured by the pLDDT confidence score, with blue in high-confidence regions.</p></caption><graphic xlink:href="487779v1_fig3" position="float" orientation="portrait" hwp:id="graphic-4"/></fig><p hwp:id="p-28">Adding Gaussian noise at the scale of 0.1 angstroms to the predicted structures during training slightly improves performance (<xref rid="tblC1" ref-type="table" hwp:id="xref-table-wrap-6-1" hwp:rel-id="T6">Table C.1</xref>). This finding is consistent with <xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-3" hwp:rel-id="ref-17">Edunov et al. (2018)</xref>, who observe that backtranslation with sampled or noisy synthetic data provides a stronger training signal than maximum a posteriori (MAP) predictions.</p></sec><sec id="s2b3b" hwp:id="sec-11"><title hwp:id="title-13">Span masking</title><p hwp:id="p-29">To enable sequence design for partially masked backbones, we introduce backbone masking during training. We experiment with both independent random masking and span masking. In natural language processing, span masking improves performance over random masking (<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-1" hwp:rel-id="ref-31">Joshi et al., 2020</xref>). We randomly select continuous spans of up to 30 amino acids until 15% of input backbone coordinates are masked. The communication patterns in the geometric layers are adapted to account for masking with details in <xref ref-type="sec" rid="s6b" hwp:id="xref-sec-35-1" hwp:rel-id="sec-35">Appendix A.2</xref>. Span masking improves the performance of GVP-Transformer both on unmasked backbones (<xref rid="tblC1" ref-type="table" hwp:id="xref-table-wrap-6-2" hwp:rel-id="T6">Table C.1</xref>) and on masked regions (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-1" hwp:rel-id="F4">Figure 4</xref>).</p><fig id="fig4" position="float" fig-type="figure" orientation="portrait" hwp:id="F4" hwp:rev-id="xref-fig-4-1 xref-fig-4-2 xref-fig-4-3 xref-fig-4-4"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIG4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F4</object-id><object-id pub-id-type="publisher-id">fig4</object-id><label>Figure 4.</label><caption hwp:id="caption-4"><p hwp:id="p-30">Perplexity on regions of masked coordinates of different lengths. The GVP-GNN architecture degrades to the perplexity of the background distribution for masked regions of more than a few tokens, while GVP-Transformer maintains moderate accuracy on long masked spans, especially when trained on masked spans.</p></caption><graphic xlink:href="487779v1_fig4" position="float" orientation="portrait" hwp:id="graphic-5"/></fig></sec></sec></sec></sec><sec id="s3" hwp:id="sec-12"><label>3.</label><title hwp:id="title-14">Results</title><p hwp:id="p-31">We evaluate models across a variety of benchmarks in two overall settings: fixed backbone sequence design and zeroshot prediction of mutation effects. For fixed backbone design, we start with evaluation in the standard setting (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-3" hwp:rel-id="ref-26">Ingraham et al., 2019</xref>; <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-5" hwp:rel-id="ref-30">Jing et al., 2021b</xref>) of sequence design given all backbone coordinates. Then, we make the sequence design task more challenging along three dimensions: (1) introducing masking on coordinates; (2) generalization to protein complexes; and (3) conditioning on multiple conformations. Additionally, we show that inverse folding models are effective zero-shot predictors for protein complex stability, binding affinity, and insertion effects.</p><sec id="s3a" hwp:id="sec-13"><label>3.1</label><title hwp:id="title-15">Fixed backbone protein design</title><p hwp:id="p-32">We begin with the task of predicting the native protein sequence given its backbone atom (N, C<italic toggle="yes">α</italic>, C) coordinates. Perplexity and sequence recovery on held-out native sequences are two commonly used metrics for this task. Perplexity measures the inverse likelihood of native sequences in the predicted sequence distribution (low perplexity for high likelihood). Sequence recovery (accuracy) measures how often sampled sequences match the native sequence at each position. To maximize sequence recovery, the predicted sequences are sampled with low temperature <italic toggle="yes">T</italic> = 1e<italic toggle="yes">−</italic>6 from the model. <xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-1" hwp:rel-id="T1">Table 1</xref> compares models using these metrics on the structurally held-out backbones.</p><table-wrap id="tbl1" orientation="portrait" position="float" hwp:id="T1" hwp:rev-id="xref-table-wrap-1-1 xref-table-wrap-1-2 xref-table-wrap-1-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBL1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T1</object-id><object-id pub-id-type="publisher-id">tbl1</object-id><label>Table 1.</label><caption hwp:id="caption-5"><p hwp:id="p-33">Fixed backbone sequence design. Evaluation on the CATH 4.3 topology split test set. Models are compared on the basis of per-residue perplexity (lower is better; lowest perplexity bolded) and sequence recovery (higher is better; highest sequence recovery bolded). Large models can make better use of the predicted UniRef50 structures. The best model trained with predicted structures (GVP-Transformer) improves sequence recovery by 8.9 percentage points over the best model (GVP-GNN) trained on CATH only.</p></caption><graphic xlink:href="487779v1_tbl1" position="float" orientation="portrait" hwp:id="graphic-6"/></table-wrap><p hwp:id="p-34">We observe that current state-of-the-art inverse folding models are limited by the CATH training set. Scaling the current 1M parameter model (GVP-GNN) to 21M parameters (GVP-GNN-large) on the CATH dataset results in overfitting with a degradation of sequence recovery from 42.2% to 39.2% (<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-2" hwp:rel-id="T1">Table 1</xref>). On the other hand, the current model at the 1M parameter scale cannot make use of the predicted structures: training GVP-GNN with predicted structures results in a degradation to 38.6% sequence recovery (<xref rid="tbl1" ref-type="table" hwp:id="xref-table-wrap-1-3" hwp:rel-id="T1">Table 1</xref>), with performance worsening with increasing numbers of predicted structures in training (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-4" hwp:rel-id="F6">Figure 6a</xref>).</p><p hwp:id="p-35">Larger models benefit from training on the AlphaFold2-predicted UniRef50 structures. Training with predicted structures increases sequence recovery from 39.2% to 50.8% for GVP-GNN-large and from 38.3% to 51.6% for GVP-Transformer over training only on the experimentally derived structures. The improvements are also reflected in perplexity. Similar improvements are observed on the test subset filtered by TM-score (<xref rid="tblB1" ref-type="table" hwp:id="xref-table-wrap-5-1" hwp:rel-id="T5">Table B.1</xref>). The best model trained with UniRef50 predicted stuctures, GVP-Transformer, improves sequence recovery by 9.4 percentage points over the best model, GVP-GNN, trained on CATH alone.</p><p hwp:id="p-36">As there are many sequences that can fold to approximately the same structure, even an ideal protein design model will not have 100% native sequence recovery. We observe that the GVP-GNN-large and GVP-Transformer models are wellcalibrated (<xref rid="figC5" ref-type="fig" hwp:id="xref-fig-14-1" hwp:rel-id="F14">Figure C.5</xref>). The substitution matrix between native sequences and model-designed sequences resembles the BLOSUM62 substitution matrix (<xref rid="figC4" ref-type="fig" hwp:id="xref-fig-13-1" hwp:rel-id="F13">Figure C.4</xref>), albeit noticeably sparser for the amino acid Proline.</p><p hwp:id="p-37">When we break down performance on core residues and surface residues, as expected, core residues are more constrained and have a high native sequence recovery rate of 72%, while surface residues are not as constrained and have a lower sequence recovery of 39% (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-1" hwp:rel-id="F5">Figure 5</xref>; top). Generally perplexity increases with the solvent accessible surface area (<xref rid="fig5" ref-type="fig" hwp:id="xref-fig-5-2" hwp:rel-id="F5">Figure 5</xref>; bottom). Despite the lower sequence recovery on the surface, sampled sequences do tend not to have hydrophobic residues on the surface (<xref rid="figC6" ref-type="fig" hwp:id="xref-fig-15-1" hwp:rel-id="F15">Figure C.6</xref>).</p><fig id="fig5" position="float" fig-type="figure" orientation="portrait" hwp:id="F5" hwp:rev-id="xref-fig-5-1 xref-fig-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIG5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F5</object-id><object-id pub-id-type="publisher-id">fig5</object-id><label>Figure 5.</label><caption hwp:id="caption-6"><p hwp:id="p-38">Comparison of perplexity and sequence recovery by structural context according to two different measures: number of neighbors (top) and solvent accessible surface area (bottom). Top: Breakdown for core and surface residues. Residues are categorized by density of neighboring C<italic toggle="yes">α</italic> atoms within 10A of the central residue C<italic toggle="yes">α</italic> atom (core: <italic toggle="yes">≥</italic> 24 neighbors; surface: <italic toggle="yes">&lt;</italic> 16 neighbors). Each box shows the distribution of perplexities for the core or surface residues across different sequences. Bottom: Perplexity and sequence recovery as a function of solvent accessible surface area. Increased sequence recovery for buried residues suggests the model learns dense hydrophobic packing constraints in the core.</p></caption><graphic xlink:href="487779v1_fig5" position="float" orientation="portrait" hwp:id="graphic-7"/></fig><p hwp:id="p-39">As an example of inverse folding of a structurally-remote protein, we re-design the receptor binding domain (RBD) sequence of the SARS-CoV-2 spike protein (PDB: 6XRA and 6VXX; illustrated in <xref rid="figC3" ref-type="fig" hwp:id="xref-fig-12-1" hwp:rel-id="F12">Figure C.3</xref>) with the two models. The SARS-CoV-2 spike protein has no match to the training data with TM-score above 0.5. Both GVP-GNN and GVP-Transformer achieve high sequence recovery (49.7% and 53.6%) for the native RBD sequence (<xref rid="tblC3" ref-type="table" hwp:id="xref-table-wrap-8-1" hwp:rel-id="T8">Table C.3</xref>).</p><sec id="s3a1" hwp:id="sec-14"><title hwp:id="title-16">Partially-masked backbones</title><p hwp:id="p-40">We evaluate the models on partial backbones. While masking during training does not significantly change test performance on unmasked backbones (<xref rid="tblC1" ref-type="table" hwp:id="xref-table-wrap-6-3" hwp:rel-id="T6">Table C.1</xref>), masking does enable models to non-trivially predict sequences for mask regions. Although GVP-GNN-large has low perplexity on short-length masks, its performance quickly degrades to the perplexity of the background distribution on masks longer than 5 amino acids (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-2" hwp:rel-id="F4">Figure 4</xref>). By contrast, the GVP-Transformer model maintains moderate performance even on longer masked regions, with less degradation if trained with span masking instead of independent random masking (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-3" hwp:rel-id="F4">Figure 4</xref>).</p></sec><sec id="s3a2" hwp:id="sec-15"><title hwp:id="title-17">Protein complexes</title><p hwp:id="p-41">Although the training data only consists of single chains, we find that models generalize to multi-chain protein complexes. We represent complexes by concatenating the chains together with 10 mask tokens between chains, and include all complexes in the test set up to length 1000. For chains that are part of a protein complex, there is a substantial improvement in perplexity of both models when given the full complex coordinates as input, versus only the single chain (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-1" hwp:rel-id="T2">Table 2</xref> and <xref rid="figC2" ref-type="fig" hwp:id="xref-fig-11-1" hwp:rel-id="F11">Figure C.2</xref>), suggesting that both GVP-GNN and GVP-Transformer can make use of inter-chain information from amino acids that are close in 3D structure but far apart in sequence.</p><table-wrap id="tbl2" orientation="portrait" position="float" hwp:id="T2" hwp:rev-id="xref-table-wrap-2-1 xref-table-wrap-2-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBL2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T2</object-id><object-id pub-id-type="publisher-id">tbl2</object-id><label>Table 2.</label><caption hwp:id="caption-7"><p hwp:id="p-42">Sequence design performance on complexes in the CATH topology test split when given the backbone coordinates of only a chain (“Chain” column) and when given all backbone coordinates of the complex (“Complex” column). The perplexity is evaluated on the same chain in the complex for both columns.</p></caption><graphic xlink:href="487779v1_tbl2" position="float" orientation="portrait" hwp:id="graphic-8"/></table-wrap></sec><sec id="s3a3" hwp:id="sec-16"><title hwp:id="title-18">Multiple conformations</title><p hwp:id="p-43">Multi-state design is of interest for engineering enzymes and biosensors (<xref rid="c36" ref-type="bibr" hwp:id="xref-ref-36-1" hwp:rel-id="ref-36">Langan et al., 2019</xref>; <xref rid="c50" ref-type="bibr" hwp:id="xref-ref-50-1" hwp:rel-id="ref-50">Quijano-Rubio et al., 2021</xref>). Some proteins exist in multiple distinct folded forms in equilibrium, while other proteins may exhibit distinct conformations when binding to partner molecules. For a backbone <italic toggle="yes">X</italic>, the inverse folding model predicts a conditional distribution <italic toggle="yes">p</italic>(<italic toggle="yes">Y</italic> |<italic toggle="yes">X</italic>) over possible sequences <italic toggle="yes">Y</italic> for the backbone. To design a protein with two states <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic>, we would like find sequences that have high likelihoods in the conditional distributions <italic toggle="yes">p</italic>(<italic toggle="yes">Y</italic>|<italic toggle="yes">A</italic>) and <italic toggle="yes">p</italic>(<italic toggle="yes">Y</italic>|<italic toggle="yes">B</italic>) for each of the two states. We use the geometric average of the two conditional likelihoods as a proxy for the desired distribution <italic toggle="yes">p</italic>(<italic toggle="yes">Y</italic>|<italic toggle="yes">A, B</italic>) conditioned on the sequence being compatible with both states.</p><p hwp:id="p-44">We compare single-state and multi-state sequence design performance on 87 test split proteins with multiple conformations in the PDBFlex dataset (<xref rid="c24" ref-type="bibr" hwp:id="xref-ref-24-1" hwp:rel-id="ref-24">Hrabe et al., 2016</xref>). On locally flexible residues, multi-state design results in lower sequence perplexity than single-state design (<xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-1" hwp:rel-id="F7">Figure 7</xref>). See <xref ref-type="sec" rid="s8" hwp:id="xref-sec-40-1" hwp:rel-id="sec-40">Appendix C</xref> for more details on the PDBFlex data.</p><fig id="fig6" position="float" fig-type="figure" orientation="portrait" hwp:id="F6" hwp:rev-id="xref-fig-6-1 xref-fig-6-2 xref-fig-6-3 xref-fig-6-4 xref-fig-6-5 xref-fig-6-6"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIG6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F6</object-id><object-id pub-id-type="publisher-id">fig6</object-id><label>Figure 6.</label><caption hwp:id="caption-8"><p hwp:id="p-45">Ablation studies on training data. (a) Effect of increasing the number of predicted structures. The original GVP-GNN degrades with training on additional data, but GVP-GNN-large and GVP-Transformer improve with increasing numbers of predicted structures. (b) Effect of increasing the mixing ratio during training between predicted and experimental structures. A higher ratio of predicted structures improves performance for both GVP-GNN-large and GVP-Transformer. (c) GVP-GNN and GVP-Transformer model size.</p></caption><graphic xlink:href="487779v1_fig6" position="float" orientation="portrait" hwp:id="graphic-9"/></fig><fig id="fig7" position="float" fig-type="figure" orientation="portrait" hwp:id="F7" hwp:rev-id="xref-fig-7-1 xref-fig-7-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIG7</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F7</object-id><object-id pub-id-type="publisher-id">fig7</object-id><label>Figure 7.</label><caption hwp:id="caption-9"><p hwp:id="p-46">Dual-state design. GVP-Transformer conditioned on two conformations results in lower sequence perplexity at locally flexible residues than single-conformation conditioning for structurally held-out proteins in PDBFlex (see <xref ref-type="sec" rid="s8" hwp:id="xref-sec-40-2" hwp:rel-id="sec-40">Appendix C</xref> for details).</p></caption><graphic xlink:href="487779v1_fig7" position="float" orientation="portrait" hwp:id="graphic-10"/></fig></sec></sec><sec id="s3b" hwp:id="sec-17"><label>3.2.</label><title hwp:id="title-19">Zero-shot predictions</title><p hwp:id="p-47">We next show that inverse folding models are effective zero-shot predictors of mutational effects across practical design applications, including prediction of complex stability, binding affinity, and insertion effects. To score the effect of a mutation on a particular sequence, we use the ratio between likelihoods of the mutated and wildtype sequences according to the inverse folding model, given the experimentally determined wildtype structure. Exact likelihood evaluations are possible from both GVP-GNN and GVP-Transformer as they are both based on autoregressive decoders. We then compare these likelihood ratio scores to experimentally-determined fitness values measured on the same set of sequences.</p><sec id="s3b1" hwp:id="sec-18"><title hwp:id="title-20"><italic toggle="yes">De novo</italic> mini-proteins</title><p hwp:id="p-48"><xref rid="c54" ref-type="bibr" hwp:id="xref-ref-54-1" hwp:rel-id="ref-54">Rocklin et al. (2017)</xref> performed deep mutational scans across a set of <italic toggle="yes">de novo</italic> designed miniproteins with 10 different folds measuring the stability in response to point mutations. The likelihoods of inverse folding models have been shown to correlate with experimentally measured stability using this dataset (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-4" hwp:rel-id="ref-26">Ingraham et al., 2019</xref>; <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-6" hwp:rel-id="ref-30">Jing et al., 2021b</xref>). We evaluate the GVP-Transformer and GVP-GNN-large models on the same mutational scans, and observe improvements in stability predictions from using predicted structures as training data for 8 out of 10 folds in the dataset (<xref rid="tblC2" ref-type="table" hwp:id="xref-table-wrap-7-1" hwp:rel-id="T7">Table C.2</xref>). Further details are in <xref ref-type="sec" rid="s8" hwp:id="xref-sec-40-3" hwp:rel-id="sec-40">Appendix C</xref>.</p></sec><sec id="s3b2" hwp:id="sec-19"><title hwp:id="title-21">Complex stability</title><p hwp:id="p-49">We evaluate models on zero-shot prediction of mutational effects on protein complex interfaces, using the Atom3D benchmark (<xref rid="c65" ref-type="bibr" hwp:id="xref-ref-65-1" hwp:rel-id="ref-65">Townshend et al., 2020</xref>) which incorporates binding free energy changes in the SKEMPI database (<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-1" hwp:rel-id="ref-27">Jankauskaitė et al., 2019</xref>) as a binary classification task. We find that sequence log-likelihoods from GVP-GNN are effective zero-shot predictors of stability changes of protein complexes even without predicted structures as training data (<xref rid="tblC4" ref-type="table" hwp:id="xref-table-wrap-9-1" hwp:rel-id="T9">Table C.4</xref>), performing comparably to the best supervised method which uses transfer learning. While we observe a substantial improvement in perplexity when predicted structures are added to training (<xref rid="tbl2" ref-type="table" hwp:id="xref-table-wrap-2-2" hwp:rel-id="T2">Table 2</xref>), this does not further improve complex stability prediction for the single-point mutations in SKEMPI (<xref rid="tblC4" ref-type="table" hwp:id="xref-table-wrap-9-2" hwp:rel-id="T9">Table C.4</xref>), indicating potential limitations of evaluating models only on single-point mutations.</p></sec><sec id="s3b3" hwp:id="sec-20"><title hwp:id="title-22">Binding affinity</title><p hwp:id="p-50">While the SKEMPI dataset features one mutation entry per protein, we also want to evaluate whether inverse folding models can rank different mutations on the same protein, potentially enabling binding-affinity optimization, which is an important task in therapeutic design. We assess whether inverse folding models can predict muta-tional effects on binding by leveraging a dataset generated by <xref rid="c60" ref-type="bibr" hwp:id="xref-ref-60-1" hwp:rel-id="ref-60">Starr et al. (2020)</xref> in which all single amino acid substi-tutions to the SARS-CoV-2 receptor binding domain (RBD) were experimentally measured for binding affinity to human ACE2. Given potential applications to interface optimization or design, we focus on mutations within the receptor binding motif (RBM), the portion of the RBD in direct contact with ACE2 (<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-1" hwp:rel-id="ref-35">Lan et al., 2020</xref>). When given all RBD and ACE2 coordinates, the best inverse folding model produces RBD-sequence log-likelihoods that have a Spearman correlation of 0.69 with experimental binding affinity measurements (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-1" hwp:rel-id="T3">Table 3</xref>). We observe weaker correlations when not providing the model with ACE2 coordinates, indicating that inverse folding models take advantage of structural information in the binding partner. When masking RBM coordinates (69 of 195 residues, a longer span than masked during model training), we no longer observe correlation between RBD log-likelihood and binding affinity, indicating that the model relies on structural information at the interface to identify interface designs that preserve binding. Zero-shot prediction via inverse folding outperforms methods for sequence-based variant effect prediction, which use the likelihood ratio between the mutant and wildtype amino acids at each position to predict the impact of a mutation on binding affinity. These likelihoods are inferred by masked language models, ESM-1b, ESM-1v, and ESM-MSA-1b, as described by <xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-1" hwp:rel-id="ref-42">Meier et al. (2021)</xref> (<xref rid="tbl3" ref-type="table" hwp:id="xref-table-wrap-3-2" hwp:rel-id="T3">Table 3</xref>); additional details are given in <xref ref-type="sec" rid="s8" hwp:id="xref-sec-40-4" hwp:rel-id="sec-40">Appendix C</xref>.</p><table-wrap id="tbl3" orientation="portrait" position="float" hwp:id="T3" hwp:rev-id="xref-table-wrap-3-1 xref-table-wrap-3-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBL3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T3</object-id><object-id pub-id-type="publisher-id">tbl3</object-id><label>Table 3.</label><caption hwp:id="caption-10"><p hwp:id="p-51">Zero-shot performance on binding affinity prediction for the receptor binding domain (RBD) of SARS-CoV-2 Spike, evaluated on ACE2-RBD mutational scan data (<xref rid="c60" ref-type="bibr" hwp:id="xref-ref-60-2" hwp:rel-id="ref-60">Starr et al., 2020</xref>). The zero-shot predictions are based on the sequence log-likelihood for the receptor binding motif (RBM), which is the portion of the RBD in direct contact with ACE2 (<xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-2" hwp:rel-id="ref-35">Lan et al., 2020</xref>). We evaluate in four settings: 1) Given sequence data alone (“No coords”); 2) Given backbone coordinates for both ACE2 and the RBD but excluding the RBM and without sequence (“No RBM coords”); 3) Given the full backbone for the RBD but no information for ACE2 (“No ACE2 coords”); and 4) Given all coordinates for the RBD and ACE2.</p></caption><graphic xlink:href="487779v1_tbl3" position="float" orientation="portrait" hwp:id="graphic-11"/></table-wrap></sec><sec id="s3b4" hwp:id="sec-21"><title hwp:id="title-23">Sequence insertions</title><p hwp:id="p-52">Using masked coordinate tokens at insertion regions, inverse folding models can also predict insertion effects. On adeno-associated virus (AAV) capsid variants, we show that relative differences in sequence log-likelihoods correlate with the experimentally measured insertion effects from <xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-2" hwp:rel-id="ref-12">Bryant et al. (2021)</xref>. As shown in <xref rid="tblC5" ref-type="table" hwp:id="xref-table-wrap-10-1" hwp:rel-id="T10">Table C.5</xref>, both GVP-GNN and GVP-Transformer outperform the sequence-only zero-shot prediction baseline ESM-1v (<xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-2" hwp:rel-id="ref-42">Meier et al., 2021</xref>). When evaluating on subsets of sequences increasingly further away from the wildtype (<italic toggle="yes">≥</italic> 2, <italic toggle="yes">≥</italic> 3, and <italic toggle="yes">≥</italic> 8 mutations), the GVP-GNN-large and GVP-Transformer models trained with predicted structures have increasing advantages compared to GVP-GNN trained with-out predicted structures.</p></sec></sec></sec><sec id="s4" hwp:id="sec-22"><label>4.</label><title hwp:id="title-24">Related work</title><sec id="s4a" hwp:id="sec-23"><title hwp:id="title-25">Structure-based protein sequence design</title><p hwp:id="p-53">Early work on design of protein sequences studied the packing of amino acid side chains to fill the interior space of predetermined backbone structures, either for a fixed backbone conformation (<xref rid="c62" ref-type="bibr" hwp:id="xref-ref-62-1" hwp:rel-id="ref-62">Street &amp; Mayo, 1999</xref>; <xref rid="c14" ref-type="bibr" hwp:id="xref-ref-14-1" hwp:rel-id="ref-14">Dahiyat &amp; Mayo, 1997</xref>; <xref rid="c16" ref-type="bibr" hwp:id="xref-ref-16-1" hwp:rel-id="ref-16">De-Grado et al., 1991</xref>), or with flexibility in the backbone conformation (<xref rid="c21" ref-type="bibr" hwp:id="xref-ref-21-1" hwp:rel-id="ref-21">Harbury et al., 1998</xref>). Since then, the Rosetta energy function (<xref rid="c1" ref-type="bibr" hwp:id="xref-ref-1-1" hwp:rel-id="ref-1">Alford et al., 2017</xref>) has become an established approach for structure-based sequence design. An alternative non-parametric approach involves decomposing the library of known structures into common sequence-structure motifs (<xref rid="c76" ref-type="bibr" hwp:id="xref-ref-76-1" hwp:rel-id="ref-76">Zhou et al., 2020</xref>).</p><p hwp:id="p-54">Early machine learning approaches in structure-based protein sequence design used fragment-based and energy-based global features derived from structures (<xref rid="c39" ref-type="bibr" hwp:id="xref-ref-39-1" hwp:rel-id="ref-39">Li et al., 2014</xref>; <xref rid="c45" ref-type="bibr" hwp:id="xref-ref-45-1" hwp:rel-id="ref-45">O’Connell et al., 2018</xref>). More recently, convolution-based deep learning methods have also been applied to predict amino acid propensities given the surrounding local structural environments (<xref rid="c4" ref-type="bibr" hwp:id="xref-ref-4-3" hwp:rel-id="ref-4">Anand-Achim et al., 2021</xref>; <xref rid="c10" ref-type="bibr" hwp:id="xref-ref-10-1" hwp:rel-id="ref-10">Boomsma &amp; Frellsen, 2017</xref>; <xref rid="c58" ref-type="bibr" hwp:id="xref-ref-58-1" hwp:rel-id="ref-58">Shroff et al., 2020</xref>; <xref rid="c38" ref-type="bibr" hwp:id="xref-ref-38-1" hwp:rel-id="ref-38">Li et al., 2020</xref>; <xref rid="c49" ref-type="bibr" hwp:id="xref-ref-49-1" hwp:rel-id="ref-49">Qi &amp; Zhang, 2020</xref>; <xref rid="c75" ref-type="bibr" hwp:id="xref-ref-75-1" hwp:rel-id="ref-75">Zhang et al., 2020</xref>; <xref rid="c13" ref-type="bibr" hwp:id="xref-ref-13-1" hwp:rel-id="ref-13">Chen et al., 2019</xref>; <xref rid="c70" ref-type="bibr" hwp:id="xref-ref-70-1" hwp:rel-id="ref-70">Wang et al., 2018</xref>). Another recent machine learning approach is to leverage structure prediction networks for sequence design. <xref rid="c44" ref-type="bibr" hwp:id="xref-ref-44-1" hwp:rel-id="ref-44">Norn et al. (2021)</xref> carried out Monte Carlo sampling in the sequence space to invert the trRosetta (<xref rid="c73" ref-type="bibr" hwp:id="xref-ref-73-1" hwp:rel-id="ref-73">Yang et al., 2020</xref>) structure prediction network for sequence design.</p></sec><sec id="s4b" hwp:id="sec-24"><title hwp:id="title-26">Generative models of proteins</title><p hwp:id="p-55">The literature on structure-based generative models of protein sequences is the closest to our work. <xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-5" hwp:rel-id="ref-26">Ingraham et al. (2019)</xref> introduced the formulation of fixed-backbone design as a conditional sequence generation problem, using invariant features with graph neural networks, modeling each amino acid as a node in the graph with edges connecting spatially adjacent amino acids. <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-7" hwp:rel-id="ref-30">Jing et al. (2021b;a)</xref> further improved graph neural networks for this task by developing architectures with translation-and rotation-equivariance to enable geometric reasoning, showing that GVP-GNN achieves higher native sequence recovery rates than Rosetta on TS50, a bench-mark set of 50 protein chains. <xref rid="c63" ref-type="bibr" hwp:id="xref-ref-63-2" hwp:rel-id="ref-63">Strokach et al. (2020)</xref> trained graph neural networks for conditional generation with the masked language modeling objective, adding homologous sequences as data augmentation to training.</p><p hwp:id="p-56">Recently models have been proposed to jointly generate structures and sequences. <xref rid="c6" ref-type="bibr" hwp:id="xref-ref-6-2" hwp:rel-id="ref-6">Anishchenko et al. (2021)</xref> generate structures by optimizing sequences through the trRosetta structure prediction network to maximize their difference from a background distribution. The joint generation approach is also being explored in the setting of infilling partial structures. Contemporary to this work, <xref rid="c70" ref-type="bibr" hwp:id="xref-ref-70-2" hwp:rel-id="ref-70">Wang et al. (2021)</xref> apply span masking to fine-tune the RosettaFold model (<xref rid="c7" ref-type="bibr" hwp:id="xref-ref-7-2" hwp:rel-id="ref-7">Baek et al., 2021</xref>) to perform infilling. However Wang et al. do not consider inverse folding, and condition on both coordinates and amino acid identities. Also contemporary to this work, <xref rid="c28" ref-type="bibr" hwp:id="xref-ref-28-1" hwp:rel-id="ref-28">Jin et al. (2021)</xref> develop a conditional generation model for jointly generating sequences and structures for antibody complementarity determining regions (CDRs), conditioned on framework region structures.</p><p hwp:id="p-57">So far there has been little work on generative models of structures directly. Interesting examples include <xref rid="c3" ref-type="bibr" hwp:id="xref-ref-3-2" hwp:rel-id="ref-3">Anand &amp; Huang (2018)</xref> who model fixed-length protein backbones with generative adversarial networks (GANs) via pairwise distance matrices, and <xref rid="c18" ref-type="bibr" hwp:id="xref-ref-18-1" hwp:rel-id="ref-18">Eguchi et al. (2020)</xref> who generate antibody structures with variational autoencoders (VAEs).</p></sec><sec id="s4c" hwp:id="sec-25"><title hwp:id="title-27">Language models</title><p hwp:id="p-58">A large body of work has focused on modeling the sequences in individual protein families. <xref rid="c57" ref-type="bibr" hwp:id="xref-ref-57-2" hwp:rel-id="ref-57">Shin et al. (2021)</xref> show that protein-specific autoregressive sequence models trained on related proteins can predict point mutation and indel effects and design functional nanobodies. <xref rid="c66" ref-type="bibr" hwp:id="xref-ref-66-1" hwp:rel-id="ref-66">Trinquier et al. (2021)</xref> also studied protein-specific autoregressive models for sequence generation.</p><p hwp:id="p-59">Recently language models have been proposed for modeling large scale databases of protein sequences rather than families of related sequences. Examples include (<xref rid="c8" ref-type="bibr" hwp:id="xref-ref-8-1" hwp:rel-id="ref-8">Bepler &amp; Berger, 2019</xref>; <xref rid="c2" ref-type="bibr" hwp:id="xref-ref-2-1" hwp:rel-id="ref-2">Alley et al., 2019</xref>; <xref rid="c22" ref-type="bibr" hwp:id="xref-ref-22-1" hwp:rel-id="ref-22">Heinzinger et al., 2019</xref>; <xref rid="c51" ref-type="bibr" hwp:id="xref-ref-51-1" hwp:rel-id="ref-51">Rao et al., 2019</xref>; <xref rid="c40" ref-type="bibr" hwp:id="xref-ref-40-1" hwp:rel-id="ref-40">Madani et al., 2020</xref>; <xref rid="c19" ref-type="bibr" hwp:id="xref-ref-19-1" hwp:rel-id="ref-19">Elnaggar et al., 2021</xref>; <xref rid="c53" ref-type="bibr" hwp:id="xref-ref-53-2" hwp:rel-id="ref-53">Rives et al., 2021</xref>; <xref rid="c52" ref-type="bibr" hwp:id="xref-ref-52-2" hwp:rel-id="ref-52">Rao et al., 2021</xref>). <xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-3" hwp:rel-id="ref-42">Meier et al. (2021)</xref> found that the log-likelihoods of large protein language models predict mutational effects. <xref rid="c41" ref-type="bibr" hwp:id="xref-ref-41-2" hwp:rel-id="ref-41">Madani et al. (2021)</xref> study an autoregressive sequence model conditioned on functional annotations and show it can generate functional proteins.</p></sec><sec id="s4d" hwp:id="sec-26"><title hwp:id="title-28">Structure-agnostic protein sequence design</title><p hwp:id="p-60">We point the reader to <xref rid="c72" ref-type="bibr" hwp:id="xref-ref-72-1" hwp:rel-id="ref-72">Wu et al. (2021)</xref> for a review of the many machine learning-based sequence design approaches that do not explicitly model protein structures. Additionally, as an alternative to sequence generation models, model-guided algorithms design sequences based on predictive models as oracles (<xref rid="c74" ref-type="bibr" hwp:id="xref-ref-74-1" hwp:rel-id="ref-74">Yang et al., 2019</xref>; <xref rid="c5" ref-type="bibr" hwp:id="xref-ref-5-1" hwp:rel-id="ref-5">Angermueller et al., 2019</xref>; <xref rid="c11" ref-type="bibr" hwp:id="xref-ref-11-1" hwp:rel-id="ref-11">Brookes et al., 2019</xref>; <xref rid="c59" ref-type="bibr" hwp:id="xref-ref-59-1" hwp:rel-id="ref-59">Sinai et al., 2020</xref>).</p></sec><sec id="s4e" hwp:id="sec-27"><title hwp:id="title-29">Back-translation</title><p hwp:id="p-61">For machine translation (MT) in NLP, <xref rid="c56" ref-type="bibr" hwp:id="xref-ref-56-2" hwp:rel-id="ref-56">Sennrich et al. (2015)</xref> studied how to leverage large amounts of monolingual data in the target language, a setting that parallels the situation we consider with protein sequences (the target language in our case). Sennrich et al. found it most effective to generate synthetic source sentences by performing the backwards translation from the target sentence, i.e. back-translation. This parallels the approach we take of predicting structures for sequence targets that have un-known structures. <xref rid="c17" ref-type="bibr" hwp:id="xref-ref-17-4" hwp:rel-id="ref-17">Edunov et al. (2018)</xref> further investigated back-translation for large-scale language models.</p></sec></sec><sec id="s5" hwp:id="sec-28"><label>5.</label><title hwp:id="title-30">Conclusions</title><p hwp:id="p-62">While there are billions of protein sequences in the largest sequence databases, the number of available experimentally determined structures is on the order of hundreds of thousands, imposing a limit on generative methods that learn from protein structure data. In this work, we explored whether predicted structures from recent deep learning methods can be used in tandem with experimental structures to train models for protein design.</p><p hwp:id="p-63">To this end, we generated structures for 12 million UniRef50 sequences using AlphaFold2. As a result of training with this data we observe improvements in perplexity and sequence recovery by substantial margins, and demonstrate generalization to longer protein complexes, to proteins in multiple conformations, and to zero-shot prediction for mutation effects on binding affinity and AAV packaging. These results highlight that in addition to the geometric inductive biases which have been the major focus for work on inverse-folding to date, finding ways to leverage more sources of training data is an equally important path to improved modeling capabilities.</p><p hwp:id="p-64">We also take initial steps toward more general structure-conditional protein design tasks. By integrating backbone span masking into the inverse folding task and using a sequence-to-sequence transformer, reasonable sequence predictions can be achieved for short masked spans.</p><p hwp:id="p-65">If ways can be found to continue to leverage predicted structures for generative models of proteins, it may be possible to create models that learn to design proteins from an expanded universe of the billions of natural sequences whose structures are currently unknown.</p></sec></body><back><ack hwp:id="ack-1"><title hwp:id="title-31">Acknowledgements</title><p hwp:id="p-66">We thank Halil Akin, Sal Candido, Ori Kabeli, Joshua Meier, Ammar Rizvi, and Zhongkai Zhu for feedback on the manuscript and insightful conversations.</p></ack><ref-list hwp:id="ref-list-1"><title hwp:id="title-32">References</title><ref id="c1" hwp:id="ref-1" hwp:rev-id="xref-ref-1-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.1" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-1"><string-name name-style="western" hwp:sortable="Alford R. F."><surname>Alford</surname>, <given-names>R. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Leaver-Fay A."><surname>Leaver-Fay</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jeliazkov J. R."><surname>Jeliazkov</surname>, <given-names>J. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="O’Meara M. J."><surname>O’Meara</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="DiMaio F. P."><surname>DiMaio</surname>, <given-names>F. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park H."><surname>Park</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shapovalov M. V."><surname>Shapovalov</surname>, <given-names>M. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Renfrew P. D."><surname>Renfrew</surname>, <given-names>P. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mulligan V. K."><surname>Mulligan</surname>, <given-names>V. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kappel K."><surname>Kappel</surname>, <given-names>K.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-2">The rosetta all-atom energy function for macromolecular modeling and design</article-title>. <source hwp:id="source-1">Journal of chemical theory and computation</source>, <volume>13</volume> (<issue>6</issue>):<fpage>3031</fpage>–<lpage>3048</lpage>, <year>2017</year>.</citation></ref><ref id="c2" hwp:id="ref-2" hwp:rev-id="xref-ref-2-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.2" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-2"><string-name name-style="western" hwp:sortable="Alley E. C."><surname>Alley</surname>, <given-names>E. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Khimulya G."><surname>Khimulya</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Biswas S."><surname>Biswas</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="AlQuraishi M."><surname>AlQuraishi</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Church G. M."><surname>Church</surname>, <given-names>G. M.</given-names></string-name> <article-title hwp:id="article-title-3">Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source hwp:id="source-2">Nature methods</source>, <volume>16</volume>(<issue>12</issue>):<fpage>1315</fpage>–<lpage>1322</lpage>, <year>2019</year>.</citation></ref><ref id="c3" hwp:id="ref-3" hwp:rev-id="xref-ref-3-1 xref-ref-3-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.3" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-3"><string-name name-style="western" hwp:sortable="Anand N."><surname>Anand</surname>, <given-names>N.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Huang P."><surname>Huang</surname>, <given-names>P.</given-names></string-name> <article-title hwp:id="article-title-4">Generative modeling for protein structures</article-title>. <source hwp:id="source-3">Advances in neural information processing systems</source>, <volume>31</volume>, <year>2018</year>.</citation></ref><ref id="c4" hwp:id="ref-4" hwp:rev-id="xref-ref-4-1 xref-ref-4-2 xref-ref-4-3"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.4" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-4"><string-name name-style="western" hwp:sortable="Anand-Achim N."><surname>Anand-Achim</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eguchi R. R."><surname>Eguchi</surname>, <given-names>R. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mathews I. I."><surname>Mathews</surname>, <given-names>I. I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perez C. P."><surname>Perez</surname>, <given-names>C. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Derry A."><surname>Derry</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Altman R. B."><surname>Altman</surname>, <given-names>R. B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Huang P.-S."><surname>Huang</surname>, <given-names>P.-S.</given-names></string-name> <article-title hwp:id="article-title-5">Protein sequence design with a learned potential</article-title>. <source hwp:id="source-4">Biorxiv</source>, pp. <fpage>2020</fpage>–<lpage>01</lpage>, <year>2021</year>.</citation></ref><ref id="c5" hwp:id="ref-5" hwp:rev-id="xref-ref-5-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.5" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-5"><string-name name-style="western" hwp:sortable="Angermueller C."><surname>Angermueller</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dohan D."><surname>Dohan</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Belanger D."><surname>Belanger</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Deshpande R."><surname>Deshpande</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Murphy K."><surname>Murphy</surname>, <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Colwell L."><surname>Colwell</surname>, <given-names>L.</given-names></string-name> <article-title hwp:id="article-title-6">Model-based reinforcement learning for biological sequence design</article-title>. <source hwp:id="source-5">In International conference on learning representations</source>, <year>2019</year>.</citation></ref><ref id="c6" hwp:id="ref-6" hwp:rev-id="xref-ref-6-1 xref-ref-6-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.6" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-6"><string-name name-style="western" hwp:sortable="Anishchenko I."><surname>Anishchenko</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pellock S. J."><surname>Pellock</surname>, <given-names>S. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chidyausiku T. M."><surname>Chidyausiku</surname>, <given-names>T. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ramelot T. A."><surname>Ramelot</surname>, <given-names>T. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ovchinnikov S."><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hao J."><surname>Hao</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bafna K."><surname>Bafna</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Norn C."><surname>Norn</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kang A."><surname>Kang</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bera A. K."><surname>Bera</surname>, <given-names>A. K.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-7">De novo protein design by deep network hallucination</article-title>. <source hwp:id="source-6">Nature</source>, <volume>600</volume>(<issue>7889</issue>):<fpage>547</fpage>–<lpage>552</lpage>, <year>2021</year>.</citation></ref><ref id="c7" hwp:id="ref-7" hwp:rev-id="xref-ref-7-1 xref-ref-7-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.7" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-7"><string-name name-style="western" hwp:sortable="Baek M."><surname>Baek</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="DiMaio F."><surname>DiMaio</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anishchenko I."><surname>Anishchenko</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dauparas J."><surname>Dauparas</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ovchinnikov S."><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee G. R."><surname>Lee</surname>, <given-names>G. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang J."><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cong Q."><surname>Cong</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kinch L. N."><surname>Kinch</surname>, <given-names>L. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Schaeffer R. D."><surname>Schaeffer</surname>, <given-names>R. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Millán C."><surname>Millán</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park H."><surname>Park</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adams C."><surname>Adams</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Glassman C. R."><surname>Glassman</surname>, <given-names>C. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="DeGiovanni A."><surname>DeGiovanni</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pereira J. H."><surname>Pereira</surname>, <given-names>J. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rodrigues A. V."><surname>Rodrigues</surname>, <given-names>A. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Dijk A. A."><surname>van Dijk</surname>, <given-names>A. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ebrecht A. C."><surname>Ebrecht</surname>, <given-names>A. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Op-perman D. J."><surname>Op-perman</surname>, <given-names>D. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sagmeister T."><surname>Sagmeister</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Buhlheller C."><surname>Buhlheller</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pavkov-Keller T."><surname>Pavkov-Keller</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rathinaswamy M. K."><surname>Rathinaswamy</surname>, <given-names>M. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dalwadi U."><surname>Dalwadi</surname>, <given-names>U.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yip C. K."><surname>Yip</surname>, <given-names>C. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Burke J. E."><surname>Burke</surname>, <given-names>J. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garcia K. C."><surname>Garcia</surname>, <given-names>K. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grishin N. V."><surname>Grishin</surname>, <given-names>N. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Adams P. D."><surname>Adams</surname>, <given-names>P. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Read R. J."><surname>Read</surname>, <given-names>R. J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Baker D."><surname>Baker</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-8">Accurate prediction of protein structures and interactions using a three-track neural network</article-title>. <source hwp:id="source-7">Science</source>, <volume>373</volume>(<issue>6557</issue>):<fpage>871</fpage>–<lpage>876</lpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1126/science.abj8754</pub-id>.</citation></ref><ref id="c8" hwp:id="ref-8" hwp:rev-id="xref-ref-8-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.8" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-8"><string-name name-style="western" hwp:sortable="Bepler T."><surname>Bepler</surname>, <given-names>T.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Berger B."><surname>Berger</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-9">Learning protein sequence embed-dings using information from structure</article-title>. <source hwp:id="source-8">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1902.08661</pub-id>, <year>2019</year>.</citation></ref><ref id="c9" hwp:id="ref-9" hwp:rev-id="xref-ref-9-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.9" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-9"><string-name name-style="western" hwp:sortable="Berman H. M."><surname>Berman</surname>, <given-names>H. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Westbrook J."><surname>Westbrook</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Feng Z."><surname>Feng</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gilliland G."><surname>Gilliland</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bhat T. N."><surname>Bhat</surname>, <given-names>T. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weissig H."><surname>Weissig</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shindyalov I. N."><surname>Shindyalov</surname>, <given-names>I. N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Bourne P. E."><surname>Bourne</surname>, <given-names>P. E.</given-names></string-name> <article-title hwp:id="article-title-10">The protein data bank</article-title>. <source hwp:id="source-9">Nucleic acids research</source>, <volume>28</volume>(<issue>1</issue>): <fpage>235</fpage>–<lpage>242</lpage>, <year>2000</year>.</citation></ref><ref id="c10" hwp:id="ref-10" hwp:rev-id="xref-ref-10-1"><citation publication-type="book" citation-type="book" ref:id="2022.04.10.487779v1.10" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-10"><string-name name-style="western" hwp:sortable="Boomsma W."><surname>Boomsma</surname>, <given-names>W.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Frellsen J."><surname>Frellsen</surname>, <given-names>J.</given-names></string-name> <chapter-title>Spherical convolutions and their application in molecular modelling</chapter-title>. In <person-group person-group-type="editor" hwp:id="person-group-1"><string-name name-style="western" hwp:sortable="Guyon I."><surname>Guyon</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Luxburg U. V."><surname>Luxburg</surname>, <given-names>U. V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bengio S."><surname>Bengio</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wallach H."><surname>Wallach</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fergus R."><surname>Fergus</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vish-wanathan S."><surname>Vish-wanathan</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Garnett R."><surname>Garnett</surname>, <given-names>R.</given-names></string-name></person-group> (eds.), <source hwp:id="source-10">Advances in Neural Information Processing Systems</source>, volume <volume>30</volume>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2017</year>. URL <ext-link l:rel="related" l:ref-type="uri" l:ref="https://proceedings.neurips.cc/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf" ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf" hwp:id="ext-link-4">https://proceedings.neurips.cc/paper/2017/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf</ext-link>.</citation></ref><ref id="c11" hwp:id="ref-11" hwp:rev-id="xref-ref-11-1"><citation publication-type="book" citation-type="book" ref:id="2022.04.10.487779v1.11" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-11"><string-name name-style="western" hwp:sortable="Brookes D."><surname>Brookes</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park H."><surname>Park</surname>, <given-names>H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Listgarten J."><surname>Listgarten</surname>, <given-names>J.</given-names></string-name> <chapter-title>Conditioning by adaptive sampling for robust design</chapter-title>. <source hwp:id="source-11">In International conference on machine learning</source>, pp. <fpage>773</fpage>–<lpage>782</lpage>. <publisher-name>PMLR</publisher-name>, <year>2019</year>.</citation></ref><ref id="c12" hwp:id="ref-12" hwp:rev-id="xref-ref-12-1 xref-ref-12-2 xref-ref-12-3"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.12" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-12"><string-name name-style="western" hwp:sortable="Bryant D. H."><surname>Bryant</surname>, <given-names>D. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bashir A."><surname>Bashir</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sinai S."><surname>Sinai</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jain N. K."><surname>Jain</surname>, <given-names>N. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ogden P. J."><surname>Ogden</surname>, <given-names>P. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Riley P. F."><surname>Riley</surname>, <given-names>P. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Church G. M."><surname>Church</surname>, <given-names>G. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Colwell L. J."><surname>Colwell</surname>, <given-names>L. J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kelsic E. D."><surname>Kelsic</surname>, <given-names>E. D.</given-names></string-name> <article-title hwp:id="article-title-11">Deep diversification of an aav capsid protein by machine learning</article-title>. <source hwp:id="source-12">Nature Biotechnology</source>, <volume>39</volume>(<issue>6</issue>):<fpage>691</fpage>–<lpage>696</lpage>, <year>2021</year>.</citation></ref><ref id="c13" hwp:id="ref-13" hwp:rev-id="xref-ref-13-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.13" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-13"><string-name name-style="western" hwp:sortable="Chen S."><surname>Chen</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sun Z."><surname>Sun</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lin L."><surname>Lin</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu Z."><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu X."><surname>Liu</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chong Y."><surname>Chong</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lu Y."><surname>Lu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhao H."><surname>Zhao</surname>, <given-names>H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Yang Y."><surname>Yang</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-12">To improve protein sequence profile prediction through image captioning on pairwise residue distance map</article-title>. <source hwp:id="source-13">Journal of chemical information and modeling</source>, <volume>60</volume>(<issue>1</issue>):<fpage>391</fpage>–<lpage>399</lpage>, <year>2019</year>.</citation></ref><ref id="c14" hwp:id="ref-14" hwp:rev-id="xref-ref-14-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.14" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-14"><string-name name-style="western" hwp:sortable="Dahiyat B. I."><surname>Dahiyat</surname>, <given-names>B. I.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Mayo S. L."><surname>Mayo</surname>, <given-names>S. L.</given-names></string-name> <article-title hwp:id="article-title-13">Probing the role of packing specificity in protein design</article-title>. <source hwp:id="source-14">Proceedings of the National Academy of Sciences</source>, <volume>94</volume>(<issue>19</issue>):<fpage>10172</fpage>–<lpage>10177</lpage>, <year>1997</year>.</citation></ref><ref id="c15" hwp:id="ref-15" hwp:rev-id="xref-ref-15-1 xref-ref-15-2 xref-ref-15-3"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.15" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-15"><string-name name-style="western" hwp:sortable="Dallago C."><surname>Dallago</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mou J."><surname>Mou</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnston K. E."><surname>Johnston</surname>, <given-names>K. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wittmann B. J."><surname>Wittmann</surname>, <given-names>B. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bhattacharya N."><surname>Bhattacharya</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goldman S."><surname>Goldman</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Madani A."><surname>Madani</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Yang K. K."><surname>Yang</surname>, <given-names>K. K.</given-names></string-name> <article-title hwp:id="article-title-14">Flip: Benchmark tasks in fitness landscape inference for proteins</article-title>. <source hwp:id="source-15">bioRxiv</source>, <year>2021</year>.</citation></ref><ref id="c16" hwp:id="ref-16" hwp:rev-id="xref-ref-16-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.16" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-16"><string-name name-style="western" hwp:sortable="DeGrado W. F."><surname>DeGrado</surname>, <given-names>W. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Raleigh D. P."><surname>Raleigh</surname>, <given-names>D. P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Handel T."><surname>Handel</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-15">De novo protein design: what are we learning?</article-title> <source hwp:id="source-16">Current Opinion in Structural Biology</source>, <volume>1</volume>(<issue>6</issue>):<fpage>984</fpage>–<lpage>993</lpage>, <year>1991</year>.</citation></ref><ref id="c17" hwp:id="ref-17" hwp:rev-id="xref-ref-17-1 xref-ref-17-2 xref-ref-17-3 xref-ref-17-4"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.17" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-17"><string-name name-style="western" hwp:sortable="Edunov S."><surname>Edunov</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ott M."><surname>Ott</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Auli M."><surname>Auli</surname>, <given-names>M.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Grangier D."><surname>Grangier</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-16">Understanding back-translation at scale</article-title>. <source hwp:id="source-17">arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1808.09381</pub-id>, <year>2018</year>.</citation></ref><ref id="c18" hwp:id="ref-18" hwp:rev-id="xref-ref-18-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.18" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-18"><string-name name-style="western" hwp:sortable="Eguchi R. R."><surname>Eguchi</surname>, <given-names>R. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anand N."><surname>Anand</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Choe C. A."><surname>Choe</surname>, <given-names>C. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Huang P.-S."><surname>Huang</surname>, <given-names>P.-S.</given-names></string-name> <article-title hwp:id="article-title-17">Ig-vae: generative modeling of immunoglobulin proteins by direct 3d coordinate generation</article-title>. <source hwp:id="source-18">bioRxiv</source>, <year>2020</year>.</citation></ref><ref id="c19" hwp:id="ref-19" hwp:rev-id="xref-ref-19-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.19" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-19"><string-name name-style="western" hwp:sortable="Elnaggar A."><surname>Elnaggar</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heinzinger M."><surname>Heinzinger</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dallago C."><surname>Dallago</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rehawi G."><surname>Rehawi</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yu W."><surname>Yu</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones L."><surname>Jones</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gibbs T."><surname>Gibbs</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Feher T."><surname>Feher</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Angerer C."><surname>Angerer</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Steinegger M."><surname>Steinegger</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bhowmik D."><surname>Bhowmik</surname>, <given-names>D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Rost B."><surname>Rost</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-18">Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing</article-title>. <source hwp:id="source-19">IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, pp. <fpage>1</fpage>–<lpage>1</lpage>, <year>2021</year>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2021.3095381</pub-id>.</citation></ref><ref id="c20" hwp:id="ref-20" hwp:rev-id="xref-ref-20-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.20" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-20"><string-name name-style="western" hwp:sortable="Gligorijevic V."><surname>Gligorijevic</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Berenberg D."><surname>Berenberg</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ra S."><surname>Ra</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Watkins A."><surname>Watkins</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kelow S."><surname>Kelow</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cho K."><surname>Cho</surname>, <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Bonneau R."><surname>Bonneau</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-19">Function-guided protein design by deep manifold sampling</article-title>. <source hwp:id="source-20">bioRxiv</source>, <year>2021</year>.</citation></ref><ref id="c21" hwp:id="ref-21" hwp:rev-id="xref-ref-21-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.21" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-21"><string-name name-style="western" hwp:sortable="Harbury P. B."><surname>Harbury</surname>, <given-names>P. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Plecs J. J."><surname>Plecs</surname>, <given-names>J. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tidor B."><surname>Tidor</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Alber T."><surname>Alber</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kim P. S."><surname>Kim</surname>, <given-names>P. S.</given-names></string-name> <article-title hwp:id="article-title-20">High-resolution protein design with backbone freedom</article-title>. <source hwp:id="source-21">Science</source>, <volume>282</volume>(<issue>5393</issue>):<fpage>1462</fpage>–<lpage>1467</lpage>, <year>1998</year>.</citation></ref><ref id="c22" hwp:id="ref-22" hwp:rev-id="xref-ref-22-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.22" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-22"><string-name name-style="western" hwp:sortable="Heinzinger M."><surname>Heinzinger</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Elnaggar A."><surname>Elnaggar</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang Y."><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dallago C."><surname>Dallago</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nechaev D."><surname>Nechaev</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Matthes F."><surname>Matthes</surname>, <given-names>F.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Rost B."><surname>Rost</surname>, <given-names>B.</given-names></string-name> <article-title hwp:id="article-title-21">Modeling aspects of the language of life through transfer-learning protein sequences</article-title>. <source hwp:id="source-22">BMC bioinformatics</source>, <volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>17</lpage>, <year>2019</year>.</citation></ref><ref id="c23" hwp:id="ref-23" hwp:rev-id="xref-ref-23-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.23" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-23"><string-name name-style="western" hwp:sortable="Hornak V."><surname>Hornak</surname>, <given-names>V.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abel R."><surname>Abel</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Okur A."><surname>Okur</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Strockbine B."><surname>Strockbine</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Roitberg A."><surname>Roitberg</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Simmerling C."><surname>Simmerling</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-22">Comparison of multiple amber force fields and development of improved protein back-bone parameters</article-title>. <source hwp:id="source-23">Proteins: Structure, Function, and Bioinformatics</source>, <volume>65</volume>(<issue>3</issue>):<fpage>712</fpage>–<lpage>725</lpage>, <year>2006</year>.</citation></ref><ref id="c24" hwp:id="ref-24" hwp:rev-id="xref-ref-24-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.24" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-24"><string-name name-style="western" hwp:sortable="Hrabe T."><surname>Hrabe</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Z."><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sedova M."><surname>Sedova</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rotkiewicz P."><surname>Rotkiewicz</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jaroszewski L."><surname>Jaroszewski</surname>, <given-names>L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Godzik A."><surname>Godzik</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-23">Pdbflex: exploring flexibility in protein structures</article-title>. <source hwp:id="source-24">Nucleic acids research</source>, <volume>44</volume>(<issue>D1</issue>):<fpage>D423</fpage>– <lpage>D428</lpage>, <year>2016</year>.</citation></ref><ref id="c25" hwp:id="ref-25" hwp:rev-id="xref-ref-25-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.25" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-25"><string-name name-style="western" hwp:sortable="Huang P.-S."><surname>Huang</surname>, <given-names>P.-S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Boyken S. E."><surname>Boyken</surname>, <given-names>S. E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Baker D."><surname>Baker</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-24">The coming of age of de novo protein design</article-title>. <source hwp:id="source-25">Nature</source>, <volume>537</volume>(<issue>7620</issue>): <fpage>320</fpage>–<lpage>327</lpage>, <year>2016</year>.</citation></ref><ref id="c26" hwp:id="ref-26" hwp:rev-id="xref-ref-26-1 xref-ref-26-2 xref-ref-26-3 xref-ref-26-4 xref-ref-26-5 xref-ref-26-6 xref-ref-26-7 xref-ref-26-8"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.26" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-26"><string-name name-style="western" hwp:sortable="Ingraham J."><surname>Ingraham</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Garg V. K."><surname>Garg</surname>, <given-names>V. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barzilay R."><surname>Barzilay</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Jaakkola T. S."><surname>Jaakkola</surname>, <given-names>T. S.</given-names></string-name> <article-title hwp:id="article-title-25">Generative models for graph-based protein design</article-title>. In <person-group person-group-type="editor" hwp:id="person-group-2"><string-name name-style="western" hwp:sortable="Wallach H. M."><surname>Wallach</surname>, <given-names>H. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Larochelle H."><surname>Larochelle</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Beygelzimer A."><surname>Beygelzimer</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="d’Alché-Buc F."><surname>d’Alché-Buc</surname>, <given-names>F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fox E. B."><surname>Fox</surname>, <given-names>E. B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Garnett R."><surname>Garnett</surname>, <given-names>R.</given-names></string-name></person-group> (eds.), <source hwp:id="source-26">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</source>, pp. <fpage>15794</fpage>–<lpage>15805</lpage>, <year>2019</year>.</citation></ref><ref id="c27" hwp:id="ref-27" hwp:rev-id="xref-ref-27-1 xref-ref-27-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.27" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-27"><string-name name-style="western" hwp:sortable="Jankauskaité J."><surname>Jankauskaité</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jiménez-García B."><surname>Jiménez-García</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dapkünas J."><surname>Dapkünas</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fernández-Recio J."><surname>Fernández-Recio</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Moal I. H."><surname>Moal</surname>, <given-names>I. H.</given-names></string-name> <article-title hwp:id="article-title-26">Skempi 2.0: an updated benchmark of changes in protein–protein binding energy, kinetics and thermodynamics upon mutation</article-title>. <source hwp:id="source-27">Bioinformatics</source>, <volume>35</volume>(<issue>3</issue>):<fpage>462</fpage>–<lpage>469</lpage>, <year>2019</year>.</citation></ref><ref id="c28" hwp:id="ref-28" hwp:rev-id="xref-ref-28-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.28" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-28"><string-name name-style="western" hwp:sortable="Jin W."><surname>Jin</surname>, <given-names>W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wohlwend J."><surname>Wohlwend</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Barzilay R."><surname>Barzilay</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Jaakkola T."><surname>Jaakkola</surname>, <given-names>T.</given-names></string-name> <article-title hwp:id="article-title-27">Iterative refinement graph neural network for anti-body sequence-structure co-design</article-title>. <source hwp:id="source-28">arXiv preprint</source> <pub-id pub-id-type="arxiv">2110.04624</pub-id>, <year>2021</year>.</citation></ref><ref id="c29" hwp:id="ref-29" hwp:rev-id="xref-ref-29-1 xref-ref-29-2 xref-ref-29-3"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.29" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-29"><string-name name-style="western" hwp:sortable="Jing B."><surname>Jing</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eismann S."><surname>Eismann</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Soni P. N."><surname>Soni</surname>, <given-names>P. N.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dror R. O."><surname>Dror</surname>, <given-names>R. O.</given-names></string-name> <article-title hwp:id="article-title-28">Equivariant graph neural networks for 3d macromolecular structure</article-title>. <source hwp:id="source-29">Proceedings of the International Conference on Machine Learning</source>, <year>2021a</year>.</citation></ref><ref id="c30" hwp:id="ref-30" hwp:rev-id="xref-ref-30-1 xref-ref-30-2 xref-ref-30-3 xref-ref-30-4 xref-ref-30-5 xref-ref-30-6 xref-ref-30-7 xref-ref-30-8 xref-ref-30-9"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.30" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-30"><string-name name-style="western" hwp:sortable="Jing B."><surname>Jing</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eismann S."><surname>Eismann</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Suriana P."><surname>Suriana</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Townshend R. J. L."><surname>Townshend</surname>, <given-names>R. J. L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dror R. O."><surname>Dror</surname>, <given-names>R. O.</given-names></string-name> <article-title hwp:id="article-title-29">Learning from protein structure with geometric vector perceptrons</article-title>. <source hwp:id="source-30">In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net</source>, <year>2021b</year>.</citation></ref><ref id="c31" hwp:id="ref-31" hwp:rev-id="xref-ref-31-1 xref-ref-31-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.31" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-31"><string-name name-style="western" hwp:sortable="Joshi M."><surname>Joshi</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen D."><surname>Chen</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu Y."><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Weld D. S."><surname>Weld</surname>, <given-names>D. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zettlemoyer L."><surname>Zettlemoyer</surname>, <given-names>L.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Levy O."><surname>Levy</surname>, <given-names>O.</given-names></string-name> <article-title hwp:id="article-title-30">Spanbert: Improving pre-training by representing and predicting spans</article-title>. <source hwp:id="source-31">Transactions of the Association for Computational Linguistics</source>, <volume>8</volume>:<fpage>64</fpage>–<lpage>77</lpage>, <year>2020</year>.</citation></ref><ref id="c32" hwp:id="ref-32" hwp:rev-id="xref-ref-32-1 xref-ref-32-2 xref-ref-32-3 xref-ref-32-4 xref-ref-32-5"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.32" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-32"><string-name name-style="western" hwp:sortable="Jumper J."><surname>Jumper</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Evans R."><surname>Evans</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pritzel A."><surname>Pritzel</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Green T."><surname>Green</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Figurnov M."><surname>Figurnov</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ronneberger O."><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tunyasuvunakool K."><surname>Tunyasuvunakool</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bates R."><surname>Bates</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Žídek A."><surname>Žídek</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Potapenko A."><surname>Potapenko</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-31">Highly accurate protein structure prediction with alphafold</article-title>. <source hwp:id="source-32">Nature</source>, <volume>596</volume>(<issue>7873</issue>):<fpage>583</fpage>–<lpage>589</lpage>, <year>2021</year>.</citation></ref><ref id="c33" hwp:id="ref-33" hwp:rev-id="xref-ref-33-1 xref-ref-33-2 xref-ref-33-3"><citation publication-type="website" citation-type="web" ref:id="2022.04.10.487779v1.33" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-33"><string-name name-style="western" hwp:sortable="Kim S."><surname>Kim</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="van Kempen M."><surname>van Kempen</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Söding J."><surname>Söding</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Steinegger M."><surname>Steinegger</surname>, <given-names>M.</given-names></string-name> <source hwp:id="source-33">foldseek</source>. <ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/steineggerlab/foldseek" ext-link-type="uri" xlink:href="https://github.com/steineggerlab/foldseek" hwp:id="ext-link-5">https://github.com/steineggerlab/foldseek</ext-link>, <year>2021</year>.</citation></ref><ref id="c34" hwp:id="ref-34" hwp:rev-id="xref-ref-34-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.34" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-34"><string-name name-style="western" hwp:sortable="Kunzmann P."><surname>Kunzmann</surname>, <given-names>P.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Hamacher K."><surname>Hamacher</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-32">Biotite: a unifying open source computational biology framework in python</article-title>. <source hwp:id="source-34">BMC bioinformatics</source>, <volume>19</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>8</lpage>, <year>2018</year>.</citation></ref><ref id="c35" hwp:id="ref-35" hwp:rev-id="xref-ref-35-1 xref-ref-35-2 xref-ref-35-3"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.35" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-35"><string-name name-style="western" hwp:sortable="Lan J."><surname>Lan</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ge J."><surname>Ge</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yu J."><surname>Yu</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shan S."><surname>Shan</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhou H."><surname>Zhou</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fan S."><surname>Fan</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang Q."><surname>Zhang</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shi X."><surname>Shi</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang Q."><surname>Wang</surname>, <given-names>Q.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang L."><surname>Zhang</surname>, <given-names>L.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-33">Structure of the sars-cov-2 spike receptor-binding domain bound to the ace2 receptor</article-title>. <source hwp:id="source-35">Nature</source>, <volume>581</volume>(<issue>7807</issue>):<fpage>215</fpage>–<lpage>220</lpage>, <year>2020</year>.</citation></ref><ref id="c36" hwp:id="ref-36" hwp:rev-id="xref-ref-36-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.36" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-36"><string-name name-style="western" hwp:sortable="Langan R. A."><surname>Langan</surname>, <given-names>R. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Boyken S. E."><surname>Boyken</surname>, <given-names>S. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ng A. H."><surname>Ng</surname>, <given-names>A. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Samson J. A."><surname>Samson</surname>, <given-names>J. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dods G."><surname>Dods</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Westbrook A. M."><surname>Westbrook</surname>, <given-names>A. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nguyen T. H."><surname>Nguyen</surname>, <given-names>T. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lajoie M. J."><surname>Lajoie</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen Z."><surname>Chen</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Berger S."><surname>Berger</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-34">De novo design of bioactive protein switches</article-title>. <source hwp:id="source-36">Nature</source>, <volume>572</volume>(<issue>7768</issue>):<fpage>205</fpage>–<lpage>210</lpage>, <year>2019</year>.</citation></ref><ref id="c37" hwp:id="ref-37" hwp:rev-id="xref-ref-37-1 xref-ref-37-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.37" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-37"><string-name name-style="western" hwp:sortable="Lees J."><surname>Lees</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yeats C."><surname>Yeats</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perkins J."><surname>Perkins</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sillitoe I."><surname>Sillitoe</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rentzsch R."><surname>Rentzsch</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dessailly B. H."><surname>Dessailly</surname>, <given-names>B. H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Orengo C."><surname>Orengo</surname>, <given-names>C.</given-names></string-name> <article-title hwp:id="article-title-35">Gene3d: a domain-based resource for comparative genomics, functional annotation and protein network analysis</article-title>. <source hwp:id="source-37">Nucleic acids research</source>, <volume>40</volume> (<issue>D1</issue>):<fpage>D465</fpage>–<lpage>D471</lpage>, <year>2012</year>.</citation></ref><ref id="c38" hwp:id="ref-38" hwp:rev-id="xref-ref-38-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.38" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-38"><string-name name-style="western" hwp:sortable="Li B."><surname>Li</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yang Y. T."><surname>Yang</surname>, <given-names>Y. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Capra J. A."><surname>Capra</surname>, <given-names>J. A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Gerstein M. B."><surname>Gerstein</surname>, <given-names>M. B.</given-names></string-name> <article-title hwp:id="article-title-36">Predicting changes in protein thermodynamic stability upon point mutation with deep 3d convolutional neural networks</article-title>. <source hwp:id="source-38">PLoS computational biology</source>, <volume>16</volume>(<issue>11</issue>):<fpage>e1008291</fpage>, <year>2020</year>.</citation></ref><ref id="c39" hwp:id="ref-39" hwp:rev-id="xref-ref-39-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.39" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-39"><string-name name-style="western" hwp:sortable="Li Z."><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yang Y."><surname>Yang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Faraggi E."><surname>Faraggi</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhan J."><surname>Zhan</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Zhou Y."><surname>Zhou</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-37">Direct prediction of profiles of sequences compatible with a protein structure by neural networks with fragment-based local and energy-based nonlocal profiles</article-title>. <source hwp:id="source-39">Proteins: Structure, Function, and Bioinformatics</source>, <volume>82</volume>(<issue>10</issue>):<fpage>2565</fpage>–<lpage>2573</lpage>, <year>2014</year>.</citation></ref><ref id="c40" hwp:id="ref-40" hwp:rev-id="xref-ref-40-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.40" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-40"><string-name name-style="western" hwp:sortable="Madani A."><surname>Madani</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McCann B."><surname>McCann</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Naik N."><surname>Naik</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Keskar N. S."><surname>Keskar</surname>, <given-names>N. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anand N."><surname>Anand</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eguchi R. R."><surname>Eguchi</surname>, <given-names>R. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang P.-S."><surname>Huang</surname>, <given-names>P.-S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Socher R."><surname>Socher</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-38">Progen: Language modeling for protein generation</article-title>. <source hwp:id="source-40">arXiv preprint</source> <pub-id pub-id-type="arxiv">2004.03497</pub-id>, <year>2020</year>.</citation></ref><ref id="c41" hwp:id="ref-41" hwp:rev-id="xref-ref-41-1 xref-ref-41-2"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.41" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-41"><string-name name-style="western" hwp:sortable="Madani A."><surname>Madani</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Krause B."><surname>Krause</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Greene E. R."><surname>Greene</surname>, <given-names>E. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Subramanian S."><surname>Subramanian</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mohr B. P."><surname>Mohr</surname>, <given-names>B. P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Holton J. M."><surname>Holton</surname>, <given-names>J. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Olmos J. L."><surname>Olmos</surname>, <given-names>J. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Xiong C."><surname>Xiong</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sun Z. Z."><surname>Sun</surname>, <given-names>Z. Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Socher R."><surname>Socher</surname>, <given-names>R.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-39">Deep neural language modeling enables functional protein generation across families</article-title>. <source hwp:id="source-41">bioRxiv</source>, <year>2021</year>.</citation></ref><ref id="c42" hwp:id="ref-42" hwp:rev-id="xref-ref-42-1 xref-ref-42-2 xref-ref-42-3 xref-ref-42-4 xref-ref-42-5"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.42" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-42"><string-name name-style="western" hwp:sortable="Meier J."><surname>Meier</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Rao R."><surname>Rao</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Verkuil R."><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu J."><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sercu T."><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Rives A."><surname>Rives</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-40">Language models enable zero-shot prediction of the effects of mutations on protein function</article-title>. <source hwp:id="source-42">Advances in Neural Information Processing Systems</source>, <volume>34</volume>, <year>2021</year>.</citation></ref><ref id="c43" hwp:id="ref-43" hwp:rev-id="xref-ref-43-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.43" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-43"><string-name name-style="western" hwp:sortable="Mirdita M."><surname>Mirdita</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="von den Driesch L."><surname>von den Driesch</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Galiez C."><surname>Galiez</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Martin M. J."><surname>Martin</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Söding J."><surname>Söding</surname>, <given-names>J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Steinegger M."><surname>Steinegger</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-41">Uniclust databases of clustered and deeply annotated protein sequences and alignments</article-title>. <source hwp:id="source-43">Nucleic acids research</source>, <volume>45</volume>(<issue>D1</issue>):<fpage>D170</fpage>–<lpage>D176</lpage>, <year>2017</year>.</citation></ref><ref id="c44" hwp:id="ref-44" hwp:rev-id="xref-ref-44-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.44" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-44"><string-name name-style="western" hwp:sortable="Norn C."><surname>Norn</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wicky B. I."><surname>Wicky</surname>, <given-names>B. I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Juergens D."><surname>Juergens</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu S."><surname>Liu</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kim D."><surname>Kim</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tischer D."><surname>Tischer</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Koepnick B."><surname>Koepnick</surname>, <given-names>B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anishchenko I."><surname>Anishchenko</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Baker D."><surname>Baker</surname>, <given-names>D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Ovchinnikov S."><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name> <article-title hwp:id="article-title-42">Protein sequence design by conformational landscape optimization</article-title>. <source hwp:id="source-44">Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>11</issue>), <year>2021</year>.</citation></ref><ref id="c45" hwp:id="ref-45" hwp:rev-id="xref-ref-45-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.45" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-45"><string-name name-style="western" hwp:sortable="O’Connell J."><surname>O’Connell</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Li Z."><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hanson J."><surname>Hanson</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Heffernan R."><surname>Heffernan</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lyons J."><surname>Lyons</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Paliwal K."><surname>Paliwal</surname>, <given-names>K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dehzangi A."><surname>Dehzangi</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yang Y."><surname>Yang</surname>, <given-names>Y.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Zhou Y."><surname>Zhou</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-43">Spin2: Predicting sequence profiles from protein structures using deep neural networks</article-title>. <source hwp:id="source-45">Proteins: Structure, Function, and Bioinformatics</source>, <volume>86</volume>(<issue>6</issue>):<fpage>629</fpage>–<lpage>633</lpage>, <year>2018</year>.</citation></ref><ref id="c46" hwp:id="ref-46" hwp:rev-id="xref-ref-46-1 xref-ref-46-2 xref-ref-46-3"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.46" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-46"><string-name name-style="western" hwp:sortable="Orengo C. A."><surname>Orengo</surname>, <given-names>C. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Michie A. D."><surname>Michie</surname>, <given-names>A. D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones S."><surname>Jones</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones D. T."><surname>Jones</surname>, <given-names>D. T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Swindells M. B."><surname>Swindells</surname>, <given-names>M. B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Thornton J. M."><surname>Thornton</surname>, <given-names>J. M.</given-names></string-name> <article-title hwp:id="article-title-44">Cath–a hierarchic classification of protein domain structures</article-title>. <source hwp:id="source-46">Structure</source>, <volume>5</volume>(<issue>8</issue>):<fpage>1093</fpage>–<lpage>1109</lpage>, <year>1997</year>.</citation></ref><ref id="c47" hwp:id="ref-47" hwp:rev-id="xref-ref-47-1 xref-ref-47-2"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.47" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-47"><string-name name-style="western" hwp:sortable="Ott M."><surname>Ott</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Edunov S."><surname>Edunov</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Baevski A."><surname>Baevski</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Fan A."><surname>Fan</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gross S."><surname>Gross</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ng N."><surname>Ng</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Grangier D."><surname>Grangier</surname>, <given-names>D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Auli M."><surname>Auli</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-45">fairseq: A fast, extensible toolkit for sequence modeling</article-title>. <source hwp:id="source-47">arXiv preprint</source> <pub-id pub-id-type="arxiv">1904.01038</pub-id>, <year>2019</year>.</citation></ref><ref id="c48" hwp:id="ref-48" hwp:rev-id="xref-ref-48-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.48" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-48"><string-name name-style="western" hwp:sortable="Potter S. C."><surname>Potter</surname>, <given-names>S. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Luciani A."><surname>Luciani</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eddy S. R."><surname>Eddy</surname>, <given-names>S. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park Y."><surname>Park</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lopez R."><surname>Lopez</surname>, <given-names>R.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Finn R. D."><surname>Finn</surname>, <given-names>R. D.</given-names></string-name> <article-title hwp:id="article-title-46">Hmmer web server: 2018 update</article-title>. <source hwp:id="source-48">Nucleic acids research</source>, <volume>46</volume>(<issue>W1</issue>):<fpage>W200</fpage>–<lpage>W204</lpage>, <year>2018</year>.</citation></ref><ref id="c49" hwp:id="ref-49" hwp:rev-id="xref-ref-49-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.49" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-49"><string-name name-style="western" hwp:sortable="Qi Y."><surname>Qi</surname>, <given-names>Y.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Zhang J. Z."><surname>Zhang</surname>, <given-names>J. Z.</given-names></string-name> <article-title hwp:id="article-title-47">Densecpd: improving the accuracy of neural-network-based computational protein sequence design with densenet</article-title>. <source hwp:id="source-49">Journal of Chemical Information and Modeling</source>, <volume>60</volume>(<issue>3</issue>):<fpage>1245</fpage>–<lpage>1252</lpage>, <year>2020</year>.</citation></ref><ref id="c50" hwp:id="ref-50" hwp:rev-id="xref-ref-50-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.50" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-50"><string-name name-style="western" hwp:sortable="Quijano-Rubio A."><surname>Quijano-Rubio</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Yeh H.-W."><surname>Yeh</surname>, <given-names>H.-W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park J."><surname>Park</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee H."><surname>Lee</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Langan R. A."><surname>Langan</surname>, <given-names>R. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Boyken S. E."><surname>Boyken</surname>, <given-names>S. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lajoie M. J."><surname>Lajoie</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cao L."><surname>Cao</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chow C. M."><surname>Chow</surname>, <given-names>C. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Miranda M. C."><surname>Miranda</surname>, <given-names>M. C.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-48">De novo design of modular and tunable protein biosensors</article-title>. <source hwp:id="source-50">Nature</source>, <volume>591</volume>(<issue>7850</issue>):<fpage>482</fpage>–<lpage>487</lpage>, <year>2021</year>.</citation></ref><ref id="c51" hwp:id="ref-51" hwp:rev-id="xref-ref-51-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.51" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-51"><string-name name-style="western" hwp:sortable="Rao R."><surname>Rao</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bhattacharya N."><surname>Bhattacharya</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Thomas N."><surname>Thomas</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Duan Y."><surname>Duan</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen P."><surname>Chen</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Canny J."><surname>Canny</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abbeel P."><surname>Abbeel</surname>, <given-names>P.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Song Y."><surname>Song</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-49">Evaluating protein transfer learning with tape</article-title>. <source hwp:id="source-51">Advances in neural information processing systems</source>, <volume>32</volume>, <year>2019</year>.</citation></ref><ref id="c52" hwp:id="ref-52" hwp:rev-id="xref-ref-52-1 xref-ref-52-2"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.52" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-52"><string-name name-style="western" hwp:sortable="Rao R."><surname>Rao</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu J."><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Verkuil R."><surname>Verkuil</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meier J."><surname>Meier</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Canny J. F."><surname>Canny</surname>, <given-names>J. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Abbeel P."><surname>Abbeel</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sercu T."><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Rives A."><surname>Rives</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-50">Msa transformer</article-title>. <source hwp:id="source-52">bioRxiv</source>, <year>2021</year>.</citation></ref><ref id="c53" hwp:id="ref-53" hwp:rev-id="xref-ref-53-1 xref-ref-53-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.53" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-53"><string-name name-style="western" hwp:sortable="Rives A."><surname>Rives</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meier J."><surname>Meier</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sercu T."><surname>Sercu</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goyal S."><surname>Goyal</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lin Z."><surname>Lin</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu J."><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Guo D."><surname>Guo</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ott M."><surname>Ott</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zitnick C. L."><surname>Zitnick</surname>, <given-names>C. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ma J."><surname>Ma</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-51">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source hwp:id="source-53">Proceedings of the National Academy of Sciences</source>, <volume>118</volume>(<issue>15</issue>), <year>2021</year>.</citation></ref><ref id="c54" hwp:id="ref-54" hwp:rev-id="xref-ref-54-1 xref-ref-54-2 xref-ref-54-3 xref-ref-54-4"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.54" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-54"><string-name name-style="western" hwp:sortable="Rocklin G. J."><surname>Rocklin</surname>, <given-names>G. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chidyausiku T. M."><surname>Chidyausiku</surname>, <given-names>T. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Goreshnik I."><surname>Goreshnik</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ford A."><surname>Ford</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Houliston S."><surname>Houliston</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lemak A."><surname>Lemak</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Carter L."><surname>Carter</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ravichandran R."><surname>Ravichandran</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mulligan V. K."><surname>Mulligan</surname>, <given-names>V. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chevalier A."><surname>Chevalier</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-52">Global analysis of protein folding using massively parallel design, synthesis, and testing</article-title>. <source hwp:id="source-54">Science</source>, <volume>357</volume>(<issue>6347</issue>):<fpage>168</fpage>–<lpage>175</lpage>, <year>2017</year>.</citation></ref><ref id="c55" hwp:id="ref-55" hwp:rev-id="xref-ref-55-1 xref-ref-55-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.55" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-55"><string-name name-style="western" hwp:sortable="Senior A. W."><surname>Senior</surname>, <given-names>A. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Evans R."><surname>Evans</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jumper J."><surname>Jumper</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kirkpatrick J."><surname>Kirkpatrick</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sifre L."><surname>Sifre</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Green T."><surname>Green</surname>, <given-names>T.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Qin C."><surname>Qin</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Žídek A."><surname>Žídek</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Nelson A. W."><surname>Nelson</surname>, <given-names>A. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bridgland A."><surname>Bridgland</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-53">Improved protein structure prediction using potentials from deep learning</article-title>. <source hwp:id="source-55">Nature</source>, <volume>577</volume>(<issue>7792</issue>): <fpage>706</fpage>–<lpage>710</lpage>, <year>2020</year>.</citation></ref><ref id="c56" hwp:id="ref-56" hwp:rev-id="xref-ref-56-1 xref-ref-56-2"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.56" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-56"><string-name name-style="western" hwp:sortable="Sennrich R."><surname>Sennrich</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haddow B."><surname>Haddow</surname>, <given-names>B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Birch A."><surname>Birch</surname>, <given-names>A.</given-names></string-name> <article-title hwp:id="article-title-54">Improving neural machine translation models with monolingual data</article-title>. <source hwp:id="source-56">arXiv preprint</source> <pub-id pub-id-type="arxiv">1511.06709</pub-id>, <year>2015</year>.</citation></ref><ref id="c57" hwp:id="ref-57" hwp:rev-id="xref-ref-57-1 xref-ref-57-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.57" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-57"><string-name name-style="western" hwp:sortable="Shin J.-E."><surname>Shin</surname>, <given-names>J.-E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Riesselman A. J."><surname>Riesselman</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kollasch A. W."><surname>Kollasch</surname>, <given-names>A. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McMahon C."><surname>McMahon</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Simon E."><surname>Simon</surname>, <given-names>E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Sander C."><surname>Sander</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Manglik A."><surname>Manglik</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kruse A. C."><surname>Kruse</surname>, <given-names>A. C.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Marks D. S."><surname>Marks</surname>, <given-names>D. S.</given-names></string-name> <article-title hwp:id="article-title-55">Protein design and variant prediction using autoregressive generative models</article-title>. <source hwp:id="source-57">Nature communications</source>, <volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>, <year>2021</year>.</citation></ref><ref id="c58" hwp:id="ref-58" hwp:rev-id="xref-ref-58-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.58" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-58"><string-name name-style="western" hwp:sortable="Shroff R."><surname>Shroff</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cole A. W."><surname>Cole</surname>, <given-names>A. W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Diaz D. J."><surname>Diaz</surname>, <given-names>D. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Morrow B. R."><surname>Morrow</surname>, <given-names>B. R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Donnell I."><surname>Donnell</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Annapareddy A."><surname>Annapareddy</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gollihar J."><surname>Gollihar</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ellington A. D."><surname>Ellington</surname>, <given-names>A. D.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Thyer R."><surname>Thyer</surname>, <given-names>R.</given-names></string-name> <article-title hwp:id="article-title-56">Discovery of novel gain-of-function mutations guided by structure-based deep learning</article-title>. <source hwp:id="source-58">ACS synthetic biology</source>, <volume>9</volume>(<issue>11</issue>):<fpage>2927</fpage>–<lpage>2935</lpage>, <year>2020</year>.</citation></ref><ref id="c59" hwp:id="ref-59" hwp:rev-id="xref-ref-59-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.59" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-59"><string-name name-style="western" hwp:sortable="Sinai S."><surname>Sinai</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang R."><surname>Wang</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Whatley A."><surname>Whatley</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Slocum S."><surname>Slocum</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Locane E."><surname>Locane</surname>, <given-names>E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kelsic E. D."><surname>Kelsic</surname>, <given-names>E. D.</given-names></string-name> <article-title hwp:id="article-title-57">Adalead: A simple and robust adaptive greedy search algorithm for sequence design</article-title>. <source hwp:id="source-59">arXiv preprint</source> <pub-id pub-id-type="arxiv">2010.02141</pub-id>, <year>2020</year>.</citation></ref><ref id="c60" hwp:id="ref-60" hwp:rev-id="xref-ref-60-1 xref-ref-60-2 xref-ref-60-3 xref-ref-60-4 xref-ref-60-5"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.60" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-60"><string-name name-style="western" hwp:sortable="Starr T. N."><surname>Starr</surname>, <given-names>T. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Greaney A. J."><surname>Greaney</surname>, <given-names>A. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Hilton S. K."><surname>Hilton</surname>, <given-names>S. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ellis D."><surname>Ellis</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Crawford K. H."><surname>Crawford</surname>, <given-names>K. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dingens A. S."><surname>Dingens</surname>, <given-names>A. S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Navarro M. J."><surname>Navarro</surname>, <given-names>M. J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Bowen J. E."><surname>Bowen</surname>, <given-names>J. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tortorici M. A."><surname>Tortorici</surname>, <given-names>M. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Walls A. C."><surname>Walls</surname>, <given-names>A. C.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-58">Deep mutational scanning of sars-cov-2 receptor binding domain reveals constraints on folding and ace2 binding</article-title>. <source hwp:id="source-60">Cell</source>, <volume>182</volume>(<issue>5</issue>): <fpage>1295</fpage>–<lpage>1310</lpage>, <year>2020</year>.</citation></ref><ref id="c61" hwp:id="ref-61" hwp:rev-id="xref-ref-61-1 xref-ref-61-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.61" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-61"><string-name name-style="western" hwp:sortable="Steinegger M."><surname>Steinegger</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Meier M."><surname>Meier</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Mirdita M."><surname>Mirdita</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vöhringer H."><surname>Vöhringer</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Haunsberger S. J."><surname>Haunsberger</surname>, <given-names>S. J.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Söding J."><surname>Söding</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-59">Hh-suite3 for fast remote homology detection and deep protein annotation</article-title>. <source hwp:id="source-61">BMC bioinformatics</source>, <volume>20</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>, <year>2019</year>.</citation></ref><ref id="c62" hwp:id="ref-62" hwp:rev-id="xref-ref-62-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.62" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-62"><string-name name-style="western" hwp:sortable="Street A. G."><surname>Street</surname>, <given-names>A. G.</given-names></string-name> and <string-name name-style="western" hwp:sortable="Mayo S. L."><surname>Mayo</surname>, <given-names>S. L.</given-names></string-name> <article-title hwp:id="article-title-60">Computational protein design</article-title>. <source hwp:id="source-62">Structure</source>, <volume>7</volume>(<issue>5</issue>):<fpage>R105</fpage>–<lpage>R109</lpage>, <year>1999</year>.</citation></ref><ref id="c63" hwp:id="ref-63" hwp:rev-id="xref-ref-63-1 xref-ref-63-2 xref-ref-63-3 xref-ref-63-4"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.63" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-63"><string-name name-style="western" hwp:sortable="Strokach A."><surname>Strokach</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Becerra D."><surname>Becerra</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Corbi-Verge C."><surname>Corbi-Verge</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Perez-Riba A."><surname>Perez-Riba</surname>, <given-names>A.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Kim P. M."><surname>Kim</surname>, <given-names>P. M.</given-names></string-name> <article-title hwp:id="article-title-61">Fast and flexible protein design using deep graph neural networks</article-title>. <source hwp:id="source-63">Cell Systems</source>, <volume>11</volume>(<issue>4</issue>):<fpage>402</fpage>–<lpage>411</lpage>, <year>2020</year>.</citation></ref><ref id="c64" hwp:id="ref-64" hwp:rev-id="xref-ref-64-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.64" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-64"><string-name name-style="western" hwp:sortable="Suzek B. E."><surname>Suzek</surname>, <given-names>B. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang Y."><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Huang H."><surname>Huang</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McGarvey P. B."><surname>McGarvey</surname>, <given-names>P. B.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu C. H."><surname>Wu</surname>, <given-names>C. H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Consortium U."><surname>Consortium</surname>, <given-names>U.</given-names></string-name> <article-title hwp:id="article-title-62">Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>. <source hwp:id="source-64">Bioinformatics</source>, <volume>31</volume>(<issue>6</issue>):<fpage>926</fpage>–<lpage>932</lpage>, <year>2015</year>.</citation></ref><ref id="c65" hwp:id="ref-65" hwp:rev-id="xref-ref-65-1 xref-ref-65-2 xref-ref-65-3 xref-ref-65-4"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.65" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-65"><string-name name-style="western" hwp:sortable="Townshend R. J. L."><surname>Townshend</surname>, <given-names>R. J. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Vögele M."><surname>Vögele</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Suriana P."><surname>Suriana</surname>, <given-names>P.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Derry A."><surname>Derry</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Powers A."><surname>Powers</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Laloudakis Y."><surname>Laloudakis</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Balachandar S."><surname>Balachandar</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anderson B. M."><surname>Anderson</surname>, <given-names>B. M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Eismann S."><surname>Eismann</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kondor R."><surname>Kondor</surname>, <given-names>R.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Altman R. B."><surname>Altman</surname>, <given-names>R. B.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Dror R. O."><surname>Dror</surname>, <given-names>R. O.</given-names></string-name> <article-title hwp:id="article-title-63">ATOM3D: tasks on molecules in three dimensions</article-title>. <source hwp:id="source-65">CoRR, abs/2012.04035</source>, <year>2020</year>.</citation></ref><ref id="c66" hwp:id="ref-66" hwp:rev-id="xref-ref-66-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.66" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-66"><string-name name-style="western" hwp:sortable="Trinquier J."><surname>Trinquier</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uguzzoni G."><surname>Uguzzoni</surname>, <given-names>G.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Pagnani A."><surname>Pagnani</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zamponi F."><surname>Zamponi</surname>, <given-names>F.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Weigt M."><surname>Weigt</surname>, <given-names>M.</given-names></string-name> <article-title hwp:id="article-title-64">Efficient generative modeling of protein sequences using simple autoregressive models</article-title>. <source hwp:id="source-66">arXiv preprint</source> <pub-id pub-id-type="arxiv">2103.03292</pub-id>, <year>2021</year>.</citation></ref><ref id="c67" hwp:id="ref-67" hwp:rev-id="xref-ref-67-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.67" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-67"><string-name name-style="western" hwp:sortable="Turc I."><surname>Turc</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chang M.-W."><surname>Chang</surname>, <given-names>M.-W.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lee K."><surname>Lee</surname>, <given-names>K.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Toutanova K."><surname>Toutanova</surname>, <given-names>K.</given-names></string-name> <article-title hwp:id="article-title-65">Well-read students learn better: On the importance of pre-training compact models</article-title>. <source hwp:id="source-67">arXiv preprint</source> <pub-id pub-id-type="arxiv">1908.08962</pub-id>, <year>2019</year>.</citation></ref><ref id="c68" hwp:id="ref-68" hwp:rev-id="xref-ref-68-1 xref-ref-68-2"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.68" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-68"><string-name name-style="western" hwp:sortable="Vaswani A."><surname>Vaswani</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Shazeer N."><surname>Shazeer</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Parmar N."><surname>Parmar</surname>, <given-names>N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Uszkoreit J."><surname>Uszkoreit</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Jones L."><surname>Jones</surname>, <given-names>L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Gomez A. N."><surname>Gomez</surname>, <given-names>A. N.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Kaiser Ł."><surname>Kaiser</surname>, <given-names>Ł.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Polosukhin I."><surname>Polosukhin</surname>, <given-names>I.</given-names></string-name> <article-title hwp:id="article-title-66">Attention is all you need</article-title>. <source hwp:id="source-68">In Advances in neural information processing systems</source>, pp. <fpage>5998</fpage>–<lpage>6008</lpage>, <year>2017</year>.</citation></ref><ref id="c69" hwp:id="ref-69"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.69" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-69"><string-name name-style="western" hwp:sortable="Walls A. C."><surname>Walls</surname>, <given-names>A. C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park Y.-J."><surname>Park</surname>, <given-names>Y.-J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tortorici M. A."><surname>Tortorici</surname>, <given-names>M. A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wall A."><surname>Wall</surname>, <given-names>A.</given-names></string-name>, <string-name name-style="western" hwp:sortable="McGuire A. T."><surname>McGuire</surname>, <given-names>A. T.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Veesler D."><surname>Veesler</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-67">Structure, function, and antigenicity of the sars-cov-2 spike glycoprotein</article-title>. <source hwp:id="source-69">Cell</source>, <volume>181</volume>(<issue>2</issue>):<fpage>281</fpage>–<lpage>292</lpage>, <year>2020</year>.</citation></ref><ref id="c70" hwp:id="ref-70" hwp:rev-id="xref-ref-70-1 xref-ref-70-2"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.70" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-70"><string-name name-style="western" hwp:sortable="Wang J."><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Cao H."><surname>Cao</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Zhang J. Z."><surname>Zhang</surname>, <given-names>J. Z.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Qi Y."><surname>Qi</surname>, <given-names>Y.</given-names></string-name> <article-title hwp:id="article-title-68">Computational protein design with deep learning neural networks</article-title>. <source hwp:id="source-70">Scientific reports</source>, <volume>8</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>9</lpage>, <year>2018</year>.</citation></ref><ref id="c71" hwp:id="ref-71" hwp:rev-id="xref-ref-71-1"><citation publication-type="other" citation-type="journal" ref:id="2022.04.10.487779v1.71" ref:linkable="no" ref:use-reference-as-is="yes" hwp:id="citation-71"><string-name name-style="western" hwp:sortable="Wang J."><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lisanza S."><surname>Lisanza</surname>, <given-names>S.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Juergens D."><surname>Juergens</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Tischer D."><surname>Tischer</surname>, <given-names>D.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anishchenko I."><surname>Anishchenko</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Baek M."><surname>Baek</surname>, <given-names>M.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Watson J. L."><surname>Watson</surname>, <given-names>J. L.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chun J. H."><surname>Chun</surname>, <given-names>J. H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Milles L. F."><surname>Milles</surname>, <given-names>L. F.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Dauparas J."><surname>Dauparas</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal> <article-title hwp:id="article-title-69">Deep learning methods for designing proteins scaffolding functional sites</article-title>. <source hwp:id="source-71">bioRxiv</source>, <year>2021</year>.</citation></ref><ref id="c72" hwp:id="ref-72" hwp:rev-id="xref-ref-72-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.72" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-72"><string-name name-style="western" hwp:sortable="Wu Z."><surname>Wu</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Johnston K. E."><surname>Johnston</surname>, <given-names>K. E.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Arnold F. H."><surname>Arnold</surname>, <given-names>F. H.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Yang K. K."><surname>Yang</surname>, <given-names>K. K.</given-names></string-name> <article-title hwp:id="article-title-70">Protein sequence design with deep generative models</article-title>. <source hwp:id="source-72">Current Opinion in Chemical Biology</source>, <volume>65</volume>:<fpage>18</fpage>–<lpage>27</lpage>, <year>2021</year>.</citation></ref><ref id="c73" hwp:id="ref-73" hwp:rev-id="xref-ref-73-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.73" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-73"><string-name name-style="western" hwp:sortable="Yang J."><surname>Yang</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Anishchenko I."><surname>Anishchenko</surname>, <given-names>I.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Park H."><surname>Park</surname>, <given-names>H.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Peng Z."><surname>Peng</surname>, <given-names>Z.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Ovchinnikov S."><surname>Ovchinnikov</surname>, <given-names>S.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Baker D."><surname>Baker</surname>, <given-names>D.</given-names></string-name> <article-title hwp:id="article-title-71">Improved protein structure prediction using predicted interresidue orientations</article-title>. <source hwp:id="source-73">Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>3</issue>):<fpage>1496</fpage>–<lpage>1503</lpage>, <year>2020</year>.</citation></ref><ref id="c74" hwp:id="ref-74" hwp:rev-id="xref-ref-74-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.74" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-74"><string-name name-style="western" hwp:sortable="Yang K. K."><surname>Yang</surname>, <given-names>K. K.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu Z."><surname>Wu</surname>, <given-names>Z.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Arnold F. H."><surname>Arnold</surname>, <given-names>F. H.</given-names></string-name> <article-title hwp:id="article-title-72">Machine-learning-guided directed evolution for protein engineering</article-title>. <source hwp:id="source-74">Nature methods</source>, <volume>16</volume>(<issue>8</issue>):<fpage>687</fpage>–<lpage>694</lpage>, <year>2019</year>.</citation></ref><ref id="c75" hwp:id="ref-75" hwp:rev-id="xref-ref-75-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.75" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-75"><string-name name-style="western" hwp:sortable="Zhang Y."><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Chen Y."><surname>Chen</surname>, <given-names>Y.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wang C."><surname>Wang</surname>, <given-names>C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Lo C.-C."><surname>Lo</surname>, <given-names>C.-C.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Liu X."><surname>Liu</surname>, <given-names>X.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Wu W."><surname>Wu</surname>, <given-names>W.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Zhang J."><surname>Zhang</surname>, <given-names>J.</given-names></string-name> <article-title hwp:id="article-title-73">Prodconn: Protein design using a convolutional neural network</article-title>. <source hwp:id="source-75">Proteins: Structure, Function, and Bioinformatics</source>, <volume>88</volume>(<issue>7</issue>):<fpage>819</fpage>–<lpage>829</lpage>, <year>2020</year>.</citation></ref><ref id="c76" hwp:id="ref-76" hwp:rev-id="xref-ref-76-1"><citation publication-type="journal" citation-type="journal" ref:id="2022.04.10.487779v1.76" ref:linkable="yes" ref:use-reference-as-is="yes" hwp:id="citation-76"><string-name name-style="western" hwp:sortable="Zhou J."><surname>Zhou</surname>, <given-names>J.</given-names></string-name>, <string-name name-style="western" hwp:sortable="Panaitiu A. E."><surname>Panaitiu</surname>, <given-names>A. E.</given-names></string-name>, and <string-name name-style="western" hwp:sortable="Grigoryan G."><surname>Grigoryan</surname>, <given-names>G.</given-names></string-name> <article-title hwp:id="article-title-74">A general-purpose protein design framework based on mining sequence–structure relationships in known protein structures</article-title>. <source hwp:id="source-76">Proceedings of the National Academy of Sciences</source>, <volume>117</volume>(<issue>2</issue>):<fpage>1059</fpage>–<lpage>1068</lpage>, <year>2020</year>.</citation></ref></ref-list><sec id="s6" hwp:id="sec-29"><label>A.</label><title hwp:id="title-33">Additional details on datasets, training procedures, and model architectures</title><sec id="s6a" hwp:id="sec-30" hwp:rev-id="xref-sec-30-1 xref-sec-30-2"><label>A.1.</label><title hwp:id="title-34">Details on dataset of predicted structures</title><p hwp:id="p-67">We used training data from two sources: 1) experimental protein structures from the CATH 40% non-redundant chain set, and 2) AlphaFold2-predicted structures from UniRef50 sequences. To evaluate the generalization performance across different protein folds, we split the train, validation, and test data based on the CATH hierarchical classification of protein structures (<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-2" hwp:rel-id="ref-46">Orengo et al., 1997</xref>) for both data sources. To achieve that a rigorous structural hold-out, we additionally use foldseek (<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-2" hwp:rel-id="ref-33">Kim et al., 2021</xref>) for pairwise TMalign between the test set the train set.</p><sec id="s6a1" hwp:id="sec-31"><title hwp:id="title-35">CATH topology split</title><p hwp:id="p-68">Following the structural split methodology in previous work (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-6" hwp:rel-id="ref-26">Ingraham et al., 2019</xref>; <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-8" hwp:rel-id="ref-30">Jing et al., 2021b</xref>; <xref rid="c63" ref-type="bibr" hwp:id="xref-ref-63-3" hwp:rel-id="ref-63">Strokach et al., 2020</xref>), we randomly split the CATH v4.3 (latest version) topology classification codes into train, validation, and test sets at a 80/10/10 ratio. The CATH (<xref rid="c46" ref-type="bibr" hwp:id="xref-ref-46-3" hwp:rel-id="ref-46">Orengo et al., 1997</xref>) structural hierarchy, classifies domains in four levels: Class (C), Architecture (A), Topology/fold (T), and Homologous superfamily (H). The topology/fold (T) level roughly corresponds to the SCOP fold classification.</p></sec><sec id="s6a2" hwp:id="sec-32"><title hwp:id="title-36">Experimental structures</title><p hwp:id="p-69">We collected full chains up to length 500 for all domains in the CATH v4.3 40% sequence identity non-redundant set. The experimental structure data contained only stand-alone chains and no multichain complexes. As each chain may be classified with more than one topology codes, we further removed chains with topology codes spanning different splits, so that there is no overlap in topology codes between train, validation, and test. This results in 16,153 chains in the train split, 1457 chains in the validation split, and 1797 chains in the test split.</p></sec><sec id="s6a3" hwp:id="sec-33"><title hwp:id="title-37">Predicted structure</title><p hwp:id="p-70">We curated a new data set of AlphaFold2 (<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-3" hwp:rel-id="ref-32">Jumper et al., 2021</xref>)-predicted structures for a selective subset of UniRef50 (202001) sequences. To prevent information leakage about the test set from the predicted structures, we proceeded in the following steps.</p><p hwp:id="p-71">First, we annotated UniRef50 sequences with CATH classification according to the Gene3D (<xref rid="c37" ref-type="bibr" hwp:id="xref-ref-37-2" hwp:rel-id="ref-37">Lees et al., 2012</xref>) database, also used by Strokach (<xref rid="c63" ref-type="bibr" hwp:id="xref-ref-63-4" hwp:rel-id="ref-63">Strokach et al., 2020</xref>) for data curation. Gene3D represents each CATH classification code as a library of representative profile HMMs. We searched all HMMs associated with the validation and test splits against the UniRef50 sequences using default parameters in hmmsearch (<xref rid="c48" ref-type="bibr" hwp:id="xref-ref-48-1" hwp:rel-id="ref-48">Potter et al., 2018</xref>) and excluded all hits.</p><p hwp:id="p-72">Additionally, as AlphaFold2 predictions use multiple sequence alignments (MSAs) as inputs, we also took precaution to avoid information leakage from sequences in the MSAs. We created a filtered version of UniRef100 by searching all the validation-split and test-split Gene3D HMMs against UniRef100 (202001) and excluding all hits. Then, we constructed our MSAs using hhblits (<xref rid="c61" ref-type="bibr" hwp:id="xref-ref-61-1" hwp:rel-id="ref-61">Steinegger et al., 2019</xref>) on this filtered version of UniRef100.</p><p hwp:id="p-73">As AlphaFold2 predictions are computationally costly, our budget only allowed for predicting structures for a subset of the UniRef50 sequences. We ranked UniRef50 sequences based on the distogram lDDT score (Supplementary Equation 6 in (<xref rid="c55" ref-type="bibr" hwp:id="xref-ref-55-2" hwp:rel-id="ref-55">Senior et al., 2020</xref>)), based on distogram predictions from MSATransformer (<xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-4" hwp:rel-id="ref-42">Rao et al., 2021</xref>), as a proxy for the quality of predicted structures. In this order, using AlphaFold2 Model 1 on the filtered UniRef100 MSAs described above, we obtained predicted structures for the top 12 million UniRef50 sequences under length 500, roughly 750 times the CATH train set size.</p><p hwp:id="p-74">We used the publicly released model weights from AlphaFold2 Model 1 for CASP14 as a single model, as opposed the 5-model ensemble in (<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-4" hwp:rel-id="ref-32">Jumper et al., 2021</xref>), to cover more sequences with the same amount of computing resources. We curated the input MSAs from UniRef100 with hhblits, with an additional filtering step as described above. To reduce computational costs, compared to the standard AlphaFold2 protocol, we did not include the UniRef90 jackhmmer MSAs, or the MGnify and BFD metagenomics MSAs, nor the pdb70 templates. Other than a reduced inputs, we followed the default settings in AlphaFold2 open source code, using 3 recycling iterations and the default Amber relaxation protocol. Despite the reduced inputs, the resulting 12 million predicted structures still have high pLDDT scores from AlphaFold, with 75% of residues having pLDDT above 90 (highly confident).</p><p hwp:id="p-75">We found that increasing the predicted data size to up to 1 million structures (75 times the CATH experimental data size) substantially improves model performance. Beyond 1 million structures, models still benefit from more data but with diminished marginal returns (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-5" hwp:rel-id="F6">Figure 6a</xref>).</p></sec><sec id="s6a4" hwp:id="sec-34"><title hwp:id="title-38">Noise on AlphaFold2-predicted backbone coordinates</title><p hwp:id="p-76">Even after Amber relaxation, the backbone coordinates predicted by AlphaFold2 contain artifacts in the sub-Angstrom scale that may give away amino acid identities. Without adding noise on predicted structures, there is a substantial gap between held-out set performance on predicted structures and on experimental structures. To prevent the model from learning non-generalizable AlphaFold2-specific rules, we added Gaussian noise at the 0.1A scale on predicted backbone coordinates. The Gaussian noise improves the invariant Transformer performance but not the GVP-GNN performance (<xref rid="figC1" ref-type="fig" hwp:id="xref-fig-10-2" hwp:rel-id="F10">Supplementary Figure C.1</xref>).</p><table-wrap id="tblA1" orientation="portrait" position="float" hwp:id="T4" hwp:rev-id="xref-table-wrap-4-1 xref-table-wrap-4-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLA1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T4</object-id><object-id pub-id-type="publisher-id">tblA1</object-id><label>Table A.1.</label><caption hwp:id="caption-11"><p hwp:id="p-77">Details on model hyperparameters and training.</p></caption><graphic xlink:href="487779v1_tblA1" position="float" orientation="portrait" hwp:id="graphic-12"/></table-wrap></sec></sec><sec id="s6b" hwp:id="sec-35" hwp:rev-id="xref-sec-35-1"><label>A.2.</label><title hwp:id="title-39">Details on span masking</title><p hwp:id="p-78">We add a binary feature indicating whether each coordinate is masked or not. In GVP-Transformer, we exclude the masked nodes in the GVP-GNN encoder layers, and then impute zeros when passing the GVP-GNN outputs into the main Transformer. Imputing zeros for missing vector features ensure the rotation-and translation-invariance of the model. In GVP-GNN, we impute zeros for the input vector features, and in the input graph connect the masked nodes to the <italic toggle="yes">k</italic> sequence nearest-neighbors (<italic toggle="yes">k</italic> = 30) in lieu of the <italic toggle="yes">k</italic> nearest nodes by spatial distance.</p><p hwp:id="p-79">For span masking, we randomly select continuous spans of up to 30 amino acids until 15% of input backbone coordinates are masked. Such a span masking scheme has shown to improve performance on natural language processing benchmarks (<xref rid="c31" ref-type="bibr" hwp:id="xref-ref-31-2" hwp:rel-id="ref-31">Joshi et al., 2020</xref>). The span lengths are sampled from a geometric distribution Geo(<italic toggle="yes">p</italic>) where <italic toggle="yes">p</italic> = 0.05 (corresponding to an average span length of 1<italic toggle="yes">/p</italic> = 20). The starting points for the spans are uniformly randomly sampled. Compared to independent random masking, span masking is better for GVP-Transformer but not for GVP-GNN (<xref rid="tblC1" ref-type="table" hwp:id="xref-table-wrap-6-4" hwp:rel-id="T6">Table C.1</xref>).</p><p hwp:id="p-80">For the amino acids with masked coordinates, we exclude the corresponding nodes from the input graph to the pre-processing GVP message passing layers, and then impute zeros for the geometric features when passing the GVP outputs into the main Transformer. Imputing zeros for missing vector features ensure the rotation-and translation-invariance of the model.</p><fig id="figB1" position="float" fig-type="figure" orientation="portrait" hwp:id="F8" hwp:rev-id="xref-fig-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGB1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F8</object-id><object-id pub-id-type="publisher-id">figB1</object-id><label>Figure B.1.</label><caption hwp:id="caption-12"><p hwp:id="p-81">An illustrative example of structural overlap between CATH topology splits. The jack bean canavalin (PDB code 1DGW; chain Y; red) and the soybean <italic toggle="yes">β</italic>-Conglycinin (PDB code 1UIJ; chain B; blue) are assigned different topology codes in CATH (1.10.10 and 2.60.120), but they align with TM-score 0.94 and CA RMSD 0.7A on a segment of 90 residues. The difference in topology classifications likely resulted from CATH annotating only a 37-residue mainly helical segment of the jack bean canavalin as a domain while annotating a longer 176-residue mainly beta sheet segment of the soybean <italic toggle="yes">β</italic>-Conglycinin as a domain.</p></caption><graphic xlink:href="487779v1_figB1" position="float" orientation="portrait" hwp:id="graphic-13"/></fig></sec><sec id="s6c" hwp:id="sec-36" hwp:rev-id="xref-sec-36-1 xref-sec-36-2"><label>A.3.</label><title hwp:id="title-40">Details on model architectures</title><sec id="s6c1" hwp:id="sec-37"><title hwp:id="title-41">Invariance to rotation and translation</title><p hwp:id="p-82">The input features for both GVP-GNN and GVP-Transformer are translation-invariant, making the overall models also invariant to translations.</p><p hwp:id="p-83">Each GVP-GNN layer is rotation-equivariant, that is, for a vector feature <italic toggle="yes">x</italic> and any arbitrary rotation <italic toggle="yes">T, Tf</italic> (<italic toggle="yes">x</italic>) = <italic toggle="yes">f</italic> (<italic toggle="yes">Tx</italic>). With equivariant intermediate layers and an invariant output projection layer, GVP-GNN is overall invariant to rotations, since the composition of an equivariant function <italic toggle="yes">f</italic> with an invariant function <italic toggle="yes">g</italic> produces an invariant function <italic toggle="yes">g</italic>(<italic toggle="yes">f</italic> (<italic toggle="yes">x</italic>)).</p><p hwp:id="p-84">The GVP-Transformer architecture is also invariant to rotations and translations. The initial GVP-GNN layers in GVP-Transformer output rotation-invariant scalar features and rotation-equivariant vector features for each amino acid. To make the overall GVP-Transformer invariant, we perform a change of basis on GVP-GNN vector outputs to produce rotation-invariant features for the Transformer. More specifically, for each amino acid, we define a local reference frame based on the N, CA, and C atom positions in the amino acid, following Algorithm 21 in AlphaFold2 (<xref rid="c32" ref-type="bibr" hwp:id="xref-ref-32-5" hwp:rel-id="ref-32">Jumper et al., 2021</xref>). We then perform a change of basis according to this local reference frame, rotating the vector features in GVP-GNN outputs into the local reference frames of each amino acid. We concatenate this rotated “local version” of vector features together with the scalar features as inputs to the Transformer. The concatenated features are invariant to both translations and rotations on the input backbone coordinates, forming a <italic toggle="yes">L × E</italic> matrix where <italic toggle="yes">L</italic> is the number of amino acids in the protein backbone and <italic toggle="yes">E</italic> is the feature dimension. For amino acids with masked or missing coordinates, the features are imputed as zeros.</p></sec><sec id="s6c2" hwp:id="sec-38"><title hwp:id="title-42">Transformer</title><p hwp:id="p-85">We closely followed the original autoregressive encoder-decoder Transformer architecture (<xref rid="c68" ref-type="bibr" hwp:id="xref-ref-68-2" hwp:rel-id="ref-68">Vaswani et al., 2017</xref>) except for using learned positional embeddings instead of sinusoidal positional embeddings, attention dropout, and layer normalization inside the residual blocks (“pre-layernorm”). For model scaling experiments, we followed the model sizes in (<xref rid="c67" ref-type="bibr" hwp:id="xref-ref-67-1" hwp:rel-id="ref-67">Turc et al., 2019</xref>), and chose the 142-million-parameter model with 8 encoder layers, 8 decoder layers, 8 attention heads, and embedding dimension 512 based on the best validation set performance (<xref rid="fig6" ref-type="fig" hwp:id="xref-fig-6-6" hwp:rel-id="F6">Figure 6c</xref> shows test set ablation).</p><p hwp:id="p-86">The GVP-GNN, GVP-GNN-large, and GVP-Transformer models used in the evaluations in this manuscript are all trained to convergence, with detailed hyperparameters listed in <xref rid="tblA1" ref-type="table" hwp:id="xref-table-wrap-4-2" hwp:rel-id="T4">Table A.1</xref>.</p></sec></sec></sec><sec id="s7" hwp:id="sec-39" hwp:rev-id="xref-sec-39-1"><label>B.</label><title hwp:id="title-43">TM-score-based test set</title><p hwp:id="p-87">In addition to the CATH topology-based test set following previous work (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-7" hwp:rel-id="ref-26">Ingraham et al., 2019</xref>; <xref rid="c30" ref-type="bibr" hwp:id="xref-ref-30-9" hwp:rel-id="ref-30">Jing et al., 2021b</xref>), we also create an even more stringent test set based on pairwise TM-score comparison between train and test examples. The CATH topology split does not completely prevent high TM-score matches between train and test structures. We illustrate such an example in <xref rid="figB1" ref-type="fig" hwp:id="xref-fig-8-1" hwp:rel-id="F8">Figure B.1</xref>, and show overall TM-score statistics <xref rid="figB2" ref-type="fig" hwp:id="xref-fig-9-1" hwp:rel-id="F9">Figure B.2</xref>.</p><fig id="figB2" position="float" fig-type="figure" orientation="portrait" hwp:id="F9" hwp:rev-id="xref-fig-9-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGB2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F9</object-id><object-id pub-id-type="publisher-id">figB2</object-id><label>Figure B.2.</label><caption hwp:id="caption-13"><p hwp:id="p-88">Distribution of the highest TM-score from each test example to the train set. For example, 54% of the CATH topology split test set has at least one match in the train set with TM-score above 0.5, and 27% of the topology split test set has at least one match in the train set with TM-score above 0.6.</p></caption><graphic xlink:href="487779v1_figB2" position="float" orientation="portrait" hwp:id="graphic-14"/></fig><table-wrap id="tblB1" orientation="portrait" position="float" hwp:id="T5" hwp:rev-id="xref-table-wrap-5-1 xref-table-wrap-5-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLB1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T5</object-id><object-id pub-id-type="publisher-id">tblB1</object-id><label>Table B.1.</label><caption hwp:id="caption-14"><p hwp:id="p-89">Fixed backbone sequence design performance on the more stringent structurally held-out test set from CATH v4.3 chains (and its short and single-chain subsets) in terms of per-residue perplexity (lower is better) and recovery (higher is better).</p></caption><graphic xlink:href="487779v1_tblB1" position="float" orientation="portrait" hwp:id="graphic-15"/></table-wrap><p hwp:id="p-90">We constructed a TM-score-based test set of 223 proteins with no TMalign matches (TM-score <italic toggle="yes">≥</italic> 0.5) from the train set, using the foldseek (<xref rid="c33" ref-type="bibr" hwp:id="xref-ref-33-3" hwp:rel-id="ref-33">Kim et al., 2021</xref>) TMalign tool with default parameters for the pairwise search.</p><p hwp:id="p-91">We found that the conclusions about model performance overall remains the same on this TM-score-based test set as on the CATH topology split test set. For consistency with prior work, we report metrics on the CATH topology test set in the main manuscript, while showing metrics on the smaller TM-score-based test set in <xref rid="tblB1" ref-type="table" hwp:id="xref-table-wrap-5-2" hwp:rel-id="T5">Table B.1</xref>.</p></sec><sec id="s8" hwp:id="sec-40" hwp:rev-id="xref-sec-40-1 xref-sec-40-2 xref-sec-40-3 xref-sec-40-4"><label>C.</label><title hwp:id="title-44">Additional results and details</title><sec id="s8a" hwp:id="sec-41"><title hwp:id="title-45">Ablation on noise and masking during training</title><p hwp:id="p-92">We found that GVP-Transformer models trained with Gaussian noise during training perform slightly better at test time than those trained without (<xref rid="tblC1" ref-type="table" hwp:id="xref-table-wrap-6-5" hwp:rel-id="T6">Table C.1</xref>). When given full backbone coordinates at test time, training with span masking only very slightly improves model performance compared to no masking or to random masking, even though there is a much larger performance gap between random masking and span masking on regions with masked backbone coordinates (<xref rid="fig4" ref-type="fig" hwp:id="xref-fig-4-4" hwp:rel-id="F4">Figure 4</xref>).</p><sec id="s8a1" hwp:id="sec-42"><title hwp:id="title-46">Dual-state design test set from PDBFlex</title><p hwp:id="p-93">We test design performance on multiple conformations by finding test split proteins with distinct conformations in the PDBFlex database. From PDBFlex, we looks for experimental structures of protein sequences in the CATH topology split test set (95% sequence identity or above), and take all paired instances that are at least 5 angstroms apart in overall RMSD between conformations. We report perplexity on locally flexible residues (defined as local RMSD above 1 angstrom). To be more conservative in our evaluation, we show the better of the two conformations to represent single-state perplexity in <xref rid="fig7" ref-type="fig" hwp:id="xref-fig-7-2" hwp:rel-id="F7">Figure 7</xref>.</p></sec><sec id="s8a2" hwp:id="sec-43"><title hwp:id="title-47">Ablation on the number of GVP-GNN encoder layers in GVP-Transformer</title><p hwp:id="p-94">Increasing the number of GVP-GNN encoder layers improves the overall model performance (<xref rid="figC1" ref-type="fig" hwp:id="xref-fig-10-3" hwp:rel-id="F10">Figure C.1</xref>), indicating that the geometric reasoning capability in GVP-GNN is complementary to the Transformer layers.</p><table-wrap id="tblC1" orientation="portrait" position="float" hwp:id="T6" hwp:rev-id="xref-table-wrap-6-1 xref-table-wrap-6-2 xref-table-wrap-6-3 xref-table-wrap-6-4 xref-table-wrap-6-5"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLC1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T6</object-id><object-id pub-id-type="publisher-id">tblC1</object-id><label>Table C.1.</label><caption hwp:id="caption-15"><p hwp:id="p-95">Effects of adding Gaussian noise to predicted structures and effects of span masking during training, as measured by perplexity on CATH topology split test set.</p></caption><graphic xlink:href="487779v1_tblC1" position="float" orientation="portrait" hwp:id="graphic-16"/></table-wrap><fig id="figC1" position="float" fig-type="figure" orientation="portrait" hwp:id="F10" hwp:rev-id="xref-fig-10-1 xref-fig-10-2 xref-fig-10-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGC1</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F10</object-id><object-id pub-id-type="publisher-id">figC1</object-id><label>Figure C.1.</label><caption hwp:id="caption-16"><p hwp:id="p-96">Effects of varying the number of GVP-GNN pre-processing layers in the GVP-Transformer model, as measured by perplexity on CATH topology split test set.</p></caption><graphic xlink:href="487779v1_figC1" position="float" orientation="portrait" hwp:id="graphic-17"/></fig></sec><sec id="s8a3" hwp:id="sec-44"><title hwp:id="title-48">Stability prediction on de novo small proteins</title><p hwp:id="p-97">We predict protein stability on an experimentally measured stability dataset for <italic toggle="yes">de novo</italic> small proteins (<xref rid="c54" ref-type="bibr" hwp:id="xref-ref-54-2" hwp:rel-id="ref-54">Rocklin et al., 2017</xref>). We use the relative difference in sequence conditional log-likelihoods as a predictor for stability and compute Pearson correlation with the mutation effect following (<xref rid="c26" ref-type="bibr" hwp:id="xref-ref-26-8" hwp:rel-id="ref-26">Ingraham et al., 2019</xref>), assuming that more stable sequences should score higher in log-likelihoods. For each fold, <xref rid="c54" ref-type="bibr" hwp:id="xref-ref-54-3" hwp:rel-id="ref-54">Rocklin et al. (2017)</xref> starts with a reference protein and generates sequence variants with single amino acid substitutions. We calculate the Pearson correlation between sequence conditional log-likelihood scores and experimental stability measurements for all designed sequences in each fold. With predicted structures as additional training data, the GVP-Transformer model improves the pearson correlation on 8 out of the 10 folds.</p></sec><sec id="s8a4" hwp:id="sec-45"><title hwp:id="title-49">Perplexity and sequence recovery of SARS-CoV-2 RBD</title><p hwp:id="p-98">We show perplexity and sequence recovery on the SARS-CoV-2 protein receptor binding domain (RBD) as an example for inverse folding. The RBD can exist in a closed-state with the RBD down or in an open-state with the RBD up (<xref rid="c60" ref-type="bibr" hwp:id="xref-ref-60-3" hwp:rel-id="ref-60">Walls et al., 2020</xref>), as illustrated in <xref rid="figC3" ref-type="fig" hwp:id="xref-fig-12-2" hwp:rel-id="F12">Figure C.3</xref>. The SARS-Cov-2 spike protein structure has no match with the training data with TM-score above 0.5. The SARS-Cov-2 spike protein has both an open and closed state (open state: PDB 6XRA; closed state: PDB 6VXX). We evaluate perplexity and sequence recovery conditioning on each of the two states independently and jointly. Conditioning on the open state results in better perplexity and sequence recovery than conditioning on the closed state. Conditioning on both states gives improvement in both perplexity and sequence recovery compared to conditioning only on the open state.</p><table-wrap id="tblC2" orientation="portrait" position="float" hwp:id="T7" hwp:rev-id="xref-table-wrap-7-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLC2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T7</object-id><object-id pub-id-type="publisher-id">tblC2</object-id><label>Table C.2.</label><caption hwp:id="caption-17"><p hwp:id="p-99">Mutation stability prediction performance for small <italic toggle="yes">de novo</italic> proteins (<xref rid="c54" ref-type="bibr" hwp:id="xref-ref-54-4" hwp:rel-id="ref-54">Rocklin et al., 2017</xref>), with highest correlation bolded.</p></caption><graphic xlink:href="487779v1_tblC2" position="float" orientation="portrait" hwp:id="graphic-18"/></table-wrap><fig id="figC2" position="float" fig-type="figure" orientation="portrait" hwp:id="F11" hwp:rev-id="xref-fig-11-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGC2</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F11</object-id><object-id pub-id-type="publisher-id">figC2</object-id><label>Figure C.2.</label><caption hwp:id="caption-18"><p hwp:id="p-100">Fixed backbone sequence design perplexity for protein complexes. The model is evaluated on 796 structurally held-out protein complexes. Comparison of conditioning on the backbone coordinates of individual chains (x-axis) with conditioning on backbone coordinates of the entire complex (y-axis). Note that for both values perplexity is evaluated on the same chain in the complex. The shift to the lower right indicates improved perplexity when the model is given the complete structure of the complex.</p></caption><graphic xlink:href="487779v1_figC2" position="float" orientation="portrait" hwp:id="graphic-19"/></fig><fig id="figC3" position="float" fig-type="figure" orientation="portrait" hwp:id="F12" hwp:rev-id="xref-fig-12-1 xref-fig-12-2 xref-fig-12-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGC3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F12</object-id><object-id pub-id-type="publisher-id">figC3</object-id><label>Figure C.3.</label><caption hwp:id="caption-19"><p hwp:id="p-101">Illustration of the closed and open states of the SARS-CoV-2 spike protein receptor-binding domain. Cryo-EM structures from (<xref rid="c60" ref-type="bibr" hwp:id="xref-ref-60-4" hwp:rel-id="ref-60">Walls et al., 2020</xref>) (open state: PDB 6XRA; closed state: PDB 6VXX).</p></caption><graphic xlink:href="487779v1_figC3" position="float" orientation="portrait" hwp:id="graphic-20"/></fig></sec><sec id="s8a5" hwp:id="sec-46"><title hwp:id="title-50">Predicting RBD-ACE2 binding affinity</title><p hwp:id="p-102">We used the binding affinity dataset provided by <xref rid="c60" ref-type="bibr" hwp:id="xref-ref-60-5" hwp:rel-id="ref-60">Starr et al. (2020)</xref> (<ext-link l:rel="related" l:ref-type="uri" l:ref="https://github.com/jbloomlab/SARS-CoV-2-RBD_DMS" ext-link-type="uri" xlink:href="https://github.com/jbloomlab/SARS-CoV-2-RBD_DMS" hwp:id="ext-link-6">https://github.com/jbloomlab/SARS-CoV-2-RBD_DMS</ext-link>), restricting to sites within the RBM subsequence. We used the RBD-ACE2 structure determined by <xref rid="c35" ref-type="bibr" hwp:id="xref-ref-35-3" hwp:rel-id="ref-35">Lan et al. (2020)</xref> (PDB: 6M0J). For mutational effect predictions with ESM-1v, ESM-1b, and ESM-MSA-1b, we scored mutations using the masked-marginal likelihood ratio between the mutant and wildtype amino acids. To generate the MSA used as input to ESM-MSA-1b, we searched uniclust30_2017_07 (<xref rid="c43" ref-type="bibr" hwp:id="xref-ref-43-1" hwp:rel-id="ref-43">Mirdita et al., 2017</xref>) with hhblits (<xref rid="c61" ref-type="bibr" hwp:id="xref-ref-61-2" hwp:rel-id="ref-61">Steinegger et al., 2019</xref>) (using two iterations and an E-value cutoff of 0.001) based on the RBD wildtype sequence as the query.</p></sec><sec id="s8a6" hwp:id="sec-47"><title hwp:id="title-51">Predicting complex stability changes upon mutations</title><p hwp:id="p-103">SKEMPI (<xref rid="c27" ref-type="bibr" hwp:id="xref-ref-27-2" hwp:rel-id="ref-27">Jankauskaitė et al., 2019</xref>) is a database of binding free energy changes upon single point mutations within protein complex interfaces. This database is used as a task in the Atom3D benchmark suite (<xref rid="c65" ref-type="bibr" hwp:id="xref-ref-65-2" hwp:rel-id="ref-65">Townshend et al., 2020</xref>) for comparing supervised stability prediction methods. The task is to classify whether the stability of the complex increases as a result of the mutation. We compare zero-shot predictions using inverse folding models to supervised and transfer learning methods (<xref rid="c65" ref-type="bibr" hwp:id="xref-ref-65-3" hwp:rel-id="ref-65">Townshend et al., 2020</xref>; <xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-2" hwp:rel-id="ref-29">Jing et al., 2021a</xref>) on the Atom3D test set. We find that sequence log-likelihoods from GVP-GNN, GVP-GNN-large, and GVP-Transformer models are all effective zero-shot predictors of stability changes of protein complexes (<xref rid="tblC4" ref-type="table" hwp:id="xref-table-wrap-9-3" hwp:rel-id="T9">Table C.4</xref>), performing comparably to the best supervised method which uses transfer learning.</p><table-wrap id="tblC3" orientation="portrait" position="float" hwp:id="T8" hwp:rev-id="xref-table-wrap-8-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLC3</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T8</object-id><object-id pub-id-type="publisher-id">tblC3</object-id><label>Table C.3.</label><caption hwp:id="caption-20"><p hwp:id="p-104">Perplexity and sequence recovery on the SARS-Cov-2 spike protein receptor binding domain (RBD), conditioned on either the closed state, the open state, or both states (illustrated in <xref rid="figC3" ref-type="fig" hwp:id="xref-fig-12-3" hwp:rel-id="F12">Figure C.3</xref>). The inputs to inverse folding models consist of the backbone coordinates for the entire spike protein, while the perplexity evaluation is only on the RBD.</p></caption><graphic xlink:href="487779v1_tblC3" position="float" orientation="portrait" hwp:id="graphic-21"/></table-wrap><table-wrap id="tblC4" orientation="portrait" position="float" hwp:id="T9" hwp:rev-id="xref-table-wrap-9-1 xref-table-wrap-9-2 xref-table-wrap-9-3"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLC4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T9</object-id><object-id pub-id-type="publisher-id">tblC4</object-id><label>Table C.4.</label><caption hwp:id="caption-21"><p hwp:id="p-105">Protein complex stability on SKEMPI test set (binary classification of increase in stability on single-point mutations). Although only trained on single chains, the inverse-folding models generalize to protein complexes. Giving the full complex as input, <italic toggle="yes">complex</italic>, improves performance compared to giving only the chain as input, <italic toggle="yes">chain</italic>. Zero-shot prediction compared to fully supervised and supervised transfer learning methods from (<xref rid="c65" ref-type="bibr" hwp:id="xref-ref-65-4" hwp:rel-id="ref-65">Townshend et al., 2020</xref>) and (<xref rid="c29" ref-type="bibr" hwp:id="xref-ref-29-3" hwp:rel-id="ref-29">Jing et al., 2021a</xref>) trained on the SKEMPI train set.</p></caption><graphic xlink:href="487779v1_tblC4" position="float" orientation="portrait" hwp:id="graphic-22"/></table-wrap><table-wrap id="tblC5" orientation="portrait" position="float" hwp:id="T10" hwp:rev-id="xref-table-wrap-10-1 xref-table-wrap-10-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLC5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T10</object-id><object-id pub-id-type="publisher-id">tblC5</object-id><label>Table C.5.</label><caption hwp:id="caption-22"><p hwp:id="p-106">Zero-shot performance on AAV split (<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-2" hwp:rel-id="ref-15">Dallago et al., 2021</xref>).</p></caption><graphic xlink:href="487779v1_tblC5" position="float" orientation="portrait" hwp:id="graphic-23"/></table-wrap></sec><sec id="s8a7" hwp:id="sec-48"><title hwp:id="title-52">Predicting insertion effects on AAV</title><p hwp:id="p-107">Using masked coordinate tokens at insertion regions, inverse folding models can also predict the effects of sequence insertions. Adeno-associated virus (AAV) capsids are a promising gene delivery vehicle, approved by the US Food and Drug Administration for use as gene delivery vectors in humans. Focusing on mutating a 28-amino acid segment, <xref rid="c12" ref-type="bibr" hwp:id="xref-ref-12-3" hwp:rel-id="ref-12">Bryant et al. (2021</xref>) generated more than 200,000 variants of AAV sequences with 12–29 mutations across this region, and measured their ability to package of a DNA payload. This dataset is unique compared to many other mutagenesis datasets in that most sequences feature random insertions in the 28-amino acid segment, as opposed to only random substitutions.</p><p hwp:id="p-108">We use inverse folding models to predict insertion and substitution effects as follows: For each sequence, we input the full backbone coordinates of the wild-type (PDB: 1LP3), and insert one masked token into the input backbone coordinates for each insertion. Then we compare the conditional sequence log-likelihood on this input with masks to the conditional sequence log-likelihood of the wild-type sequence on the wild-type backbone. The difference in these two conditional log-likelihoods are used as the score for predicting packaging ability.</p><p hwp:id="p-109">We report the zero-shot performance on each of the 7 data subsets evaluated in the FLIP (<xref rid="c15" ref-type="bibr" hwp:id="xref-ref-15-3" hwp:rel-id="ref-15">Dallago et al., 2021</xref>) benchmark suite. As shown in <xref rid="tblC5" ref-type="table" hwp:id="xref-table-wrap-10-2" hwp:rel-id="T10">Table C.5</xref>, GVP-Transformer trained with predicted structures outperforms the sequence-only zero-shot prediction baseline ESM-1v on 6 out of the 7 data subsets. For ESM-1v, we scored variant sequences based on the independent marginals formula, as described in <xref ref-type="disp-formula" rid="eqn1" hwp:id="xref-disp-formula-1-1" hwp:rel-id="disp-formula-1">Equation 1</xref> from <xref rid="c42" ref-type="bibr" hwp:id="xref-ref-42-5" hwp:rel-id="ref-42">Meier et al. (2021)</xref>.</p></sec><sec id="s8a8" hwp:id="sec-49"><title hwp:id="title-53">Confusion matrix</title><p hwp:id="p-110">We calculated the substitution scores between native sequences and sampled sequences (sampled with temperature <italic toggle="yes">T</italic> = 1) by using the same log odds ratio formula as in the BLOSUM62 substition matrix. For two amino acids <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, the substitution score <italic toggle="yes">s</italic>(<italic toggle="yes">x, y</italic>) is
<disp-formula id="eqn2" hwp:id="disp-formula-2">
<alternatives hwp:id="alternatives-2"><graphic xlink:href="487779v1_eqn2.gif" position="float" orientation="portrait" hwp:id="graphic-24"/></alternatives>
</disp-formula>
where <italic toggle="yes">p</italic>(<italic toggle="yes">x, y</italic>) is the jointly likelihood that native amino acid <italic toggle="yes">x</italic> is substituted by sampled amino acid <italic toggle="yes">y, q</italic>(<italic toggle="yes">x</italic>) is the marginal likelihood in the native distribution, and <italic toggle="yes">q</italic>(<italic toggle="yes">y</italic>) is the marginal likelihood in the sampled distribution.</p></sec><sec id="s8a10" hwp:id="sec-50"><title hwp:id="title-54">Calibration</title><p hwp:id="p-111">Calibration curves examines how well the probabilistic predictions of a classifier are calibrated, plotting the true frequency of the label against its predicted probability. When computing the calibration curve, for each amino acid, we bin the predicted probabilities into 10 bins and then compare with the true probability.</p></sec><sec id="s8a11" hwp:id="sec-51"><title hwp:id="title-55">Placement of hydrophobic residues</title><p hwp:id="p-112">We define the amino acids IVLFCMA as hydrophobic residues, and inspect the distribution of solvent accessible surface area for both hydrophobic residues and polar (non-hydrophobic) residues. Solvent accessible surface area calculated with the Shrake-Rupley (“rolling probe”) algorithm from the biotite package (<xref rid="c34" ref-type="bibr" hwp:id="xref-ref-34-1" hwp:rel-id="ref-34">Kunzmann &amp; Hamacher, 2018</xref>) and summed over all atoms in each amino acid. All models have similar distributions of accessible surface area for hydrophobic residues, also similar to the distribution in native sequences (<xref rid="figC6" ref-type="fig" hwp:id="xref-fig-15-2" hwp:rel-id="F15">Figure C.6</xref>).</p></sec><sec id="s8a12" hwp:id="sec-52"><title hwp:id="title-56">Sampling speed</title><p hwp:id="p-113">We profile the sampling speed with PyTorch Profiler, averaging over the sampling time for 30 sequences in each sequence length bucket on a Quadro RTX 8000 GPU with 48GB memory. For the generic Transformer decoder, we use the incremental causal decoding implementation in fairseq (<xref rid="c47" ref-type="bibr" hwp:id="xref-ref-47-1" hwp:rel-id="ref-47">Ott et al., 2019</xref>). For GVP-GNN, we use the implementation from the gvp-pytorch GitHub repository.</p><fig id="figC4" position="float" fig-type="figure" orientation="portrait" hwp:id="F13" hwp:rev-id="xref-fig-13-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGC4</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F13</object-id><object-id pub-id-type="publisher-id">figC4</object-id><label>Figure C.4.</label><caption hwp:id="caption-23"><p hwp:id="p-114">Confusion matrix between native sequence and sampled sequences from the model, compared to BLOSUM62 as reference.</p></caption><graphic xlink:href="487779v1_figC4" position="float" orientation="portrait" hwp:id="graphic-25"/></fig><fig id="figC5" position="float" fig-type="figure" orientation="portrait" hwp:id="F14" hwp:rev-id="xref-fig-14-1"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGC5</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F14</object-id><object-id pub-id-type="publisher-id">figC5</object-id><label>Figure C.5.</label><caption hwp:id="caption-24"><p hwp:id="p-115">Calibration.</p></caption><graphic xlink:href="487779v1_figC5" position="float" orientation="portrait" hwp:id="graphic-26"/></fig><fig id="figC6" position="float" fig-type="figure" orientation="portrait" hwp:id="F15" hwp:rev-id="xref-fig-15-1 xref-fig-15-2"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/FIGC6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">F15</object-id><object-id pub-id-type="publisher-id">figC6</object-id><label>Figure C.6.</label><caption hwp:id="caption-25"><p hwp:id="p-116">The majority of hydrophobic residues are buried, following a long tail accessible surface area distribution as in native sequences.</p></caption><graphic xlink:href="487779v1_figC6" position="float" orientation="portrait" hwp:id="graphic-27"/></fig><table-wrap id="tblC6" orientation="portrait" position="float" hwp:id="T11"><object-id pub-id-type="other" hwp:sub-type="pisa">biorxiv;2022.04.10.487779v1/TBLC6</object-id><object-id pub-id-type="other" hwp:sub-type="slug">T11</object-id><object-id pub-id-type="publisher-id">tblC6</object-id><label>Table C.6.</label><caption hwp:id="caption-26"><p hwp:id="p-117">Average time required for sampling one sequence, using open source implementation of GVP-GNN and open source implementation of Transformer from fairseq (<xref rid="c47" ref-type="bibr" hwp:id="xref-ref-47-2" hwp:rel-id="ref-47">Ott et al., 2019</xref>).</p></caption><graphic xlink:href="487779v1_tblC6" position="float" orientation="portrait" hwp:id="graphic-28"/></table-wrap></sec></sec></sec></back></article>
